var relearn_searchindex = [
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Technology that teaches empathy? How mpathic uses AI to help us listen to each other by Bonnie McClure and Chalaire Miller | on 30 APR 2024 | Startup, Startup Spotlight\nOn a basic human level, we want to be heard. We want to connect with others, and we want to be understood. Unfortunately, we’re often faced with many things competing for our attention, which makes us bad listeners.Danielle Schlosser, Chief Innovation Officer\nActive listening is a learned behavior and not easy to master. But what if artificial intelligence (AI) could augment our ability to really listen and truly relate to others? What if technology could draw upon our collective lived experiences and help us be more human to each other?\nThese are the questions Dr. Grin Lord, clinical psychologist and founder of conversation analytics company mpathic, has spent the last 15 years chasing. During her research, Grin and the team at mpathic have identified trust-building words, phrases, and communication behaviors and modeled them using AI.\n“We look at what is promoting trust, what is promoting engagement, and how those impact outcomes,” explains mpathic’s Chief Innovation Officer, Dr. Danielle Schlosser.\nIn pursuit of a technology-driven approach to unlock empathy, mpathic developed something unique: a solution that not only analyzes and assesses the health of conversations but also provides recommendations for increasing their levels of empathy, trust, and engagement in real-time.\n“Our differentiator is trying to be more behavioral and actionable,” says Grin. “We want to coach people on how to improve.”\nDrawing on responses from a diverse range of experts with extensive empathy training, mpathic’s API quickly tags instances of misunderstanding within ongoing conversations and immediately offers feedback and suggestions on how to listen and respond with more empathy.\nThe results have been astonishing. When deployed in clinical trials, healthcare providers using mpathic’s API have been seven times more likely to capture participant risk and provide critical feedback. Similarly, in sales and HR software as a service (SaaS) use cases, businesses using mpathic products witnessed more customer engagement, satisfaction, and other outcomes.\nIterating on empathy education Taking context and nuance into consideration, mpathic defines empathy as “accurate understanding.” But designing a successful method for teaching empathy turned out to be much more elusive than defining it.\nIn the early 2000s, Grin began her journey as part of a research study working with drivers involved in drunk driving accidents. The experiment consisted of brief interventions, including 15 minutes of empathic listening, showing acceptance and understanding of the driver’s experience. This brief empathic intervention led to reductions in drinking that held over three years later and a 46 percent reduction in readmissions to the hospital.\nAfter that, Grin trained medical professionals on how to listen with empathy, teaching behaviors such as reflective listening, asking open-ended questions instead of closed-ended ones, and using affirmations.\nWhen she found that a two-day workshop was not enough time to change deep-seated behaviors and styles of communication, she retooled her approach. Grin learned techniques from a nationwide phone coaching study where doctors would record themselves giving feedback. A psychologist would listen and provide doctors with performance-based suggestions on how to improve. This process could take weeks, so in 2008 she seized an opportunity to use machine learning (ML) to speed up the process.\nAt the University of Washington, Grin was a part of the team that built the first speech signal processing pipelines for performance-based feedback in a medical settings. “With the computing power at the time, it took about 6 hours to process a 30-minute call,” she says. “But the fact you could get any feedback the same day was considered really revolutionary.”\nNow, with enhanced computing, power the original vision of performance-based feedback for medical providers was accelerated to actual real-time. Over the years, Grin built a team of dedicated subject matter experts and specialists pulling from those involved in the original research at University of Washington, as well as AI experts at Carnegie Mellon University, and industry experts from big tech.\nThe idea for mpathic came about when Grin and team realized the commercial value of empathic listening: “Could we make an API that would instantly take any communication and make it more empathic, regardless of the use case?”\nThe team built some of mpathic’s first models using data collected from Empathy Rocks, an empathy training game. In the game, therapists, including members of the Idaho State Crisis Line and California Indian Health Service, would respond to anonymous users from data in public forums with empathy and rank each other’s statements; they received continuing education for playing these games. “We had really diverse groups of people building these models through crowdsourcing that information,” explains Grin.\nExpanding empathy training and tools across industries As mpathic continues to evolve and grow their capabilities, the startup now has more than 200 different models for communication behaviors with tips and suggestions, including how to improve collaboration and power-sharing, and listen with more accuracy using reflections and open-ended questions. They also measure more unconscious metrics of human alignment, like language style synchrony, that have been found in Grin’s research to be more predictive of objective ratings of empathy than other skills. “The goal is not to replace human experience,” says Dr. Amber Jolley-Paige, Vice President of Clinical Product, “but to enhance it.”\nWith a tailored and flexible approach, mpathic uses analysis and metrics to support customers’ specific needs and KPIs, whatever the industry. They currently offer a suite of AI-powered products: the core mpathic API, mConsult, and mTrial. The core API integrates into other software, analyzing communications and proposing actionable suggestions. For example, when mpathic used their API to analyze recruitment interviews for different companies, they found that those who received empathetic feedback had an 8 percent increase in candidate acceptance. mConsult provides immediate recommendations and coaching by reviewing audio or video recordings. And mTrial streamlines clinical trials by enhancing data quality and ensuring consistent care, while proactively reducing risk and easing medical professionals’ workloads.\nEnvisioning the future of health equity mpathic’s journey shows no signs of slowing down. To better reach their goal of improving human communication, the team is expanding its API to specifically address diverse cultural behaviors and coach providers in cultural adaptation.\nCulture can affect how people communicate in various ways. For example, it may affect communication styles, how people deliver information, and their attitudes toward conflict. “With mpathic, we have the ability like never before to create more empathy in healthcare interactions and imagine a future where we can leverage AI to improve health equity,” says Dr. Alison Cerezo, Head of Research and Health Equity.\nThe startup built training data from a diverse group of different genders, cultures, and backgrounds to help curb AI bias. “A lot of the issues that you see with AI bias comes down to models built from data collected from only one or two backgrounds and not understanding the lived experience of the people that those models will impact,” explains Grin. mpathic ensures that they regularly build, refine, and deploy their models with attention and alignment to an ethical AI framework.\nMoving forward, the team at mpathic plans to continue developing AI tools that recognize the nuanced and diverse viewpoints present in all human interactions. “There is no limit to the potential of this technology to train anyone to listen with empathy,” says Grin.\nGoing big with AWS To scale their platform, mpathic needed a robust infrastructure. AWS provided a reliable, solid foundation for mpathic to grow and innovate securely. “We built on AWS to help us scale effectively and meet our customers’ needs quickly and seamlessly,” says Grin. “We’re a relatively tiny startup to be serving customers globally. To be able to tell our customers that we can host data wherever they are in the world is awesome, and wouldn’t be possible without AWS.” mpathic uses AWS for all of its foundational platform components, including compute, storage, and networking infrastructure, ensuring secure cross-border data transfer and storage.Megan Greenlaw, VP, Life Sciences and Psychedelic AI\nBeyond technology, collaboration between mpathic and AWS was built on a shared commitment to helping mpathic reach their goals. “There is a degree of interest and support that’s really impressive, especially coming from such a large organization,” says Danielle. “It’s not just about the technology, it’s also about the connections.”\n“AWS has also done a lot of work highlighting women founders, which I think is great,” adds Megan Greenlaw, Vice President of Life Sciences and Psychedelic AI. “To me it signifies a shift that’s happening in venture, the fact that a company can raise over $10 million and that 90% of those checks are being written by women is pretty outstanding,” says Grin.",
    "description": "Technology that teaches empathy? How mpathic uses AI to help us listen to each other by Bonnie McClure and Chalaire Miller | on 30 APR 2024 | Startup, Startup Spotlight\nOn a basic human level, we want to be heard. We want to connect with others, and we want to be understood. Unfortunately, we’re often faced with many things competing for our attention, which makes us bad listeners.Danielle Schlosser, Chief Innovation Officer",
    "tags": [],
    "title": "Blog 1",
    "uri": "/3-blogstranslated/3.1-blog1/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e EventParticipated",
    "content": "Event Summary: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Event Objectives Bring together enterprises, developers, and leaders to explore innovations in cloud computing and AI. Help Vietnam accelerate digital transformation, explore Gen AI technologies, cloud, and new digital solutions for the future Explore advanced solutions from Amazon Web Services (AWS) experts and partners Provide opportunities to connect with the technology community Speakers Eric Yeo : Country General Manager, Vietnam, Cambodia, Laos \u0026 Myanmar, AWS Dr. Jens Lottner: CEO, Techcombank Ms. Trang Phung: CEO \u0026 Co-Founder, U2U Network Jaime Valles: Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson: Managing Director, ASEAN, AWS Vu Van: Co-founder \u0026 CEO, ELSA Corp Nguyen Hoa Binh: Chairman, Nexttech Group' Dieter Botha: CEO, TymeX Hung Nguyen Gia: Head of Solutions Architect, AWS Son Do: Technical Account Manager, AWS Nguyen Van Hai: Director of Software Engineering, Techcombank Phuc Nguyen: Solutions Architect, AWS Alex Tran: AI Director, OCB Nguyen Minh Ngan: AI Specialist, OCB Nguyen Manh Tuyen: Head of Data Application, LPBank Securities Vinh Nguyen: Co-Founder \u0026 CTO, Ninety Eight Hung Hoang: Customer Solutions Manager, AWS Taiki Dang: Solutions Architect, AWS Key Highlights Leveraging AWS Q to Improve and Enhance Developer Productivity Improve code quality Accelerate solution deployment Provide intelligent programming assistant Maintain strict compliance, privacy, and data security Unlocking Vietnam’s AI Potential AI adoption growth rate reaches 39% 55% of Vietnamese enterprises consider limited digital skills as the main barrier to AI adoption and scaling. Migrate, Modernize and Build on AWS Learn about large-scale migration and modernization strategies on AWS through real-world case studies from Techcombank. Enhance knowledge about application modernization using Generative AI-powered tools, with practical insights from VPBank Gain deep insights from leading industry experts through panel discussions on application modernization Learn about AI-driven cloud modernization specifically for VMware environments Understand AWS security best practices from development to production environments Connect and learn directly from AWS Solutions Architects and industry experts Key Takeaways Why is modernization matter ? Challenges from legacy systems: slowing innovation and increasing costs Benefits of modernization: enabling flexibility, providing deep insights, and driving customer-centric innovation. Impact on business: efficiency, scalability, resilience, competitiveness, and sustainability Modernization Journey Assess: Inventory current environment, identify weaknesses. Mobilize: Establish Center of Excellence (CCoE), build guiding principles, and develop cloud capabilities. Migrate \u0026 Modernize: Prioritize high-impact workloads for migration and modernization. Reinvent: Apply artificial intelligence (AI), automation, data products, and new business models. AWS Transform (First agentic AI service for large-scale migration and modernization) Assessment: Build business case for migrating to AWS. VMware: Supporting tools to help migrate from VMware environment to Amazon EC2. Mainframe: Agent to modernize IBM z/OS (mainframe) applications. .NET: Agent to modernize Windows-based .NET applications to Linux platform. Cloud Security Challenges Innovation roadblocks: Traditional security approaches can make security teams seen as barriers to innovation. Expertise \u0026 resource shortages: Current security teams are operating at maximum capacity and spending most of their time on repetitive, low-value tasks. Scale and complexity: Managing security at scale in diverse and complex cloud environments is a major challenge. Evolving threat landscapes: New attack tactics and technologies continuously emerge, creating an ever-changing risk environment. Data protection \u0026 privacy demands: To maintain customer trust, organizations must meet strict data protection and privacy requirements according to industry regulations (e.g., GDPR, PCI-DSS…). Threat Intelligence is the foundation that helps AWS services respond quickly and proactively to new threats. AWS Shield Amazon GuardDuty Amazon S3 AWS WAF (Web Application Firewall) AWS Network Firewall Amazon Route 53 Resolver DNS Firewall Amazon VPC (Virtual Private Cloud) Event Experience Participating in the workshop “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” was a very beneficial experience, helping me gain a comprehensive view of application modernization and modern tools. Some notable experiences:\nLearning from highly skilled speakers Speakers from AWS and major technology organizations shared best practices in modern application design. Through real case studies, I better understand how to apply Migration, Modernization, and Cloud Security to large projects. Networking and exchange The workshop created opportunities for direct exchange with experts, colleagues, and business teams, helping to enhance ubiquitous language between business and tech. Through practical examples, I realized the importance of a business-first approach, always starting from business needs rather than just focusing on technology. Lessons learned Modernization strategy needs a phased approach with measurement and accurate assessment of current state, should not rush to transform the entire system. AI tools like Amazon Q Developer can boost productivity if integrated into current development workflows.",
    "description": "Event Summary: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Event Objectives Bring together enterprises, developers, and leaders to explore innovations in cloud computing and AI. Help Vietnam accelerate digital transformation, explore Gen AI technologies, cloud, and new digital solutions for the future Explore advanced solutions from Amazon Web Services (AWS) experts and partners Provide opportunities to connect with the technology community Speakers Eric Yeo : Country General Manager, Vietnam, Cambodia, Laos \u0026 Myanmar, AWS Dr. Jens Lottner: CEO, Techcombank Ms. Trang Phung: CEO \u0026 Co-Founder, U2U Network Jaime Valles: Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson: Managing Director, ASEAN, AWS Vu Van: Co-founder \u0026 CEO, ELSA Corp Nguyen Hoa Binh: Chairman, Nexttech Group' Dieter Botha: CEO, TymeX Hung Nguyen Gia: Head of Solutions Architect, AWS Son Do: Technical Account Manager, AWS Nguyen Van Hai: Director of Software Engineering, Techcombank Phuc Nguyen: Solutions Architect, AWS Alex Tran: AI Director, OCB Nguyen Minh Ngan: AI Specialist, OCB Nguyen Manh Tuyen: Head of Data Application, LPBank Securities Vinh Nguyen: Co-Founder \u0026 CTO, Ninety Eight Hung Hoang: Customer Solutions Manager, AWS Taiki Dang: Solutions Architect, AWS Key Highlights Leveraging AWS Q to Improve and Enhance Developer Productivity Improve code quality Accelerate solution deployment Provide intelligent programming assistant Maintain strict compliance, privacy, and data security Unlocking Vietnam’s AI Potential AI adoption growth rate reaches 39% 55% of Vietnamese enterprises consider limited digital skills as the main barrier to AI adoption and scaling. Migrate, Modernize and Build on AWS Learn about large-scale migration and modernization strategies on AWS through real-world case studies from Techcombank. Enhance knowledge about application modernization using Generative AI-powered tools, with practical insights from VPBank Gain deep insights from leading industry experts through panel discussions on application modernization Learn about AI-driven cloud modernization specifically for VMware environments Understand AWS security best practices from development to production environments Connect and learn directly from AWS Solutions Architects and industry experts Key Takeaways Why is modernization matter ? Challenges from legacy systems: slowing innovation and increasing costs Benefits of modernization: enabling flexibility, providing deep insights, and driving customer-centric innovation. Impact on business: efficiency, scalability, resilience, competitiveness, and sustainability Modernization Journey Assess: Inventory current environment, identify weaknesses. Mobilize: Establish Center of Excellence (CCoE), build guiding principles, and develop cloud capabilities. Migrate \u0026 Modernize: Prioritize high-impact workloads for migration and modernization. Reinvent: Apply artificial intelligence (AI), automation, data products, and new business models. AWS Transform (First agentic AI service for large-scale migration and modernization) Assessment: Build business case for migrating to AWS. VMware: Supporting tools to help migrate from VMware environment to Amazon EC2. Mainframe: Agent to modernize IBM z/OS (mainframe) applications. .NET: Agent to modernize Windows-based .NET applications to Linux platform. Cloud Security Challenges Innovation roadblocks: Traditional security approaches can make security teams seen as barriers to innovation. Expertise \u0026 resource shortages: Current security teams are operating at maximum capacity and spending most of their time on repetitive, low-value tasks. Scale and complexity: Managing security at scale in diverse and complex cloud environments is a major challenge. Evolving threat landscapes: New attack tactics and technologies continuously emerge, creating an ever-changing risk environment. Data protection \u0026 privacy demands: To maintain customer trust, organizations must meet strict data protection and privacy requirements according to industry regulations (e.g., GDPR, PCI-DSS…). Threat Intelligence is the foundation that helps AWS services respond quickly and proactively to new threats. AWS Shield Amazon GuardDuty Amazon S3 AWS WAF (Web Application Firewall) AWS Network Firewall Amazon Route 53 Resolver DNS Firewall Amazon VPC (Virtual Private Cloud) Event Experience Participating in the workshop “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” was a very beneficial experience, helping me gain a comprehensive view of application modernization and modern tools. Some notable experiences:",
    "tags": [],
    "title": "Event 1",
    "uri": "/4-eventparticipated/4.1-event1/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console management. Discuss topic and technology for first project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit - Watch the video lecture on module 1 - Learn about the terms: AWS Regions, AWS Availability Zones, AWS Points of Presence (Edge Locations) 8/9/2025 8/9/2025 https://policies.fcjuni.com/ 3 - Practice: + lab01: Create first account, use IAM, create MFA + lab07: Create budgets 9/9/2025 9/9/2025 https://000001.awsstudygroup.com/vi/ https://000007.awsstudygroup.com/vi/ 4 - Learn how to do personal workshops using hugo.io - Translate AWS blogs 10/9/2025 10/9/2025 https://mcshelby.github.io/hugo-theme-relearn/index.html https://aws.amazon.com/blogs/startups/technology-that-teaches-empathy-how-mpathic-uses-ai-to-help-us-listen-to-each-other/ https://gohugo.io/getting-started/quick-start/ 5 - Write logs and configuration for the workshop 11/9/2025 11/9/2025 Week 1 Achievements: Understand what AWS is and have a basic understanding of AWS’s global infrastructure:\nCompute Storage IAM … Practice initializing aws account:\nCreate a new account in aws Set up MFA for the new account Create an IAM User as Admin to use in daily tasks Learn about AWS Support Get familiar with the AWS Management Console and know how to find, access, and use services from the web interface.\nManage budgets by creating budgets and setting alerts.\nConfigure a hugo project to write a workshop.",
    "description": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console management. Discuss topic and technology for first project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit - Watch the video lecture on module 1 - Learn about the terms: AWS Regions, AWS Availability Zones, AWS Points of Presence (Edge Locations) 8/9/2025 8/9/2025 https://policies.fcjuni.com/ 3 - Practice: + lab01: Create first account, use IAM, create MFA + lab07: Create budgets 9/9/2025 9/9/2025 https://000001.awsstudygroup.com/vi/ https://000007.awsstudygroup.com/vi/ 4 - Learn how to do personal workshops using hugo.io - Translate AWS blogs 10/9/2025 10/9/2025 https://mcshelby.github.io/hugo-theme-relearn/index.html https://aws.amazon.com/blogs/startups/technology-that-teaches-empathy-how-mpathic-uses-ai-to-help-us-listen-to-each-other/ https://gohugo.io/getting-started/quick-start/ 5 - Write logs and configuration for the workshop 11/9/2025 11/9/2025 Week 1 Achievements: Understand what AWS is and have a basic understanding of AWS’s global infrastructure:",
    "tags": [],
    "title": "Week 1",
    "uri": "/1-worklog/1.1-week1/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Building VPC and Hybrid DNS with Route53\nWeek 3: Learn about Compute VM services on AWS\nWeek 4: Learn about Amazon S3 storage service\nWeek 5: Learn about security services on AWS\nWeek 6: Learn about database services on AWS Cloud\nWeek 7: Practice deploying applications to aws with docker",
    "description": "Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Building VPC and Hybrid DNS with Route53\nWeek 3: Learn about Compute VM services on AWS\nWeek 4: Learn about Amazon S3 storage service\nWeek 5: Learn about security services on AWS\nWeek 6: Learn about database services on AWS Cloud",
    "tags": [],
    "title": "Worklog",
    "uri": "/1-worklog/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Tech-savvy savings: innovative ways to cut costs in your small businessn Henrique Trevisan, Jonathan Woods, and Vince Anderson | 23/4/2024 | in Best Practices, Permalink\nRunning a small business means making the most of every dollar while maintaining high-quality services. As your business grows and your cloud usage expands, you must find ways to efficiently manage technology resources to protect your bottom line. In today’s challenging economic environment, cost optimization has become a top priority for business owners who want to reduce expenses without sacrificing performance, security, or customer experience.\nThis blog post explores practical strategies — in the short, medium, and long term — for small businesses to optimize their cloud costs while continuing to leverage the powerful capabilities that cloud platforms like Amazon Web Services (AWS) offer. Whether you’re just starting your cloud journey or looking to refine your existing setup, these insights will provide valuable guidance on making smart technology investments while keeping costs in check.\nQuick wins for immediate cost savings 1. Modernize your storage strategy Many small businesses overpay for data storage because they haven’t optimized their configuration. Smart storage management involves three key strategies:\nImplement tiering by moving rarely accessed data to lower-cost storage options, keeping only frequently used information in premium storage. Right-size your storage allocations—many businesses pay for significantly more space than they actually use. Configure your storage performance settings to match your actual needs rather than using default configurations. This balanced approach can reduce storage costs while maintaining or even improving performance. These optimizations require minimal technical effort but deliver immediate and ongoing savings to your monthly bill.\n2. Eliminate unnecessary certificate expenses Why pay a third party for Secure Sockets Layer/Transport Layer Security(SSL/TLS) certificates when you can get them for free? Many businesses continue paying annual fees to certificate providers out of habit, often spending hundreds of dollars annually for something now available at no cost. By moving your website security certificates to Amazon Route 53 using AWS Certificate Manager (ACM), you eliminate these recurring expenses while gaining automatic certificate renewal. Small businesses that make this switch remove the administrative burden of tracking expiration dates and manually renewing certificates. This straightforward change reduces both costs and security risks with minimal implementation effort.\n3. Optimize your log data retention Many businesses unknowingly store log data longer than necessary, increasing their cloud storage costs month after month. Many log management tools allow you to implement automated retention policies that align with your actual business requirements. By configuring appropriate log retention periods—rather than using unlimited default settings—you can substantially reduce your storage footprint while maintaining compliance. On AWS, Amazon CloudWatch makes this process straightforward with simple controls to retain critical security logs for 90 days while reducing operational logs to just 30 days. This customized approach typically reduces log storage costs compared to indefinite retention, and the automation keeps your team from having to manually delete outdated logs. For most compliance frameworks, 30-90 days of logs is sufficient rather than retaining years of rarely-accessed data.\nMid-term cost optimization strategies 1. Consolidate network resources while maintaining security As your business grows to serve multiple clients or departments, creating completely separate cloud environments for each seems like the safest approach. While this works initially, it multiplies your costs unnecessarily as you scale. Smart businesses are finding ways to share core network infrastructure while using virtual boundaries using a service such as Amazon Virtual Private Cloud (Amazon VPC) to keep client data securely separated. This balanced approach can help you accomplish your security and compliance goals while reducing redundant network components. Companies implementing this network consolidation strategy can experience a reduction in their monthly infrastructure costs without compromising client data isolation. The savings become increasingly significant as your business adds more clients, creating a more efficient growth model that maintains security while eliminating wasteful duplication.\n2. Optimize your content delivery costs Content delivery networks help websites load faster by caching content closer to users, but many small businesses pay for global coverage when their customers are concentrated in just a few regions. Reviewing your actual user geography and aligning your content delivery strategy accordingly can yield significant savings. For example, if your business primarily serves North American and European customers, you can select regional coverage options in Amazon CloudFront rather than the default global setting. This approach helps reduce your content delivery costs while maintaining fast performance in the markets that matter to your business. Geographic optimization creates immediate savings without any negative impact on your actual customer experience.\n3. Automate routine tasks with AI Time is money, especially for small businesses where teams wear multiple hats. Generative AI tools can now automate many repetitive tasks, freeing team capacity to focus on growth activities. For example, sales teams can use AI assistance to generate customized proposals based on previous successful bids, dramatically reducing the time spent on repetitive documentation. Similarly, customer service teams can deploy AI to handle routine inquiries, reducing response times while maintaining your brand voice across all customer interactions. By identifying these high-volume, repeatable tasks in your business, AI automation reduces labor costs while improving consistency and allowing your valuable human resources to focus on strategy and relationship-building.\nThis approach has proven effective for companies likeCreative Realities, Inc. Bart Massey, their EVP of Software Development, reports, “Using Amazon Q Business to build a private model for our product information has cut our RFP and RFI response times by over 50%, allowing us to respond faster to client requests from day one.” Their experience demonstrates how generative AI can significantly streamline business processes, leading to improved efficiency and responsiveness in customer interactions.\nBuilding cost efficiency in the long-term 1. Reduce software development costs Software development expenses can quickly consume your technology budget. Modern code assistance tools accelerate development by suggesting completions, identifying bugs early, and automating routine coding tasks. Amazon Q Developer exemplifies this approach, reducing costly rework by catching issues before they reach production and providing real-time guidance throughout the development process. This AI-powered assistant helps developers implement best practices, write secure code, and troubleshoot issues faster. When coupled with Infrastructure as Code (IaC) practices, businesses can standardize environments from development through production, eliminating time-consuming manual configurations and reducing provisioning time from days to minutes. Companies implementing these approaches can experience lower development costs while simultaneously accelerating their time to market.\n2. Automate infrastructure management Create resources when needed and remove them when not in use. Use AWS Lambda with Amazon EventBridge schedules to automatically shut down development environments after hours and restart them before the workday begins, thereby reducing non-production environment costs by running resources only during working hours. Another effective approach is automating scaling actions for predictable business cycles by employing AWS Auto Scaling groups to automatically increase capacity during busy periods and scale down during slower periods. These automation strategies can deliver cost reductions for targeted workloads.\n3. Rethink your system architecture Not every part of your business technology needs premium “always-on” protection. Apply a layered approach: invest in redundancy only for critical customer-facing components while using standard setups for internal tools. For example, a business can maintain high-availability only for client-facing systems using flexible deployment options from Amazon Relational Database Service (Amazon RDS). Similarly, moving completed client projects to archival tiers fromAmazon Simple Storage Service (Amazon S3) while maintaining accessibility can also reduce costs. For businesses with predictable busy periods, services like Amazon Aurora Serverless automatically adjust resources during peak times and scale down during quieter periods, generating savings compared to maintaining constant maximum capacity. By matching system reliability to actual business needs, most small businesses can reduce cloud costs without affecting critical\nConclusion Optimizing your cloud costs doesn’t have to mean compromising quality or capabilities. By implementing these strategies, small businesses can reduce expenses while continuing to leverage powerful technology to drive growth and innovation. Start with quick wins like optimizing storage and certificate management, then progress to more sophisticated approaches like network resource sharing and AI-powered automation. Each step you take will contribute to long-term savings and efficiency.\nRemember that cost optimization is an ongoing journey. As your business grows and evolves, continue to reassess your technology needs and adjust your strategy accordingly. With thoughtful planning and implementation of these approaches, you can build a cost-efficient, scalable technology foundation that supports your business objectives both today and in the future.\nTo get started on your cost-cutting journey with AWS, reach out to a sales specialist.",
    "description": "Tech-savvy savings: innovative ways to cut costs in your small businessn Henrique Trevisan, Jonathan Woods, and Vince Anderson | 23/4/2024 | in Best Practices, Permalink\nRunning a small business means making the most of every dollar while maintaining high-quality services. As your business grows and your cloud usage expands, you must find ways to efficiently manage technology resources to protect your bottom line. In today’s challenging economic environment, cost optimization has become a top priority for business owners who want to reduce expenses without sacrificing performance, security, or customer experience.",
    "tags": [],
    "title": "Blog 2",
    "uri": "/3-blogstranslated/3.2-blog2/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 2 Objectives: Learn the basic VPC architecture and its important components Do the labs in module 2 Read the book Aws advanced network Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn more terms related to VPC practice 15/9/2025 15/9/2025 https://viblo.asia/p/tim-hieu-ve-aws-phan-1-vpc-virtual-private-cloud-924lJGv05PM 3 - Practice: + lab03: Amazon VPC and AWS Site-to-Site VPN Workshop 16/9/2025 16/9/2025 http://000003.awsstudygroup.com/ 4 - Learn how to do DNS setup exercises with route 53 Practice: + lab10: Set up Hybrid DNS with Route 53 Resolver 17/9/2025 17/9/2025 https://000010.awsstudygroup.com/ 5 - Join Cloud Day Vietnam in Ho Chi Minh City 18/09/2025 18/9/2025 6 - Translate blogs - Write worklogs and event reports 1 19/09/2025 19/9/2025 https://aws.amazon.com/vi/blogs/smb/tech-savvy-savings-innovative-ways-to-cut-costs-in-your-small-business/ 7 - Practice: + lab20: Set up AWS Transit Gatewayy + lab19: Setting up VPC Peering 20/09/2025 20/9/2025 https://000020.awsstudygroup.com/vi/ https://000019.awsstudygroup.com/vi/ Week 2 Achievements: Understand VPC service and how to create a virtual private network on AWS\nKnow how to configure security group, subnet, route table\nCreate the first EC2\nBuild a hybrid DNS system based on ROUTE53\nParticipate in event cloud day vietnam and learn more about migration, modernization\nKnow how to clean up unused resources",
    "description": "Week 2 Objectives: Learn the basic VPC architecture and its important components Do the labs in module 2 Read the book Aws advanced network Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn more terms related to VPC practice 15/9/2025 15/9/2025 https://viblo.asia/p/tim-hieu-ve-aws-phan-1-vpc-virtual-private-cloud-924lJGv05PM 3 - Practice: + lab03: Amazon VPC and AWS Site-to-Site VPN Workshop 16/9/2025 16/9/2025 http://000003.awsstudygroup.com/ 4 - Learn how to do DNS setup exercises with route 53 Practice: + lab10: Set up Hybrid DNS with Route 53 Resolver 17/9/2025 17/9/2025 https://000010.awsstudygroup.com/ 5 - Join Cloud Day Vietnam in Ho Chi Minh City 18/09/2025 18/9/2025 6 - Translate blogs - Write worklogs and event reports 1 19/09/2025 19/9/2025 https://aws.amazon.com/vi/blogs/smb/tech-savvy-savings-innovative-ways-to-cut-costs-in-your-small-business/ 7 - Practice: + lab20: Set up AWS Transit Gatewayy + lab19: Setting up VPC Peering 20/09/2025 20/9/2025 https://000020.awsstudygroup.com/vi/ https://000019.awsstudygroup.com/vi/ Week 2 Achievements: Understand VPC service and how to create a virtual private network on AWS",
    "tags": [],
    "title": "Week 2",
    "uri": "/1-worklog/1.2-week2/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 3 Objectives: Learn about VM services on AWS Perform application development on EC2, create backups and use cloudwatch to monitor the system Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures about Compute VM service on AWS - Translate blogs for week 3 22/9/2025 22/9/2025 https://aws.amazon.com/vi/blogs/startups/accelerating-the-next-wave-of-generative-ai-startups/ 3 - Practice: + lab04: Initialize and learn basic features of Amazon EC2 23/9/2025 23/9/2025 https://000004.awsstudygroup.com/ 4 - Practice: + lab04 section 8: Limit resource usage using IAM service + lab02: Manage access rights with AWS Identity and Access Management (IAM) 24/9/2025 24/9/2025 https://000004.awsstudygroup.com/vi/8-costusagegovernance/ https://000002.awsstudygroup.com/vi/ 5 - Practice: + lab08: Using CloudWatch 25/9/2025 25/9/2025 https://000008.awsstudygroup.com/ 6 - Practice: + lab06: Deploy FCJ Management application with Auto Scaling Group + lab13: Deploy aws backup for the system 26/9/2025 26/9/2025 https://000006.awsstudygroup.com/ https://000013.awsstudygroup.com/ Week 3 Achievements: Understand the Compute VM service on AWS\nKnow how to deploy a simple application to an AWS Linux server\nManage access to resources\nImplement system scaling strategies to balance the system load\nUse CloudWatch to monitor the system’s health\nDeploy AWS Backup for the system\nKnow how to limit access to resources using IAM",
    "description": "Week 3 Objectives: Learn about VM services on AWS Perform application development on EC2, create backups and use cloudwatch to monitor the system Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures about Compute VM service on AWS - Translate blogs for week 3 22/9/2025 22/9/2025 https://aws.amazon.com/vi/blogs/startups/accelerating-the-next-wave-of-generative-ai-startups/ 3 - Practice: + lab04: Initialize and learn basic features of Amazon EC2 23/9/2025 23/9/2025 https://000004.awsstudygroup.com/ 4 - Practice: + lab04 section 8: Limit resource usage using IAM service + lab02: Manage access rights with AWS Identity and Access Management (IAM) 24/9/2025 24/9/2025 https://000004.awsstudygroup.com/vi/8-costusagegovernance/ https://000002.awsstudygroup.com/vi/ 5 - Practice: + lab08: Using CloudWatch 25/9/2025 25/9/2025 https://000008.awsstudygroup.com/ 6 - Practice: + lab06: Deploy FCJ Management application with Auto Scaling Group + lab13: Deploy aws backup for the system 26/9/2025 26/9/2025 https://000006.awsstudygroup.com/ https://000013.awsstudygroup.com/ Week 3 Achievements: Understand the Compute VM service on AWS",
    "tags": [],
    "title": " Week 3",
    "uri": "/1-worklog/1.3-week3/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Accelerating the next wave of generative AI startups Swami Sivasubramanian | 13/06/2024 | in AWS for Startups, Featured, Startup, Startup Spotlight | Permalink\nSince day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. Each year since 2020, we have provided startups nearly $1 billion in AWS Promotional Credits. It’s no coincidence then that 80% of the world’s unicorns use AWS. I am lucky to have had a front row seat to the development of so many of these startups over my time at AWS—companies like Netflix, Wiz, and Airtasker. And I’m enthusiastic about the rapid pace at which startups are adopting generative artificial intelligence (AI) and how this technology is creating an entirely new generation of startups.\nThese generative AI startups have the ability to transform industries and shape the future, which is why today we announced a commitment of $230 million to accelerate the creation of generative AI applications by startups around the world. We are excited to collaborate with visionary startups, nurture their growth, and unlock new possibilities. In addition to this monetary investment, today we’re also announcing the second-annual AWS Generative AI Accelerator in partnership with NVIDIA. This global 10-week hybrid program is designed to propel the next wave of generative AI startups. This year, we’re expanding the program 4x to serve 80 startups globally. Selected participants will each receive up to $1 million in AWS Promotional Credits to fuel their development and scaling needs. The program also provides go-to-market support as well as business and technical mentorship. Participants will tap into a network that includes domain experts from AWS as well as key AWS partners such as NVIDIA, Meta, Mistral AI, and venture capital firms investing in generative AI.\nBuilding in the cloud with generative AI In addition to these programs, AWS is committed to making it possible for startups of all sizes and developers of all skill levels to build and scale generative AI applications with the most comprehensive set of capabilities across the three layers of the generative AI stack. At the bottom layer of the stack, we provide infrastructure to train large language models (LLMs) and foundation models (FMs) and produce inferences or predictions. This includes the best NVIDIA GPUs and GPU-optimized software, custom, machine learning (ML) chips including AWS Trainium and AWS Inferentia, as well as Amazon SageMaker, which greatly simplifies the ML development process. In the middle layer, Amazon Bedrock makes it easier for startups to build secure, customized, and responsible generative AI applications using LLMs and other FMs from leading AI companies. And at the top layer of the stack, we have Amazon Q, the most capable generative AI-powered assistant for accelerating software development and leveraging companies’ internal data.\nCustomers are innovating using technologies across the stack. For instance, during my time at the VivaTech conference in Paris last month, I sat down Michael Chen, VP of Strategic Alliances at PolyAI, which offers customized voice AI solutions for enterprises. PolyAI develops natural-sounding text-to-speech models using Amazon SageMaker. And they build on Amazon Bedrock to ensure responsible and ethical AI practices. They use Amazon Connect to integrate their voice AI into customer service operations.\nAt the bottom layer of the stack, NinjaTech uses Trainium and Inferentia2 chips, along with Amazon SageMaker, to build, train, and scale custom AI agents. From conducting research to scheduling meetings, these AI agents save time and money for NinjaTech’s users by bringing the power of generative AI into their everyday workflows. I recently sat down with Sam Naghshineh, Co-founder and CTO, to discuss how this approach enables them to save time and resources for their users.\nLeonardo.AI, a startup from the 2023 AWS Generative AI Accelerator cohort, is also harnessing the capabilities of AWS Inferentia2 to enable artists and professionals to produce high-quality visual assets with unmatched speed and consistency. By reducing their inference costs without sacrificing performance, Leonardo.AI can offer their most advanced generative AI features at a more accessible price point.\nLeading generative AI startups, including Perplexity, Hugging Face, AI21 Labs, Articul8, Luma AI, Hippocratic AI, Recursal AI, and DatologyAI are building, training, and deploying their models on Amazon SageMaker. For instance, Hugging Face used Amazon SagaMaker HyperPod, a feature that accelerates training by up to 40%, to create new open-source FMs. The automated job recovery feature helps minimize disruptions during the FM training process, saving them hundreds of hours of training time a year.\nAt the middle layer, Perplexity leverages Amazon Bedrock with Anthropic Claude 3 to build their AI-powered search engine. Bedrock ensures robust data protection, ethical alignment through content filtering, and scalable deployment of Claude 3. While Nexxiot, an innovator in transportation and supply chain solutions, quickly moved its Scope AI assistant solution to Amazon Bedrock with Anthropic Claude in order to give their customers the best real-time, conversational insights into their transport assets.\nAt the top layer, Amazon Q Developer helps developers at startups build, test, and deploy applications faster and more efficiently, allowing them to focus their valuable energy on driving innovation. Ancileo, an insurance SaaS provider for insurers, re-insurers, brokers, and affinity partners, uses Amazon Q Developer to reduce the time to resolve coding-related issues by 30%, and is integrating ticketing and documentation with Amazon Q to speed up onboarding and allow anyone in the company to quickly find their answers. Amazon Q Business enables everyone at a startup to be more data-driven and make better, faster decisions using the organization’s collective knowledge. Brightcove, a leading provider of cloud video services, deployed Amazon Q Business to streamline their customer support workflow, allowing the team to expedite responses, provide more personalized service, and ultimately enhance the customer experience.\nResources for generative AI startups The future of generative AI belongs to those who act now. The application window for the AWS Generative AI Accelerator program is open from June 13 to July 19, 2024, and we’ll be selecting a global cohort of the most promising generative AI startups. Don’t miss this unique chance to redefine what’s possible with generative AI, and apply now!\nOther helpful resources include:\nYou can use your AWS Activate credits for Amazon Bedrock to experiment with FMs, along with a broad set of capabilities needed to build responsible generative AI applications with security and privacy. Dive deeper by exploring our Generative AI Community space for technical content, insights, and connections with fellow builders. AWS also provides free training to help the current and future workforce take advantage of Amazon’s generative AI tools. For those interested in learning to build with generative AI on AWS, explore the comprehensive Generative AI Learning Plan for Developers to gain the skills you need to create cutting-edge applications NVIDIA offers NVIDIA Inception, a free program designed to help startups evolve faster through cutting-edge technology, opportunities to connect with venture capitalists, and access to the latest technical resources from NVIDIA. Apply now, explore the resources, and join the generative AI revolution with AWS. Additional Resources Twitch series: Let’s Ship It – with AWS! Generative AI\nAWS Generative AI Accelerator Program: Apply now\nTAGS: Accelerators, AWS Startups",
    "description": "Accelerating the next wave of generative AI startups Swami Sivasubramanian | 13/06/2024 | in AWS for Startups, Featured, Startup, Startup Spotlight | Permalink\nSince day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. Each year since 2020, we have provided startups nearly $1 billion in AWS Promotional Credits. It’s no coincidence then that 80% of the world’s unicorns use AWS. I am lucky to have had a front row seat to the development of so many of these startups over my time at AWS—companies like Netflix, Wiz, and Airtasker. And I’m enthusiastic about the rapid pace at which startups are adopting generative artificial intelligence (AI) and how this technology is creating an entirely new generation of startups.",
    "tags": [],
    "title": "Blog 3",
    "uri": "/3-blogstranslated/3.3-blog3/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "Blog 1 - Technology that teaches empathy? How mpathic uses AI to help us listen to each other What if artificial intelligence (AI) could augment our ability to really listen and truly relate to others? What if technology could draw upon our collective lived experiences and help us be more human to each other? These are the questions Dr. Grin Lord, clinical psychologist and founder of conversation analytics company mpathic, has spent the last 15 years chasing.\nBlog 2 - Tech-savvy savings: innovative ways to cut costs in your small business Discover practical strategies to reduce your small business costs through smart cloud management, storage optimization, and AI automation. Learn how to save money while maintaining quality.\nBlog 3 - Tech-savvy savings: innovative ways to cut costs in your small business Since day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. These startups have the ability to transform industries and shape the future, which is why today we announced a commitment of $230 million to accelerate the creation of generative AI applications by startups around the world. Read to learn how to apply to become a member of this global program.\nBlog 4 - Scaling Cloudera’s development environment: Leveraging Amazon EKS, Karpenter, Bottlerocket, and Cilium for hybrid cloud This post is co-written with Shreelola Hegde,Sriharsha Devineni and Lee Watterworth from Cloudera. Cloudera is a global leader in enterprise data management, analytics, and AI. The Cloudera platform enables organizations to manage, process, and analyze massive datasets, helping businesses across industries like finance, healthcare, manufacturing, and telecommunications accelerate AI/ML adoption and unlock real-time insights. A […]\nBlog 5 - Creating a Multi-Region Application with AWS Services – Part 1, Compute, Networking, and Security Many AWS services have features to help you build and manage a multi-Region architecture, but identifying those capabilities across 200+ services can be overwhelming. In this 3-part blog series, we filter through those 200+ services and focus on those that have specific features to assist you in building multi-Region applications. In Part 1, we’ll build a foundation with AWS security, networking, and compute services.\nBlog 6 - GitOps continuous delivery with ArgoCD and EKS using natural language ArgoCD is a leading GitOps tool that empowers teams to manage Kubernetes deployments declaratively, using Git as the single source of truth. Its robust feature set, including automated sync, rollback support, drift detection, advanced deployment strategies, RBAC integration, and multi-cluster support, makes it a go-to solution for Kubernetes application delivery. However, as organizations scale, several pain points and operational challenges become apparent.\nBlog 7 - Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple Today we are excited to announce version 1.0 of the Strands Agents SDK, marking a significant milestone in our journey to make building AI agents simple, reliable, and production-ready. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production.\nBlog 8 -Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances The analysis of ever-increasing amounts of genomic and multiomic data demands efficient, scalable, and cost-effective computational solutions. Amazon Web Services (AWS) continues to support these workloads through FPGA accelerated compute offerings such as Amazon EC2 F2 instances.",
    "description": "Blog 1 - Technology that teaches empathy? How mpathic uses AI to help us listen to each other What if artificial intelligence (AI) could augment our ability to really listen and truly relate to others? What if technology could draw upon our collective lived experiences and help us be more human to each other? These are the questions Dr. Grin Lord, clinical psychologist and founder of conversation analytics company mpathic, has spent the last 15 years chasing.",
    "tags": [],
    "title": "BlogsTranslated",
    "uri": "/3-blogstranslated/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Scaling Cloudera’s development environment: Leveraging Amazon EKS, Karpenter, Bottlerocket, and Cilium for hybrid cloud Harpreet Virk and Padma Iyer | 26/09/2025 | in Advanced (300), Amazon Elastic Kubernetes Service, Best Practices, Containers, Graviton, Migration, Migration Acceleration Program (MAP) | Permalink |\nThis post is co-written with Shreelola Hegde,Sriharsha Devineni and Lee Watterworth from Cloudera.\nCloudera is a global leader in enterprise data management, analytics, and AI. The Cloudera platform enables organizations to manage, process, and analyze massive datasets, helping businesses across industries like finance, healthcare, manufacturing, and telecommunications accelerate AI/ML adoption and unlock real-time insights.\nA key element driving the success of the platform is the development environment, where developers can build and test new features for release. The environment, built on Kubernetes, faced multiple challenges on-premises especially in the scaling and utilization of resources.\nThis post covers how Cloudera modernized its development operations by adopting a hybrid cloud approach built on Amazon Elastic Kubernetes Service (Amazon EKS). This strategy balances the existing capacity of on-premises environments with the elasticity of the cloud while leveraging enhancements such as Karpenter, Bottlerocker, and Cilium to optimize scaling, security, and cost efficiency.\nOperational Challenges faced On-Premises The development environment faced challenges running on-premises: The environment was required to scale up and down frequently but was constrained by fixed capacity. Build and test processes for services like Apache Spark, Hive, and HBase required containers as large as 64 GB RAM and 32 vCPUs, which quickly exceeded available resources. Pull requests during intensive coding sprints surged as high as 300% leading to 45-minute build time increases, creating a bottleneck in the CI/CD pipeline that significantly slowed development velocity. This, in turn, delayed feature delivery, and risked release schedules while increasing infrastructure costs and developer idle time. The application design also involved retrieving and saving artifacts and datasets from Amazon S3, which introduced latency for on-premises agents, further extending build and test cycles. These constraints created bottlenecks in the development pipeline and highlighted the need for elasticity beyond on-premises clusters. Solution Overview Cloudera addressed these challenges by adopting a hybrid cloud model where predictable workloads remained on-premises and dynamic workloads moved to AWS. The architecture brought together the elasticity of AWS with Cloudera’s established on-premises infrastructure, delivering seamless scaling, reduced latency, and optimized costs.\nHigh-level architecture of the development environment in AWS\nThe modernized Kubernetes architecture provided the following benefits:\nElasticity with Amazon EKS and Karpenter: Cloudera, using Karpenter, was able to scale its workloads from a handful of nodes to thousands within minutes and also contract when demand dropped. This ensured efficient scaling during surges while eliminating idle resource waste during freezes. This also enabled multiple pull requests to run in parallel without waiting for capacity, giving developers faster turnaround times and improving release velocity. Intelligent provisioning ensured that compute instances were always aligned with workload requirements, which improved utilization rates and reduced costs by up to 40%.\nHandling large containers with Karpenter: Build and test jobs requiring massive containers were matched instantly with optimized compute through Karpenter’s intelligent node provisioning. This real-time elasticity ensured no delays or resource contention.\nStrengthening security with Bottlerocket: The linux-based OS further enhanced the scaling process by delivering a container-optimized environment with minimal operating system overhead. Its immutable filesystem strengthened security by preventing unauthorized changes, while atomic updates simplified system patching and reduced maintenance downtime. This change reduced the development environment’s attack surface by 60%, streamlined patching through atomic updates, and improved compute efficiency by 35%.\nReducing build delays with Bottlerocket and Amazon Elastic Block Store (Amazon EBS) (Amazon EBS) snapshots: od launch times fell from 30 minutes to seconds using Bottlerocket and Amazon EBS snapshots to pre-cache large images. This improvement gave developers the ability to start new builds almost instantly, transforming productivity. Pull request spikes no longer created bottlenecks.\nScaling network with Cilium: Networking was modernized with Cilium, which provided identity-based security, advanced pod-level observability, and eBPF-driven networking. By introducing flexible IP address management, Cilium allowed Cloudera to scale beyond 10,000 workloads without encountering IP exhaustion issues, all while offering clear visibility into pod-level networking.\nEliminating idle resource waste with AWS Graviton and Flex instances: Graviton và Flex instances đóng vai trò quan trọng trong tối ưu chi phí và hiệu năng. Graviton mang lại lợi ích hiệu năng-giá tốt cho workloads dựa trên ARM, trong khi Flex instances tăng hiệu quả cho các tác vụ biên dịch x64. Kết hợp lại, các tùy chọn tính toán này giảm gần một phần ba chi phí vận hành và tối đa 40% chi phí hạ tầng, đảm bảo Cloudera cân bằng giữa hiệu suất và chi phí cho các nhu cầu workload đa dạng.\nResolving S3 latency with cloud-native integration: Running builds in Amazon EKS, dropped latency to Amazon S3 to milliseconds, accelerating artifact retrieval. This had the additional benefit of lowering network transfer costs by 30%.\nThis holistic solution not only addressed each bottleneck but also created a foundation that scales elastically, operates securely, and optimizes costs across hybrid environments.\nBusiness Outcomes The adoption of this hybrid Kubernetes environment transformed Cloudera’s development operations. Build and test cycle times improved by 50%, enabling faster delivery of new features and improvements. The ability to scale from 10 to more than 1,000 nodes in minutes gave developers reliable access to the resources they needed, eliminating bottlenecks during high demand. By optimizing data transfers with Amazon S3, network costs were reduced by 30% and latency was cut to milliseconds. Intelligent scaling and workload-aligned compute selection lowered infrastructure costs by 40%. Bottlerocket reduced the attack surface by 60% and improved compute efficiency by 35%.\nThese advances not only strengthened security but also delivered freed engineers from infrastructure management and allowing them to focus on core development.\nConclusion Cloudera’s successful implementation showcases the transformative power of AWS’s container stack – Amazon EKS, Karpenter, and Bottlerocket. The modernized Kubernetes environment resulted in seamless scaling, enhanced security, and optimized cost management while delivering peak performance for dynamic workloads. Cloudera’s journey proves how the integration of purpose-built AWS solutions can dramatically improve infrastructure management, reduce operational overhead, and accelerate developer productivity.\nThrough automated node provisioning, intelligent workload placement, and streamlined operations, Cloudera demonstrates how organizations can achieve efficiency in container environments. Following Cloudera’s proven architecture, enterprises can build a robust, scalable, and cost-effective Kubernetes environment that meets today’s demanding development needs while preparing for future growth. Speak with your AWS account team to take the next steps to building a modernized Amazon EKS environment.",
    "description": "Scaling Cloudera’s development environment: Leveraging Amazon EKS, Karpenter, Bottlerocket, and Cilium for hybrid cloud Harpreet Virk and Padma Iyer | 26/09/2025 | in Advanced (300), Amazon Elastic Kubernetes Service, Best Practices, Containers, Graviton, Migration, Migration Acceleration Program (MAP) | Permalink |\nThis post is co-written with Shreelola Hegde,Sriharsha Devineni and Lee Watterworth from Cloudera.\nCloudera is a global leader in enterprise data management, analytics, and AI. The Cloudera platform enables organizations to manage, process, and analyze massive datasets, helping businesses across industries like finance, healthcare, manufacturing, and telecommunications accelerate AI/ML adoption and unlock real-time insights.",
    "tags": [],
    "title": " Blog 4",
    "uri": "/3-blogstranslated/3.4-blog4/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026 Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee",
    "description": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026 Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee",
    "tags": [],
    "title": "EventParticipated",
    "uri": "/4-eventparticipated/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 4 Objectives: Learn about Amazon S3 Storage Service Learn about Security Services on AWS Implement static web application deployment through S3 and cloudfront Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures on S3 storage service - Translate blogs for week 4 9/29/2025 9/29/2025 https://aws.amazon.com/blogs/migration-and-modernization/scaling-clouderas-development-environment-leveraging-amazon-eks-karpenter-bottlerocket-and-cilium-for-hybrid-cloud/ 3 - Practice: + lab57: STARTING WITH AMAZON S3 + lab13: Deploy AWS Backup to the System + lab 14: VM Import/Export 9/30/2025 9/30/2025 https://000057.awsstudygroup.com https://000014.awsstudygroup.com https://000013.awsstudygroup.com/ 4 - Watch video lectures on security services on AWS 10/1/2025 10/1/2025 5 - Practice: + lab2: AWS Identity and Access Management (IAM) Access Control + lab44: IAM Role \u0026 Condition + lab 48: Granting authorization for an application to access AWS services with an IAM role. 10/2/2025 10/2/2025 https://000048.awsstudygroup.com https://000002.awsstudygroup.com https://000044.awsstudygroup.com 6 - Practice: + lab30: LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY 3/10/2025 3/10/2025 https://000030.awsstudygroup.com Week 4 Achievements: Understand about storage services on S3\nKnow how to deploy a static website through Amazon S3 and cloudFront\nPractice deploying backups for the system\nKnow how to import virtual servers from vm ware to EC2 server\nManage access rights to the system with AWS IAM\nCreate an IAM Role to access AWS services without using AccessKey/Secret Key to ensure security",
    "description": "Week 4 Objectives: Learn about Amazon S3 Storage Service Learn about Security Services on AWS Implement static web application deployment through S3 and cloudfront Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures on S3 storage service - Translate blogs for week 4 9/29/2025 9/29/2025 https://aws.amazon.com/blogs/migration-and-modernization/scaling-clouderas-development-environment-leveraging-amazon-eks-karpenter-bottlerocket-and-cilium-for-hybrid-cloud/ 3 - Practice: + lab57: STARTING WITH AMAZON S3 + lab13: Deploy AWS Backup to the System + lab 14: VM Import/Export 9/30/2025 9/30/2025 https://000057.awsstudygroup.com https://000014.awsstudygroup.com https://000013.awsstudygroup.com/ 4 - Watch video lectures on security services on AWS 10/1/2025 10/1/2025 5 - Practice: + lab2: AWS Identity and Access Management (IAM) Access Control + lab44: IAM Role \u0026 Condition + lab 48: Granting authorization for an application to access AWS services with an IAM role. 10/2/2025 10/2/2025 https://000048.awsstudygroup.com https://000002.awsstudygroup.com https://000044.awsstudygroup.com 6 - Practice: + lab30: LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY 3/10/2025 3/10/2025 https://000030.awsstudygroup.com Week 4 Achievements: Understand about storage services on S3",
    "tags": [],
    "title": "Week 4",
    "uri": "/1-worklog/1.4-week4/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Creating a Multi-Region Application with AWS Services – Part 1, Compute, Networking, and Security Joe Chapman và Sebastian Leks | 08/12/2021 | Amazon CloudFront, Amazon EC2, Amazon Elastic Block Store (Amazon EBS), Amazon Route 53, Amazon Simple Storage Service (S3), Amazon VPC, Architecture, AWS CloudTrail, AWS Global Accelerator, AWS Identity and Access Management (IAM), AWS Secrets Manager, AWS Security Hub, AWS Transit Gateway, AWS Well-Architected | Permalink\nMany AWS services have features to help you build and manage a multi-Region architecture, but identifying those capabilities across 200+ services can be overwhelming.\nIn this 3-part blog series, we filter through those 200+ services and focus on those that have specific features to assist you in building multi-Region applications. In Part 1, we’ll build a foundation with AWS security, networking, and compute services. In Part 2, we’ll add in data and replication strategies. Finally, in Part 3, we’ll look at the application and management layers. As we go through each part, we’ll build up an example application to display one way of combining these services to create a multi-Region application.\nConsiderations before getting started AWS Region are built with multiple isolated and physically separate Availbility Zone. . This approach allows you to create highly available Well-Architected workloads that span AZs to achieve greater fault tolerance. This satisfies the availability goals for most applications, but there are some general reasons that you may be thinking about expanding beyond a single Region:\nExpansion to a global audience as an application grows and its user base becomes more geographically dispersed, there can be a need to reduce latencies for different parts of the world.\nReducing Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) as part of a multi-Region disaster recovery (DR) plan.\nLocal laws and regulationsmay have strict data residency and privacy requirements that must be followed.\nIf you’re building a new multi-Region application, you may want to consider focusing on AWS services that have built-in functionality to assist. Existing applications will need to be further examined to determine the most expandable architecture to support its growth. The following sections review these services, and highlight use cases and best practices.\nIdentity and access across Regions Creating a security foundation starts with setting proper authentication and authorization rules. The system handling these requests must be highly resilient to verify and authorize requests quickly and reliably. AWS Identity and Access Management (IAM) accomplishes this by creating a reliable mechanism for you to manage access to AWS services and resources. IAM has multi-Region availability automatically, with no configuration required on your part.\nFor help managing Windows users, devices, and applications on a multi-Region network, you can set up AWS Directory Service for Microsoft Active Directory Enterprise Edition to automatically replicate directory data across Regions..This reduces directory lookup latencies by using the closest directory and creates durability by spanning multiple Regions. Note that this will also introduce a shared fate across domain controllers for multi-Region topologies, because group policy changes will be propagated to all member servers.\nApplications that need to securely store, rotate, and audit secrets, such as database passwords, should use AWS Secrets Manager. This service encrypts secrets with AWS Key Management Service (AWS KMS) keys and can replicate secrets to secondary Regions to ensure applications are able to quickly retrieve a secret in the closest Region.\nEncryption across Regions AWS KMS can be used to encrypt data at rest, and is used extensively for encryption across AWS services. By default, keys are confined to a single Region. AWS services such as AWS như Amazon Simple Storage Service (Amazon S3) scross-Region replication and Amazon Aurora Global Database (both covered in phần 2), simplify the process of encryption and decryption with different keys in each Region. For other parts of your multi-Region application that rely on KMS keys, you can set up AWS KMS multi-Region keys to replicate the key material and key ID to a second Region. This eliminates the need to decrypt and re-encrypt data with a different key in each Region. For example, multi-Region keys can be used to reduce the complexity of a multi-Region application’s encryption operations for data that is stored across Regions.\nAuditing and observability across Regions It is a best practice to configure AWS CloudTrail to keep a record of all relevant AWS API activity in your account for auditing purposes. When you utilize multiple Regions or accounts, these CloudTrail logs should be aggregated into a single Amazon S3 bucket for easier analysis. To prevent misuse, the centralized logs should be treated with higher severity, with only limited access to key systems and personnel.\nTo stay on top of AWS Security Hub findings, you can aggregate and link findings from multiple locations to a single Region. This is an easy way to create a centralized view of Security Hub findings across accounts and Regions. Once set up, the findings are continuously synced between Regions to keep you updated on global results in a single dashboard.\nWe put these features together in Figure 1. We used IAM to grant fine-grained access to AWS services and resources, Directory Service for Microsoft AD for authentication to Microsoft applications, and Secrets Manager to store sensitive database credentials. Our data, which moves freely between Regions, is encrypted with KMS multi-Region keys, and all AWS API access is logged with CloudTrail and aggregated to a central S3 bucket that only our security team has access to.\nFigure 1. Multi-Region security, identity, and compliance services\nBuilding a global network For resources launched into virtual networks in different Regions, Amazon Virtual Cloud (Amazon VPC) allows private routing between Regions and accounts with VPC peering. These resources can communicate using private IP addresses and do not require an internet gateway, VPN, or separate network appliances. This feature works well for smaller networks that only require a few peering connections. However, transitive routing is not allowed, and as the number of peered virtual private cloud (VPCs) increases, the mesh of peered connections can become difficult to manage and troubleshoot.\nAWS Transit Gateway reduces these difficulties by creating a network transit hub that connects your VPCs and on-premises networks. A Transit Gateway’s routing capabilities can expand to additional Regions with Transit Gateway inter-Region peering to create a globally distributed, private network for your resources.\nBuilding a reliable, cost-effective way to route users to distributed Internet applications requires highly available and scalable Domain Name System (DNS) records. Amazon Route 53 does exactly that.\nRoute 53 includes many routing policies. For example, you can route a request to a record with the lowest network latency, or send users in a specific geolocation to a localized application endpoint. For DR, Route 53 Application Recovery Controller (Route 53 ARC) offers a comprehensive failover solution with minimal dependencies. Route 53 ARC routing policies, safety checks, and readiness checks help you to failover across Regions, AZs, and on-premises reliably.\nRoute 53 includes many routing policies. For example, you can route a request to a record with the lowest network latency, or send users in a specific geolocation to a localized application endpoint. For DR, Route 53 Application Recovery Controller (Route 53 ARC) offers a comprehensive failover solution with minimal dependencies. Route 53 ARC routing policies, safety checks, and readiness checks help you to failover across Regions, AZs, and on-premises reliably.\nThe Amazon CloundFront content delivery network is global, built across 300+ points of presence (PoP) spread throughout the world. Applications that have multiple possible origins, such as across Regions, can use CloudFront origin failover to automatically fail over to a recovery origin when the primary is not available. CloudFront’s capabilities expand beyond serving content, with the ability to run compute at the edge. CloudFront Functions make it easy to run lightweight JavaScript code, and AWS Lambda@Edge enables you to run Node.js and Python functions closer to users of your application, which improves performance and reduces latency. By placing compute at the edge, you can take load off of your origin and provide quicker responses for your global end users.\nBuilt on the AWS global network, AWS Global Accelerator provides two static anycast IPs to give a single-entry point for internet-facing applications. You can seamlessly add or remove origins while continuing to automatically route traffic to the closest healthy Regional endpoint. If a failure is detected, Global Accelerator will automatically redirect traffic to a healthy endpoint within seconds, with no changes to the static IP.\nFigure 2 uses a Route 53 latency-based routing policy to route users to the quickest endpoint, CloudFront is used to serve static content such as videos and images, and Transit Gateway creates a global private network for our devices to talk securely across Regions.\nBuilding and managing the compute layer\nXây dựng và quản lý lớp tính toán (compute layer) Although Amazon Elastic Compute Cloud (Amazon EC2) instances and their associated Amazon Elastic Block Store (Amazon EBS) volumes reside in a single AZ Amazon Data Lifecycle Manager can automate the process of taking and copying EBS snapshots across Regions. This can enhance DR strategies by providing an easy cold backup-and-restore option for EBS volumes. If you need to back up more than just EBS volumes, AWS Backup provides a central place to do this across multiple services and is covered in part 2.\nAn Amazon EC2 instance is based on an Amazon Machine Image (AMI). An AMI specifies instance configurations such as the instance’s storage, launch permissions, and device mappings. When a new standard image needs to be created and released, EC2 Image Builder simplifies the building, testing, and deployment of new AMIs. It can also help with copying of AMIs to additional Regions to eliminate needing to manually copy source AMIs to target Regions.\nMicroservice-based applications that use containers benefit from quicker start-up times. Amazon Elastic Container Registry (Amazon ECR) can help ensure this happens consistently across Regions with private image replication at the registry level. An ECR private registry can be configured for either cross-Region or cross-account replication to ensure your images are ready in secondary Regions when needed.\nAs an architecture expands into multiple Regions, it can become difficult to track where resources are provisioned. Amazon EC2 Global View helps alleviate this by providing a centralized dashboard to see Amazon EC2 resources such as instances, VPCs, subnets, security groups, and volumes in all active Regions.\nWe bring these compute layer features together in Figure 3 by using EC2 Image Builder to copy our latest golden AMI across Regions for deployment. We also back up each EBS volume for 3 days and replicate it across Regions using Data Lifecycle Manager.\nFigure 3. AMI and EBS snapshot copy across Regions\nBringing it together At the end of each part of this blog series, we build on a sample application based on the services covered. This shows you how to bring these services together to build a multi-Region application with AWS services. We don’t use every service mentioned, just those that fit the use case.\nWe built this example to expand to a global audience. It requires high availability across Regions, and favors performance over strict consistency. We have chosen the following services covered in this post to accomplish our goals:\nA Route 53 latency routing policy that routes users to the deployment with the least latency.\nCloudFront is set up to serve our static content. Region 1 is our primary origin, but we’ve configured origin failover to Region 2 in case of a disaster.\nThe application relies on several third-party APIs, so Secrets Manager with cross-Region replication has been set up to store sensitive API key information.\nWe centralize our CloudTrail logs in Region 1 for easier analysis and auditing.\nSecurity Hub in Region 1 is where we have chosen to aggregate findings from all Regions.\nThis is a containers-based application, and we rely on Amazon ECR replication for each location to quickly pull the latest images locally.\nTo communicate using private IPs across Regions, a Transit Gateway is set up in each Region with intra-Region between them. VPC peering could have also worked, but we expect to expand to several more Regions in the future and decided this would be the better long-term choice.\nIAM is used to grant access to manage our AWS resources.\nFigure 4. Building an application with AWS multi-Region services using services covered in Part 1\nSummary It’s important to create a solid foundation when architecting a multi-Region application. These foundations lay the groundwork for you to move fast in a secure, reliable, and elastic way as you build out your application. Many AWS services include native features to help you build a multi-Region architecture. Your architecture will be different depending on the reason for expanding beyond a single Region. In this post, we covered specific features across AWS security, networking, and compute services that have built-in functionality to take away some of the undifferentiated heavy lifting. We’ll cover data, application, and management services in future posts.\nReady to get started? We’ve chosen some AWS Solutions and AWS Blogs để hỗ trợ bạn!\nLooking for more architecture content? AWS Architecture Center provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more!\nLink bài viết gốc: https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/",
    "description": "Creating a Multi-Region Application with AWS Services – Part 1, Compute, Networking, and Security Joe Chapman và Sebastian Leks | 08/12/2021 | Amazon CloudFront, Amazon EC2, Amazon Elastic Block Store (Amazon EBS), Amazon Route 53, Amazon Simple Storage Service (S3), Amazon VPC, Architecture, AWS CloudTrail, AWS Global Accelerator, AWS Identity and Access Management (IAM), AWS Secrets Manager, AWS Security Hub, AWS Transit Gateway, AWS Well-Architected | Permalink\nMany AWS services have features to help you build and manage a multi-Region architecture, but identifying those capabilities across 200+ services can be overwhelming.",
    "tags": [],
    "title": " Blog 5",
    "uri": "/3-blogstranslated/3.5-blog5/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 5 Objectives: Complete the remaining labs of module 5\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translate blogs for week 5 06/10/2025 06/10/2025 https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/ 3 - Practice: + lab33: Encryption at Rest with AWS KMS 07/10/2025 07/10/2025 https://000033.awsstudygroup.com/vi/ 4 - API programming for the final project module project, section 08/10/2025 08/10/2025 5 - Practice: + lab12: Using AWS IAM Identity Center for robust identity management 09/10/2025 09/10/2025 https://000012.awsstudygroup.com/vi/ 6 - API programming for module task final project 10/10/2025 10/10/2025 Week 5 Achievements: Encrypt data on S3 using KMS\nKnow how to create groups, users, permission sets to assign account permissions to the organization\nKnow how to connect accounts in the organization on CLI to use project resources\nCreate permission sets in many ways for more flexible management",
    "description": "Week 5 Objectives: Complete the remaining labs of module 5\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translate blogs for week 5 06/10/2025 06/10/2025 https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/ 3 - Practice: + lab33: Encryption at Rest with AWS KMS 07/10/2025 07/10/2025 https://000033.awsstudygroup.com/vi/ 4 - API programming for the final project module project, section 08/10/2025 08/10/2025 5 - Practice: + lab12: Using AWS IAM Identity Center for robust identity management 09/10/2025 09/10/2025 https://000012.awsstudygroup.com/vi/ 6 - API programming for module task final project 10/10/2025 10/10/2025 Week 5 Achievements: Encrypt data on S3 using KMS",
    "tags": [],
    "title": "Week 5",
    "uri": "/1-worklog/1.5-week5/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "GitOps continuous delivery with ArgoCD and EKS using natural language Jagdish Komakula, Aditya Ambati, and Anand Krishna Varanasi | 17/07/2025 | Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, Developer Tools, Technical How-to | Permalink\nIntroduction ArgoCD is a leading GitOps tool that empowers teams to manage Kubernetes deployments declaratively, using Git as the single source of truth. Its robust feature set, including automated sync, rollback support, drift detection, advanced deployment strategies, RBAC integration, and multi-cluster support, makes it a go-to solution for Kubernetes application delivery. However, as organizations scale, several pain points and operational challenges become apparent.\nPain Points with Traditional ArgoCD Usage GArgoCD’s UI and CLI are designed for users with extensive technical background. Interacting with YAML manifests, understanding Kubernetes resource relationships, and troubleshooting sync errors require specialized knowledge. This limits access to GitOps workflows for less technical stakeholders and increases reliance on DevOps engineers.\nManaging ArgoCD across multiple clusters or environments (using hub-spoke, per-cluster, or grouped models) introduces significant operational complexity. Teams must handle multiple ArgoCD instances, maintain consistent configuration, and coordinate deployments, which can become a bottleneck as service footprints grow.\nArgoCD excels at syncing and monitoring Kubernetes resources but lacks built-in mechanisms for pre-deployment (e.g., image scanning) or post-deployment (e.g., load testing) tasks. This forces teams to rely on external tools or custom scripts, fragmenting the deployment pipeline and increasing maintenance effort.\nPromoting applications across environments (Dev → Test → Prod) is not natively streamlined. Teams must manually orchestrate or script these promotions, slowing down urgent fixes and complicating the release process.\nAs organizations adopt multi-cluster strategies, managing ArgoCD’s access, RBAC, and resource visibility across environments becomes cumbersome, often leading to fragmented workflows and potential security gaps.\nHow ArgoCD MCP Server with Amazon Q CLI addresses these challenges: The integration of the ArgoCD MCP (Model Context Protocol) Server with Amazon Q CLI fundamentally transforms the user experience by introducing natural language interaction for GitOps operations.\nWith MCP, users can manage deployments, monitor application states, and perform sync or rollback operations using plain conversational language rather than technical commands or YAML. For example, a user can simply ask, “What applications are out of sync in production?” or “Sync the api-service application,” and the system executes the appropriate ArgoCD API calls in the background.\nThis democratizes access to GitOps, enabling less technical team members (such as QA, product managers, or support engineers) to safely interact with deployment workflows.\nNatural language interfaces abstract away the complexity of multi-cluster and multi-environment management. Users can query or act on resources across clusters without memorizing resource names, namespaces, or API endpoints.\nThe MCP server handles authentication, session management, and robust error handling, reducing the need for manual troubleshooting and custom scripting.\nThe integration provides detailed feedback, intelligent endpoint handling, and comprehensive error messages, making it easier to diagnose and resolve issues. Full static type checking and environment-based configuration further enhance reliability and maintainability.\nBy leveraging Amazon Q CLI’s extensibility, users gain access to pre-built integrations and context-aware prompts, accelerating development and deployment workflows.\nThe MCP server enables AI assistants and language models to automate routine tasks, recommend actions, and even debug issues, acting as a virtual DevOps engineer. This can significantly reduce manual effort and speed up incident response.\nTraditional ArgoCD vs. ArgoCD MCP Server with Amazon Q CLI Feature/Challenge Traditional ArgoCD With MCP Server + Amazon Q CLI User Interface Technical UI/CLI, YAML required Natural language, conversational Access for Non-Engineers Limited Broad, democratized Multi-Cluster Management Complex, manual Simplified, abstracted Pre-Post Deployment Tasks External tools/scripts needed (Still external, but easier to invoke) Application Promotion Manual or scripted Natural language, easier orchestration Troubleshooting Technical, error-prone Guided, AI-assisted, detailed feedback Automation Scripting required AI/agent-driven, proactive You can perform the following actions using natural language using Amazon Q CLI integration with ArgoCD MCP server.\nApplication Management: List, create, update, and delete ArgoCD applications\nSync Operations: Trigger sync operations and monitor their status\nResource Tree Visualization: View the hierarchy of resources managed by applications\nHealth Status Monitoring: Check the health of applications and their resources\nEvent Tracking: View events related to applications and resources\nLog Access: Retrieve logs from application workloads\nResource Actions: Execute actions on resources managed by applications\nSetting Up Your Environment Pre-requisites Following are the pre-requisites for setting up your EKS environment to be managed by ArgoCD using Amazon Q CLI.\nAn AWS account with appropriate permissions\nAWS CLI v2.13.0 or later\nNode.js v18.0.0 or later\nnpm v9.0.0 or later\nAmazon Q CLI v1.0.0 or later (npm install -g @aws/amazon-q-cli)\nAn EKS cluster (v1.27 or later) with ArgoCD v2.8 or later installed\nConnecting to your EKS cluster Use AWS CLI to update your kubeconfig aws eks update-kubeconfig --name \u003ccluster_name\u003e --region \u003cregion\u003e --role-arn \u003ciam_role_arn\u003e\nVerify ArgoCD pods are running properly in the argocd namespace kubectl get pods -n argocd\nAccess the ArgoCD server UI locally using port forwarding command kubectl port-forward svc/blueprints-addon-argocd-server -n argocd 8080:443\nCreate AgroCD API Token Access the ArgoCD UI at https://localhost:8080 Log in with the admin credentials Navigate to User Settings \u003e API Tokens Click “Generate New” to create a token Create an Amazon Q CLI MCP configuration file at.amazonq/mcp.json and update the ARGOCD_BASE_URL and ARGOCD_API_TOKEN as per your environment setup. Integrating with Amazon Q CLI { \"mcpServers\": { \"argocd-mcp-stdio\": { \"type\": \"stdio\", \"command\": \"npx\", \"args\": [ \"argocd-mcp@latest\", \"stdio\" ], \"env\": { \"ARGOCD_BASE_URL\": \"\u003cARGOCD_BASE_URL\u003e\", \"ARGOCD_API_TOKEN\": \"\u003cARGOCD_API_TOKEN\u003e\", \"NODE_TLS_REJECT_UNAUTHORIZED\": \"0\" } } } }\rOnce configured, you can start using natural language commands with Amazon Q CLI to interact with your ArgoCD applications.\nManaging ArgoCD applications using natural language Listed below are some example prompts to interact with ArgoCD applications in your EKS cluster.\nList ArgoCD application\nPrompt: List all ArgoCD applications in my cluster\nAmazon Q will use the ArgoCD MCP server to retrieve and display all applications\nCreate new ArgoCD application\nPrompt: Create new argocd application using App name: game-2048 Repo: https://github.com/aws-ia/terraform-aws-eks-blueprints Path: patterns/gitops/getting-started-argocd/k8s. Branch: main Namespace: argocd\nAmazon Q will create a new application from GitRepo information provided\nViewing deployment status\nCâu lệnh: Show me the resource tree for team-carmen app\nAmazon Q will display the hierarchy of Kubernetes resources managed by the application\nSynchronizing applications\nPrompt: Show me the applications that’s out of sync\nAmazon Q will display the out of sync applications\nPrompt: Sync the application\nAmazon Q syncing application\nAmazon Q will:\nInitiate a sync operation for the specified application Monitor the sync progress Report the final status of the sync operation Healthchecks and monitoring\nPrompt: Check the health of all resources in the team-geordie application\nAmazon Q showing health status of all the resources in an application\nAmazon Q will:\nRetrieve the health status of all resources Identify any unhealthy components Provide recommendations for addressing issues Prompt: Show me the logs for the failing pod in the team-platform application\nAmazon Q showing logs of problematic pod\nAmazon Q will:\nIdentify problematic pods Retrieve and display relevant logs Highlight potential error messages Conclusion The integration of Amazon Q CLI with ArgoCD through the MCP server marks a transformative advancement in Kubernetes management, combining ArgoCD’s GitOps capabilities with Amazon Q’s natural language processing. By transforming complex Kubernetes operations into simple conversational interactions, this solution allows teams to focus on what truly matters – creating value for their business. Rather than spending time memorizing commands or navigating technical complexities, teams can now manage their cloud infrastructure through natural dialogue, making the cloud-native journey more accessible and efficient for everyone.Ready to transform your EKS and ArgoCD experience? It’s highly recommended to try out Amazon Q CLI integration with ArgoCD MCP and discover why DevOps teams are making it an essential part of their toolkit.",
    "description": "GitOps continuous delivery with ArgoCD and EKS using natural language Jagdish Komakula, Aditya Ambati, and Anand Krishna Varanasi | 17/07/2025 | Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, Developer Tools, Technical How-to | Permalink\nIntroduction ArgoCD is a leading GitOps tool that empowers teams to manage Kubernetes deployments declaratively, using Git as the single source of truth. Its robust feature set, including automated sync, rollback support, drift detection, advanced deployment strategies, RBAC integration, and multi-cluster support, makes it a go-to solution for Kubernetes application delivery. However, as organizations scale, several pain points and operational challenges become apparent.",
    "tags": [],
    "title": " Blog 6",
    "uri": "/3-blogstranslated/3.6-blog6/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 6 Objectives: Learn about AWS cloud database services Practice labs related to using RDS Review the content and practice of module 2 Tasks to be carried out this week: Day Task Start date Completion date Reference Material 2 - Review module 2: Understanding basic VPC architecture and related important components 12/10/2025 12/10/2025 3 - Study module: Database hosting service on AWS 13/10/2025 13/10/2025 Video series about the module on AWS Study Group channel 4 - Practice: lab 05: Amazon Relational Database Service (Amazon RDS) 14/10/2025 14/10/2025 https://000005.awsstudygroup.com/vi/ 5 - Learn to use cache, Redis for final project 15/10/2025 15/10/2025 https://www.youtube.com/watch?v=HSknuSIoK6A 6 - Practice: lab 03: Getting Started with Amazon Virtual Private Cloud (VPC) and AWS Site-to-Site VPN 16/10/2025 16/10/2025 https://000003.awsstudygroup.com/vi/ Week 6 Achievements: Learn about different database services: RDS, ElastiCache, Redshift.\nUnderstand database concepts and the key terminology to know when working with databases.\nPractice a lab to deploy a system using RDS for storing information and check the database logs.\nRe-practice the VPC deployment lab to become more proficient and learn how to fix errors during deployment.\nLearn how to cache data in Redis and implement measures to protect the system from spam and data loss.",
    "description": "Week 6 Objectives: Learn about AWS cloud database services Practice labs related to using RDS Review the content and practice of module 2 Tasks to be carried out this week: Day Task Start date Completion date Reference Material 2 - Review module 2: Understanding basic VPC architecture and related important components 12/10/2025 12/10/2025 3 - Study module: Database hosting service on AWS 13/10/2025 13/10/2025 Video series about the module on AWS Study Group channel 4 - Practice: lab 05: Amazon Relational Database Service (Amazon RDS) 14/10/2025 14/10/2025 https://000005.awsstudygroup.com/vi/ 5 - Learn to use cache, Redis for final project 15/10/2025 15/10/2025 https://www.youtube.com/watch?v=HSknuSIoK6A 6 - Practice: lab 03: Getting Started with Amazon Virtual Private Cloud (VPC) and AWS Site-to-Site VPN 16/10/2025 16/10/2025 https://000003.awsstudygroup.com/vi/ Week 6 Achievements: Learn about different database services: RDS, ElastiCache, Redshift.",
    "tags": [],
    "title": "Week 6",
    "uri": "/1-worklog/1.6-week6/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple Ryan Coleman and Belle Guttman | 15/07/2025 | Amazon Machine Learning, Announcements, Artificial Intelligence, Open Source| Permalink | Comments\nToday we are excited to announce version 1.0 of the Strands Agents SDK, marking a significant milestone in our journey to make building AI agents simple, reliable, and production-ready. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production.\nSince launching as a preview in May 2025, we’ve seen over 2,000 stars on GitHub and over 150K downloads on PyPI. Strands 1.0 brings the same level of simplicity to multi-agent applications that Strands has provided for single agents, with the addition of four new primitives and support for the Agent to Agent (A2A) protocol. To take multi-agent architectures into production, 1.0 also includes a new session manager for retrieving agent state from a remote datastore, and improved async support throughout the SDK. For flexibility to build your agents with any model, support for five additional model provider APIs were contributed by partners like Anthropic, Meta, OpenAI, Cohere, Mistral, Stability, Writer and Baseten (see the pull request). Let’s get into these updates in detail. Complete code samples are available on strandsagents.com.\nSimplifying multi-agent patterns Multi-agent patterns enable specialized AI agents to work together—delegating tasks, sharing knowledge, and coordinating actions—to solve complex problems that single agents cannot handle alone. Strands 1.0 introduces four intuitive primitives that make orchestrating multiple agents a simple extension of the model/tool/prompt combination that you use to create single agents.\n1. Agents-as-Tools: Hierarchical Delegation Made Simple\nThe agents-as-tools pattern transforms specialized agents into intelligent tools that other agents can call, enabling hierarchical delegation where agents acting as the orchestrator dynamically consult domain experts without giving up control of the request. This mirrors how human teams work—a project manager doesn’t need to know everything, they just need to know which specialist to consult for each task.\nfrom strands import Agent, tool from strands_tools import calculator, file_write, python_repl, journal @tool def web_search(query: str) -\u003e str: return \"Dummy web search results here!\" # Create specialized agents research_analyst_agent = Agent( system_prompt=\"You are a research specialist who gathers and analyzes information about local startup markets\", tools=[web_search, calculator, file_write, python_repl] ) travel_advisor_agent = Agent( system_prompt=\"You are a travel expert who helps with trip planning and destination advice\", tools=[web_search, journal] ) # Convert the agents into tools @tool def research_analyst(query: str) -\u003e str: response = research_analyst_agent(query) return str(response) @tool def travel_advisor(query: str) -\u003e str: response = travel_advisor_agent(query) return str(response) # Orchestrator naturally delegates to specialists executive_assistant = Agent( tools=[research_analyst, travel_advisor] ) result = executive_assistant(\"I have a business meeting in Portland next week. Suggest a nice place to stay near the local startup scene, and suggest a few startups to visit\") In this abridged example, we define travel and research agents who have specialized prompts and tools for their areas of focus, which the executive assistant agent can call upon for input on the user’s request. The executive assistant agent is responsible for synthesizing input from other agents into the response back to the user. Learn more about Agents-as-Tools in the Strands documentation.\n2. Handoffs: Explicit transfer of control\nHandoffs enable agents to explicitly pass responsibility to humans when they encounter tasks outside their expertise, preserving full conversation context during the transfer. Strands provides a built-in handoff_to_user tool that agents can use to seamlessly transfer control while maintaining conversation history and context—like a customer service representative asking the customer for more information about their case.\nfrom strands import Agent from strands_tools import handoff_to_user SYSTEM_PROMPT=\"\"\" Answer the user's support query. Ask them questions with the handoff_to_user tool when you need more information \"\"\" # Include the handoff_to_user tool in our agent's tool list agent = Agent( system_prompt=SYSTEM_PROMPT, tools=[handoff_to_user] ) # The agent calls the handoff_to_user tool which includes the question for the customer agent(\"I have a question about my order.\")\rCác tác tử cũng có thể đặt câu hỏi cho con người khi được yêu cầu làm như vậy\nfrom strands import Agent SYSTEM_PROMPT=\"\"\" Answer the user's support query. Ask them questions when you need more information \"\"\" agent = Agent( system_prompt=SYSTEM_PROMPT, ) # The agent asks questions by streaming them back as text agent(\"I have a question about my order.\")\r3. Swarms: Self-Organizing Collaborative Teams\nA Swarm creates autonomous agent teams that dynamically coordinate through shared memory, allowing multiple specialists to collaborate on complex tasks. Think of it as a brainstorming session where experts build on each other’s ideas, with the team self-organizing to deliver the best collective result.\nimport logging from strands import Agent from strands.multiagent import Swarm from strands_tools import memory, calculator, file_write # Enables Strands debug logs level, and prints to stderr logging.getLogger(\"strands.multiagent\").setLevel(logging.DEBUG) logging.basicConfig( format=\"%(levelname)s | %(name)s | %(message)s\", handlers=[logging.StreamHandler()] ) researcher = Agent( name=\"researcher\", system_prompt=\"You research topics thoroughly using your memory and built-in knowledge\", tools=[memory] ) analyst = Agent( name=\"analyst\", system_prompt=\"You analyze data and create insights\", tools=[calculator, memory] ) writer = Agent( name=\"writer\", system_prompt=\"You write comprehensive reports based on research and analysis\", tools=[file_write, memory] ) # Swarm automatically coordinates agents market_research_team = Swarm([researcher, analyst, writer]) result = market_research_team( \"What is the history of AI since 1950? Create a comprehensive report\" ) Learn more about Swarms in the Strands documentation.\n4. Graphs: Deterministic Workflow Control\nGraphs let you define explicit agent workflows with conditional routing and decision points, helpful for processes that require specific steps, approvals, or quality gates. Like a well-designed assembly line or approval chain, graphs ensure agents work through predefined business rules in the correct order every time.\nfrom strands import Agent from strands.multiagent import GraphBuilder analyzer_agent = Agent( name=\"analyzer\", system_prompt=\"Analyze customer requests and categorize them\", tools=[text_classifier, sentiment_analyzer] ) normal_processor = Agent( name=\"normal_processor\", system_prompt=\"Handle routine requests automatically\", tools=[knowledge_base, auto_responder] ) critical_processor = Agent( name=\"critical_processor\", system_prompt=\"Handle critical requests quickly\", tools=[knowledge_base, escalate_to_support_agent] ) # Build deterministic workflow builder = GraphBuilder() builder.add_node(analyzer_agent, \"analyze\") builder.add_node(normal_processor, \"normal_processor\") builder.add_node(critical_processor, \"critical_processor\") # Define conditional routing def is_approved(state): return True def is_critical(state): return False builder.add_edge(\"analyze\", \"normal_processor\", condition=is_approved) builder.add_edge(\"analyze\", \"critical_processor\", condition=is_critical) builder.set_entry_point(\"analyze\") customer_support_graph = builder.build() # Execute the graph with user input results = customer_support_graph(\"I need help with my order!\") Learn more about Graphs in the Strands documentation.\nThese multi-agent patterns are designed to be gradually adopted and freely combined—start with single agents, add specialists as tools, evolve to swarms, and orchestrate with graphs as your needs grow. Mix and match patterns to create sophisticated systems: swarms can contain graphs, graphs can orchestrate swarms, and any pattern can use agents equipped with other agents as tools.\nfrom strands import Agent, tool from strands.multiagent import GraphBuilder, Swarm from strands_tools import memory, calculator, python_repl, file_write # Start simple with a single agent agent = Agent(tools=[memory]) # Create specialist agents that a lead orchestrator agent can consult data_analyst = Agent(name=\"analyst\", tools=[calculator, python_repl]) @tool def data_analyst_tool(query: str) -\u003e str: return str(data_analyst(query)) analyst_orchestrator = Agent(tools=[memory, data_analyst_tool]) # Agents-as-tools # Compose patterns together - a graph that uses a swarm researcher = Agent(name=\"researcher\", tools=[memory]) writer = Agent(name=\"writer\", tools=[file_write]) research_swarm = Swarm([researcher, analyst_orchestrator, writer]) review_agent = Agent(system_prompt=\"Review the research quality and suggest improvements\") builder = GraphBuilder() builder.add_node(research_swarm, \"research\") # Swarm as graph node builder.add_node(review_agent, \"review\") builder.add_edge(\"research\", \"review\") graph = builder.build() # The patterns nest naturally - swarms in graphs, agents as tools everywhere result = graph(\"How has green energy evolved over the last few years?\") Multi-Agent Systems with A2A Strands 1.0 includes support for the Agent-to-Agent (A2A) protocol, an open standard that enables agents from different platforms to communicate seamlessly. Any Strands agent can be wrapped with A2A capabilities to become network accessible and adhere to the A2A protocol. A2A agents from external organizations can also be used directly within all Strands multi-agent patterns.\nfrom strands import Agent from strands.multiagent.a2a import A2AServer from strands_tools.a2a_client import A2AClientToolProvider # Serve your agent via A2A protocol local_agent = Agent(name=\"analyzer\", tools=[web_search, data_analysis]) a2a_agent = A2AServer(agent=local_agent, port=9000) a2a_agent.serve() # AgentCard available at http://localhost:9000/.well-known/agent.json # Use remote A2A agents partner_agent_url = \"https://partner.com\" cloud_agent_url = \"https://cloud.ai\" # Connect to remote A2A enabled agents a2a_tool_provider = A2AClientToolProvider(known_agent_urls=[partner_agent_url, cloud_agent_url]) # Orchestrate remote agents orchestrator = Agent(tools=[a2a_tool_provider.tools]) Because A2A provides features like the agent card, a standardized description of agent capabilities, A2A-enabled multi-agent systems can easily discover and connect to agents created by other teams or other organizations. Strands auto-generates the agent card based on the tools you’ve given the agent. To see complete working examples and get started with the A2A integration, check out oursample repository and the Strands A2A documentation.\nProduction-Ready While Strands has been used in production by Amazon teams like Amazon Q Developer and AWS Glue long before its public release, we’ve been working backwards with hundreds of customers worldwide to extend Strands to support your production needs. These updates include a session management abstraction to support persisting data to and recovering from external data stores, structured output, improved async support, and much more (releases changelog).\nDurable Session Management: We’ve added SessionManager, a session management abstraction that enables automatic persistence and restoration of agent conversations and state. This allows agents to save their complete history to a storage backend like Amazon Simple Storage Service (Amazon S3) and seamlessly resume conversations even after compute restarts. Here’s an example using basic file-based persistence.\nfrom strands import Agent from strands.session.file_session_manager import FileSessionManager # Create a session manager with file-based storage Session_manager = FileSessionManager(session_id=”customer_support”, base_dir=\"./agent_sessions\") # Agent automatically persists all conversations agent = Agent( id=\"support_bot_1\", session_manager=session_manager, tools=[knowledge_base, ticket_system] ) # Messages are automatically saved as the conversation progresses agent(\"Help me reset my password\") agent(\"I can't access my email\") # Later, even after a restart, restore the full conversation You can extend this abstraction with your own storage backend implementation through a Data Access Object (DAO) pattern, and Strands includes local filesystem and Amazon S3 backends by default. Each agent gets a unique ID for tracking, and the system handles concurrent agents within the same session for multi-agent scenarios, ensuring your agents maintain context across deployments, scaling events, and system restarts. Learn more about Session Management in the Strands documentation.\nNative Async Support and Improved Performance roduction workloads demand reliability and responsive performance. For 1.0, we’ve improved the Strands event loop architecture to support async operations throughout the entire stack. Tools and model providers can now run asynchronously without blocking, enabling true concurrent execution. The new stream_async method streams all agent events—text, tool usage, reasoning steps—in real-time, with built-in cancellation support for when users navigate away.\nimport asyncio from fastapi import FastAPI from fastapi.responses import StreamingResponse from strands import Agent from strands_tools import calculator app = FastAPI() @app.post(\"/chat\") async def chat_endpoint(message: str): async def stream_response(): agent = Agent(tools=[web_search, calculator]) # Stream agent responses in real-time async for event in agent.stream_async(message): if \"data\" in event: yield f\"data: {event['data']}\\n\\n\" elif \"current_tool_use\" in event: yield f\"event: tool\\ndata: Using {event['current_tool_use']['name']}\\n\\n\" return StreamingResponse(stream_response(), media_type=\"text/event-stream\") # Concurrent agent evaluation async def evaluate_models_concurrently(prompt: str): async def stream(agent: Agent): print(f\"STARTING: {agent.name}\") async for event in agent.stream_async(prompt): # handle events print(f\"ENDING: {agent.name}\") return event[“result”] # last event is the agent result agents = [ Agent(name=\"claude\", model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0”), Agent(name=\"deepseek”, model=\"us.deepseek.r1-v1:0”), Agent(name=\"nova\", model=\"us.amazon.nova-pro-v1:0\") ] # Execute all agents concurrently responses = await asyncio.gather(*[stream(agent) for agent in agents]) return responses Learn more about Native Async Support in the Strands documentation.\nExpanded Model Provider Support: Customers told us they needed flexibility to use different models for different tasks. To deliver this, Strands Agents has received strong support from the model provider community. Model providers like Anthropic, Meta, OpenAI, Cohere, Mistral, Stability and Writer have made contributions which enable their own model API to be used by a Strands Agent with code. Accessing Strands Agents through a provider’s API infrastructure allows developers to focus on building AI-powered solutions, without managing infrastructure. These additions complement preview launch support for any model on Amazon Bedrock, OpenAI, and any OpenAI-compatible endpoint through LiteLLM. Strands lets you use different models for each agent, or switch models and model providers without modifying your tools or logic.\nfrom strands import Agent from strands.models import BedrockModel from strands.models.openai import OpenAIModel from strands.models.anthropic import AnthropicModel # Configure different model providers bedrock_model = BedrockModel( model_id=\"us.amazon.nova-pro-v1:0\", temperature=0.3, top_p=0.8, region_name=\"us-west-2\" ) openai_model = OpenAIModel( client_args={ \"api_key\": \"your-api-key\", }, model_id=\"gpt-4o\", params={ \"max_tokens\": 1000, \"temperature\": 0.7, } ) anthropic_model = AnthropicModel( client_args={ \"api_key\": \"your-api-key\", }, max_tokens=1028, model_id=\"claude-3-7-sonnet-20250219\", params={ \"temperature\": 0.5, } ) # Swap models or use different models for different agents in the same system researcher = Agent( name=\"researcher\", model=anthropic_model, tools=[web_search] ) writer = Agent( name=\"writer\", model=openai_model, tools=[document_formatter] ) analyzer = Agent( name=\"analyzer\", model=bedrock_model, tools=[data_processor] )\rThe Strands community has been a critical voice in shaping all of these updates, through usage, feedback and direct code contributions. Of the over 150 PRs merged into Strands between 0.1.0 and 1.0, 22% were contributed by community members who fixed bugs, added model providers, wrote docs, added features, and refactored classes to improve performance. We’re deeply grateful to each of you for helping make Strands the simplest way to take an agent from prototype to production.\nThe future of AI is multi-agent, and with Strands 1.0, that future is ready for production. Start building today at strandsagents.com.",
    "description": "Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple Ryan Coleman and Belle Guttman | 15/07/2025 | Amazon Machine Learning, Announcements, Artificial Intelligence, Open Source| Permalink | Comments\nToday we are excited to announce version 1.0 of the Strands Agents SDK, marking a significant milestone in our journey to make building AI agents simple, reliable, and production-ready. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production.",
    "tags": [],
    "title": " Blog 7",
    "uri": "/3-blogstranslated/3.7-blog7/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 7 Objectives: Practice labs on deploying applications to AWS\nTranslate blog posts\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Editing the automatic notification API for the final project 20/10/2025 20/10/2025 3 - Practice: lab15: Deploying applications on Docker with AWS 21/10/2025 21/10/2025 https://000015.awsstudygroup.com/vi/ 4 - Translating blog posts: Continuous deployment based on GitOps methodology with ArgoCD and EKS using natural language 22/10/2025 22/10/2025 Blog 6 5 - Translated blogs: Introducing Strands Agents 1.0: Simplifying Multi-Agent Orchestration Ready for Deployment 23/10/2025 23/10/2025 Blog 7 6 - Practice: lab58: Working with Amazon System Manager - Session Manager 24/10/2025 24/10/2025 https://000058.awsstudygroup.com/vi/ Week 7 Achievements: Practice lab to create image and deploy application with docker\nPractice saving docker image to docker hub and Amazon ECR\nPractice accessing ec2 through session manager to enhance security. Avoid opening port 22 to ssh or exposing key-pair to ssh into instance",
    "description": "Week 7 Objectives: Practice labs on deploying applications to AWS\nTranslate blog posts\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Editing the automatic notification API for the final project 20/10/2025 20/10/2025 3 - Practice: lab15: Deploying applications on Docker with AWS 21/10/2025 21/10/2025 https://000015.awsstudygroup.com/vi/ 4 - Translating blog posts: Continuous deployment based on GitOps methodology with ArgoCD and EKS using natural language 22/10/2025 22/10/2025 Blog 6 5 - Translated blogs: Introducing Strands Agents 1.0: Simplifying Multi-Agent Orchestration Ready for Deployment 23/10/2025 23/10/2025 Blog 7 6 - Practice: lab58: Working with Amazon System Manager - Session Manager 24/10/2025 24/10/2025 https://000058.awsstudygroup.com/vi/ Week 7 Achievements: Practice lab to create image and deploy application with docker",
    "tags": [],
    "title": "Week 7",
    "uri": "/1-worklog/1.7-week7/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances Eric Allen, Mark Azadpour, Deepthi Shankar, Olivia Choudhury, and Shyamal Mehtalia | 15/07/2025 | High Performance Computing, Life Sciences, Partner solutions\nThis post was contributed by Eric Allen (AWS), Olivia Choudhury (AWS), Mark Azadpour (AWS), Deepthi Shankar (Illumina), and Shyamal Mehtalia (Illumina)\nThe analysis of ever-increasing amounts of genomic and multiomic data demands efficient, scalable, and cost-effective computational solutions. Amazon Web Services (AWS) continues to support these workloads through FPGA accelerated compute offerings such as Amazon EC2 F2 instances.\nIllumina’s DRAGEN (Dynamic Read Analysis for GENomics) secondary analysis solution has established itself as a leading secondary analysis solution for next generation sequencing data offering highly optimized algorithms for genomic analysis and hardware-accelerated implementations of comprehensive genomic and multiomic pipelines including germline DNA, somatic DNA, as well as bulk and single cell RNA analysis, proteomics, spatial, and more.\nDRAGEN runs natively on EC2 F2 instances and provides customers with an accelerated approach for analyzing their biological data sets. Migration to F2s is streamlined as the same DRAGEN AMI is used for both F1 and F2, and analysis results will be equivalent1. In this post, we will discuss the performance characteristics of DRAGEN across different AWS compute environments as well as how to run the recently launched DRAGEN v4.4 on Amazon EC2 F2 instances.\nAmazon EC2 F2 instances overview F2 instances are the second generation of FPGA powered EC2 instances designed to accelerate analysis of genomic and multimedia data in the cloud. These instances offer significant improvements over their predecessors, the F1 instances, providing up to 60% better price performance. Here are the key features and specifications of F2 instances:\nFPGA Configuration: F2 instances are equipped with up to eight AMD Virtex UltraScale+ HBM VU47P FPGAs, each featuring 16GB of high-bandwidth memory (HBM).\nProcessor: Powered by 3rd generation AMD EPYC (Milan) processors, F2 instances offer up to 192 vCPUs, which is three times the number of processor cores compared to F1 instances.\nMemory: These instances provide up to 2 TiB of system memory, doubling the memory capacity of F1 instances.\nStorage: F2 instances come with up to 7.6 TiB of NVMe SSD storage, which is twice the storage capacity of F1 instances.\nNetworking: They offer up to 100 Gbps of networking bandwidth, quadrupling the network bandwidth available in F1 instances.\nInstance Name\rFPGAs\rvCPUs\rInstance Memory\rNVMe Storage\rNetwork Bandwidth\rf1.2xlarge\r1\r8\r122\r470\rUp to 10 Gbps\rf1.4xlarge\r2\r16\r244\r940\r10 Gbps\rf2.6xlarge\r1\r24\r256\r950\r10 Gbps\rF2.12xlarge\r2\r48\r512\r1900\r25 Gbps\rf1.16xlarge\r8\r64\r976\r44x940\r25 Gbps\rF2.48xlarge\r8\r192\r2048\r7600\r100 Gbps\rTable 1: Table showing comparative compute, memory, storage, and networking specs for F1 vs F2 instances.\nPerformance benchmarking approach Illumina recommends using f1.4xlarge when using F1 instances and f2.6xlarge when using F2 instances. To assess performance across these instances, DRAGEN was configured and run on AWS following the Illumina DRAGEN user guide and the genome reference file links can be found on the Illumina DRAGEN Product Files web page.\nWhole Genome Sequencing (WGS) analysis at approximately 35x depth using a publicly available sample. The sample HG002 from the NIST Genome in a Bottle project was used. This sample was analyzed using DRAGEN v4.4 Germline pipeline in two different manners. The first “basic” analysis included only basic alignment and small variant calling, to facilitate comparisons with common bioinformatics pipelines for germline samples, such as BWA/GATK. The second “full” analysis was with all variant callers (including for copy number and structural variants) and additional options such as pharmacogenetic star allele calling and HLA (Human Leukocyte Antigen) calling turned on, to produce a fully analyzed whole genome. In both cases, DRAGEN’s hg38 multigenome graph reference was used for WGS analysis. The sample data fastq files can be accessed on Amazon S3 using these links: fastq R1, fastq R2.\nTumor Normal analysis using a pair of samples examined in a prior DRAGEN cancer analysis publication. These two samples had approximate depths of 110x (tumor) and 40x (normal). These samples were analyzed using the DRAGEN v4.4 Somatic pipeline, including alignment and small variant calling, plus CNV and SV analysis. DRAGEN’s hg38 linear genome reference was used for the Tumor Normal analysis. The sample data fastq files can be accessed on Amazon S3 using these links: Tumor fastq R1, Tumor fastq R2, Normal fastq R1, Normal fastq R2.\nSpeed \u0026 cost performance comparison: WGS analysis WGS analysis using DRAGEN v4.4 demonstrated significant price performance advantages on Amazon EC2 F2 instances vs F1, while also generating equivalent analysis results between the two instance generations1:\nBasic WGS analysis, including alignment and small variant calling. DRAGEN v4.4 analysis on f2.6xlarge had 1.5x the speed and 40% of the EC2 compute cost vs f1.4xlarge.\nComplete WGS analysis, including alignment, small variant calling and calling of CNVs, SVs, and repeat expansions, and variant annotation. DRAGEN analysis on f2.6xlarge had 2x the speed and 30% of the EC2 compute cost vs f1.4xlarge.\nFigure 1: f2.6xlarge is 1.5x faster for WGS Basic Analysis and 2.1x faster for WGS Full Analysis than f1.4xlarge.\nFigure 2: EC2 compute cost on f2.6xlarge is 40% of the cost on f1.4xlarge for WGS Basic Analysis and 30% of the cost on f1.4xlarge for WGS Full Analysis.\nSpeed \u0026 cost performance comparison: Tumor Normal analysis As with the WGS results, for Tumor Normal analysis DRAGEN v4.4 also demonstrated significant price performance advantages on Amazon EC2 F2 instances vs F1, while also generating equivalent analysis results between the two instance generations1:\nTumor Normal analysis, including alignment, small variant calling, and calling of CNVs and SVs. DRAGEN analysis on f2.6xlarge had 1.7x the speed and 35% of the EC2 compute cost vs f1.4xlarge. Figure 3: f2.6xlarge is 1.7x faster than f1.4xlarge for Tumor Normal Analysis.\nFigure 4: EC2 compute cost on f2.6xlarge is 35% of the cost on f1.4xlarge for WGS Tumor Normal Analysis.\nOther advantages Traditional genomic analysis pipelines using BWA-MEM and GATK running on CPUs have been the industry standard in the past, but DRAGEN has gained traction by providing speed and accuracy advantages. A number of peer reviewed publications have compared DRAGEN speed and accuracy vs BWA/GATK based pipelines running on CPU. For example, Ziegler et al. (2022) found that DRAGEN analysis on FPGA hardware was over 8x faster and more accurate than BWA/GATK based pipelines running on CPU, while Sedlazek et al. (2024) also found DRAGEN to have higher speed and accuracy vs BWA/GATK based pipelines.\nUtilizing DRAGEN on F instances, which are powered by FPGAs, also offers power consumption advantages over traditional CPU and GPU solutions. FPGAs are inherently more energy-efficient, consuming less power for the same level of computational performance for these workloads. This is particularly important in genomic data analysis tasks, where the volume of data and amount of processing time required can be immense.\nFor instance, FPGAs achieve higher performance per watt compared to CPUs and GPUs for DRAGEN WGS workloads. FPGA-based accelerators can deliver superior throughput with lower power consumption. This is due to the dynamic customizability of FPGAs, which allows for optimized configurations that further enhance energy efficiency. In contrast, CPUs and GPUs, while powerful, often require more energy to perform the same tasks, leading to higher operational costs and a larger environmental footprint.\nThe lower power consumption of FPGAs translates to reduced cooling requirements, which can be a significant cost factor in large-scale computing environments. Additionally, the energy efficiency of FPGAs makes them an attractive option for high-performance computing applications, where scalability and cost-effectiveness are critical.\nIn summary, leveraging DRAGEN on Amazon EC2 ‘F’ family instances provides a more energy-efficient solution for genomic data analysis compared to traditional CPU or GPU-based approaches, offering both cost savings and environmental benefits.\nF2 instance availability and cloud implementation options Amazon EC2 F2 instances are available in multiple AWS Regions, including US East (N. Virginia), US West (Oregon), Europe (London), and Asia Pacific (Sydney), with plans for expanded Regional availability. They come in different sizes, such as f2.6xlarge, f2.12xlarge and f2.48xlarge, catering to various workload requirements.\nWhen configuring storage and compute for running DRAGEN workloads on F instances on AWS, it’s important to select the right options to balance performance and cost. Consider storage options such as Amazon EBS gp3 volumes configured for RAID, Amazon FSx for Lustre for higher throughput, and Amazon Elastic File System (EFS) for persistent storage. Additionally, streaming BAM files and reference data from Amazon Simple Storage Service (Amazon S3) or using Mountpoint for Amazon S3 to mount S3 buckets on the local file system can provide economical and simplified file access. By carefully choosing and configuring these storage solutions, you can ensure optimal performance and cost-effectiveness for your HPC workloads. Additionally, consider usingg Illumina Connected Analytics (ICA) or AWS Batch for workflow management. Illumina provides guidance for running DRAGEN trên AWS as part of their online DRAGEN user guide.\nConclusion In summary, Amazon EC2 F2 instances represent a significant advancement in FPGA-powered cloud computing, offering enhanced performance, memory, storage, and networking capabilities over the previous generations. The combination of DRAGEN’s comprehensive pipelines with Amazon EC2 F2’s improved computational power delivers faster, more efficient processing of complex datasets – from whole genome sequencing to single-cell RNA analysis.\nStart your move to F2s today. Please reach out to your AWS account team or Illumina for help with your migration to AWS EC2 F2 instances.\nFor more info about DRAGEN on AWS, search for DRAGEN Complete Suite trên AWS Marketplace, view the blog DRAGEN v4.4, or visit trang chủ Illumina Informatics Solutions.\nReferences Scheffler, K. et al. “Somatic small-variant calling methods in Illumina DRAGEN™ Secondary Analysis.” BioRxiv (2023). https://www.biorxiv.org/content/10.1101/2023.03.23.534011v2\nSedlazeck, F. J. et al. “Comprehensive genome analysis and variant detection at scale using DRAGEN.” Nature Biotechnology (2024). https://www.nature.com/articles/s41587-024-02382-1\nZiegler, A. et al. “Comparison of calling pipelines for whole genome sequencing: an empirical study demonstrating the importance of mapping and alignment.” Scientific Reports (2022). https://pubmed.ncbi.nlm.nih.gov/36513709/\nFootnotes Equivalence between F1 and F2 hard-filtered.vcf and other results files based on analysis results using DRAGEN 4.4.4 following Illumina DRAGEN usage instructions. The vim diff command shows the only difference between the two VCF files is an entry showing the analysis execution time. Customers can do similar analyses themselves to verify or reach out to Illumina for more information.",
    "description": "Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances Eric Allen, Mark Azadpour, Deepthi Shankar, Olivia Choudhury, and Shyamal Mehtalia | 15/07/2025 | High Performance Computing, Life Sciences, Partner solutions\nThis post was contributed by Eric Allen (AWS), Olivia Choudhury (AWS), Mark Azadpour (AWS), Deepthi Shankar (Illumina), and Shyamal Mehtalia (Illumina)\nThe analysis of ever-increasing amounts of genomic and multiomic data demands efficient, scalable, and cost-effective computational solutions. Amazon Web Services (AWS) continues to support these workloads through FPGA accelerated compute offerings such as Amazon EC2 F2 instances.",
    "tags": [],
    "title": " Blog 8",
    "uri": "/3-blogstranslated/3.8-blog8/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Student Information: Full Name: Le Trung Kien\nPhone Number: 0931261009\nEmail: trungkien1862@gmail.om\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1213\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback",
    "description": "Student Information: Full Name: Le Trung Kien\nPhone Number: 0931261009\nEmail: trungkien1862@gmail.om\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1213\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback",
    "tags": [],
    "title": "Intership Report",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
