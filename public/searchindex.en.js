var relearn_searchindex = [
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Technology that teaches empathy? How mpathic uses AI to help us listen to each other by Bonnie McClure and Chalaire Miller | on 30 APR 2024 | Startup, Startup Spotlight\nOn a basic human level, we want to be heard. We want to connect with others, and we want to be understood. Unfortunately, we’re often faced with many things competing for our attention, which makes us bad listeners.Danielle Schlosser, Chief Innovation Officer\nActive listening is a learned behavior and not easy to master. But what if artificial intelligence (AI) could augment our ability to really listen and truly relate to others? What if technology could draw upon our collective lived experiences and help us be more human to each other?\nThese are the questions Dr. Grin Lord, clinical psychologist and founder of conversation analytics company mpathic, has spent the last 15 years chasing. During her research, Grin and the team at mpathic have identified trust-building words, phrases, and communication behaviors and modeled them using AI.\n“We look at what is promoting trust, what is promoting engagement, and how those impact outcomes,” explains mpathic’s Chief Innovation Officer, Dr. Danielle Schlosser.\nIn pursuit of a technology-driven approach to unlock empathy, mpathic developed something unique: a solution that not only analyzes and assesses the health of conversations but also provides recommendations for increasing their levels of empathy, trust, and engagement in real-time.\n“Our differentiator is trying to be more behavioral and actionable,” says Grin. “We want to coach people on how to improve.”\nDrawing on responses from a diverse range of experts with extensive empathy training, mpathic’s API quickly tags instances of misunderstanding within ongoing conversations and immediately offers feedback and suggestions on how to listen and respond with more empathy.\nThe results have been astonishing. When deployed in clinical trials, healthcare providers using mpathic’s API have been seven times more likely to capture participant risk and provide critical feedback. Similarly, in sales and HR software as a service (SaaS) use cases, businesses using mpathic products witnessed more customer engagement, satisfaction, and other outcomes.\nIterating on empathy education Taking context and nuance into consideration, mpathic defines empathy as “accurate understanding.” But designing a successful method for teaching empathy turned out to be much more elusive than defining it.\nIn the early 2000s, Grin began her journey as part of a research study working with drivers involved in drunk driving accidents. The experiment consisted of brief interventions, including 15 minutes of empathic listening, showing acceptance and understanding of the driver’s experience. This brief empathic intervention led to reductions in drinking that held over three years later and a 46 percent reduction in readmissions to the hospital.\nAfter that, Grin trained medical professionals on how to listen with empathy, teaching behaviors such as reflective listening, asking open-ended questions instead of closed-ended ones, and using affirmations.\nWhen she found that a two-day workshop was not enough time to change deep-seated behaviors and styles of communication, she retooled her approach. Grin learned techniques from a nationwide phone coaching study where doctors would record themselves giving feedback. A psychologist would listen and provide doctors with performance-based suggestions on how to improve. This process could take weeks, so in 2008 she seized an opportunity to use machine learning (ML) to speed up the process.\nAt the University of Washington, Grin was a part of the team that built the first speech signal processing pipelines for performance-based feedback in a medical settings. “With the computing power at the time, it took about 6 hours to process a 30-minute call,” she says. “But the fact you could get any feedback the same day was considered really revolutionary.”\nNow, with enhanced computing, power the original vision of performance-based feedback for medical providers was accelerated to actual real-time. Over the years, Grin built a team of dedicated subject matter experts and specialists pulling from those involved in the original research at University of Washington, as well as AI experts at Carnegie Mellon University, and industry experts from big tech.\nThe idea for mpathic came about when Grin and team realized the commercial value of empathic listening: “Could we make an API that would instantly take any communication and make it more empathic, regardless of the use case?”\nThe team built some of mpathic’s first models using data collected from Empathy Rocks, an empathy training game. In the game, therapists, including members of the Idaho State Crisis Line and California Indian Health Service, would respond to anonymous users from data in public forums with empathy and rank each other’s statements; they received continuing education for playing these games. “We had really diverse groups of people building these models through crowdsourcing that information,” explains Grin.\nExpanding empathy training and tools across industries As mpathic continues to evolve and grow their capabilities, the startup now has more than 200 different models for communication behaviors with tips and suggestions, including how to improve collaboration and power-sharing, and listen with more accuracy using reflections and open-ended questions. They also measure more unconscious metrics of human alignment, like language style synchrony, that have been found in Grin’s research to be more predictive of objective ratings of empathy than other skills. “The goal is not to replace human experience,” says Dr. Amber Jolley-Paige, Vice President of Clinical Product, “but to enhance it.”\nWith a tailored and flexible approach, mpathic uses analysis and metrics to support customers’ specific needs and KPIs, whatever the industry. They currently offer a suite of AI-powered products: the core mpathic API, mConsult, and mTrial. The core API integrates into other software, analyzing communications and proposing actionable suggestions. For example, when mpathic used their API to analyze recruitment interviews for different companies, they found that those who received empathetic feedback had an 8 percent increase in candidate acceptance. mConsult provides immediate recommendations and coaching by reviewing audio or video recordings. And mTrial streamlines clinical trials by enhancing data quality and ensuring consistent care, while proactively reducing risk and easing medical professionals’ workloads.\nEnvisioning the future of health equity mpathic’s journey shows no signs of slowing down. To better reach their goal of improving human communication, the team is expanding its API to specifically address diverse cultural behaviors and coach providers in cultural adaptation.\nCulture can affect how people communicate in various ways. For example, it may affect communication styles, how people deliver information, and their attitudes toward conflict. “With mpathic, we have the ability like never before to create more empathy in healthcare interactions and imagine a future where we can leverage AI to improve health equity,” says Dr. Alison Cerezo, Head of Research and Health Equity.\nThe startup built training data from a diverse group of different genders, cultures, and backgrounds to help curb AI bias. “A lot of the issues that you see with AI bias comes down to models built from data collected from only one or two backgrounds and not understanding the lived experience of the people that those models will impact,” explains Grin. mpathic ensures that they regularly build, refine, and deploy their models with attention and alignment to an ethical AI framework.\nMoving forward, the team at mpathic plans to continue developing AI tools that recognize the nuanced and diverse viewpoints present in all human interactions. “There is no limit to the potential of this technology to train anyone to listen with empathy,” says Grin.\nGoing big with AWS To scale their platform, mpathic needed a robust infrastructure. AWS provided a reliable, solid foundation for mpathic to grow and innovate securely. “We built on AWS to help us scale effectively and meet our customers’ needs quickly and seamlessly,” says Grin. “We’re a relatively tiny startup to be serving customers globally. To be able to tell our customers that we can host data wherever they are in the world is awesome, and wouldn’t be possible without AWS.” mpathic uses AWS for all of its foundational platform components, including compute, storage, and networking infrastructure, ensuring secure cross-border data transfer and storage.Megan Greenlaw, VP, Life Sciences and Psychedelic AI\nBeyond technology, collaboration between mpathic and AWS was built on a shared commitment to helping mpathic reach their goals. “There is a degree of interest and support that’s really impressive, especially coming from such a large organization,” says Danielle. “It’s not just about the technology, it’s also about the connections.”\n“AWS has also done a lot of work highlighting women founders, which I think is great,” adds Megan Greenlaw, Vice President of Life Sciences and Psychedelic AI. “To me it signifies a shift that’s happening in venture, the fact that a company can raise over $10 million and that 90% of those checks are being written by women is pretty outstanding,” says Grin.",
    "description": "Technology that teaches empathy? How mpathic uses AI to help us listen to each other by Bonnie McClure and Chalaire Miller | on 30 APR 2024 | Startup, Startup Spotlight\nOn a basic human level, we want to be heard. We want to connect with others, and we want to be understood. Unfortunately, we’re often faced with many things competing for our attention, which makes us bad listeners.Danielle Schlosser, Chief Innovation Officer",
    "tags": [],
    "title": "Blog 1",
    "uri": "/3-blogstranslated/3.1-blog1/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e EventParticipated",
    "content": "Event Summary: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Event Objectives Bring together enterprises, developers, and leaders to explore innovations in cloud computing and AI. Help Vietnam accelerate digital transformation, explore Gen AI technologies, cloud, and new digital solutions for the future Explore advanced solutions from Amazon Web Services (AWS) experts and partners Provide opportunities to connect with the technology community Speakers Eric Yeo : Country General Manager, Vietnam, Cambodia, Laos \u0026 Myanmar, AWS Dr. Jens Lottner: CEO, Techcombank Ms. Trang Phung: CEO \u0026 Co-Founder, U2U Network Jaime Valles: Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson: Managing Director, ASEAN, AWS Vu Van: Co-founder \u0026 CEO, ELSA Corp Nguyen Hoa Binh: Chairman, Nexttech Group' Dieter Botha: CEO, TymeX Hung Nguyen Gia: Head of Solutions Architect, AWS Son Do: Technical Account Manager, AWS Nguyen Van Hai: Director of Software Engineering, Techcombank Phuc Nguyen: Solutions Architect, AWS Alex Tran: AI Director, OCB Nguyen Minh Ngan: AI Specialist, OCB Nguyen Manh Tuyen: Head of Data Application, LPBank Securities Vinh Nguyen: Co-Founder \u0026 CTO, Ninety Eight Hung Hoang: Customer Solutions Manager, AWS Taiki Dang: Solutions Architect, AWS Key Highlights Leveraging AWS Q to Improve and Enhance Developer Productivity Improve code quality Accelerate solution deployment Provide intelligent programming assistant Maintain strict compliance, privacy, and data security Unlocking Vietnam’s AI Potential AI adoption growth rate reaches 39% 55% of Vietnamese enterprises consider limited digital skills as the main barrier to AI adoption and scaling. Migrate, Modernize and Build on AWS Learn about large-scale migration and modernization strategies on AWS through real-world case studies from Techcombank. Enhance knowledge about application modernization using Generative AI-powered tools, with practical insights from VPBank Gain deep insights from leading industry experts through panel discussions on application modernization Learn about AI-driven cloud modernization specifically for VMware environments Understand AWS security best practices from development to production environments Connect and learn directly from AWS Solutions Architects and industry experts Key Takeaways Why is modernization matter ? Challenges from legacy systems: slowing innovation and increasing costs Benefits of modernization: enabling flexibility, providing deep insights, and driving customer-centric innovation. Impact on business: efficiency, scalability, resilience, competitiveness, and sustainability Modernization Journey Assess: Inventory current environment, identify weaknesses. Mobilize: Establish Center of Excellence (CCoE), build guiding principles, and develop cloud capabilities. Migrate \u0026 Modernize: Prioritize high-impact workloads for migration and modernization. Reinvent: Apply artificial intelligence (AI), automation, data products, and new business models. AWS Transform (First agentic AI service for large-scale migration and modernization) Assessment: Build business case for migrating to AWS. VMware: Supporting tools to help migrate from VMware environment to Amazon EC2. Mainframe: Agent to modernize IBM z/OS (mainframe) applications. .NET: Agent to modernize Windows-based .NET applications to Linux platform. Cloud Security Challenges Innovation roadblocks: Traditional security approaches can make security teams seen as barriers to innovation. Expertise \u0026 resource shortages: Current security teams are operating at maximum capacity and spending most of their time on repetitive, low-value tasks. Scale and complexity: Managing security at scale in diverse and complex cloud environments is a major challenge. Evolving threat landscapes: New attack tactics and technologies continuously emerge, creating an ever-changing risk environment. Data protection \u0026 privacy demands: To maintain customer trust, organizations must meet strict data protection and privacy requirements according to industry regulations (e.g., GDPR, PCI-DSS…). Threat Intelligence is the foundation that helps AWS services respond quickly and proactively to new threats. AWS Shield Amazon GuardDuty Amazon S3 AWS WAF (Web Application Firewall) AWS Network Firewall Amazon Route 53 Resolver DNS Firewall Amazon VPC (Virtual Private Cloud) Event Experience Participating in the workshop “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” was a very beneficial experience, helping me gain a comprehensive view of application modernization and modern tools. Some notable experiences:\nLearning from highly skilled speakers Speakers from AWS and major technology organizations shared best practices in modern application design. Through real case studies, I better understand how to apply Migration, Modernization, and Cloud Security to large projects. Networking and exchange The workshop created opportunities for direct exchange with experts, colleagues, and business teams, helping to enhance ubiquitous language between business and tech. Through practical examples, I realized the importance of a business-first approach, always starting from business needs rather than just focusing on technology. Lessons learned Modernization strategy needs a phased approach with measurement and accurate assessment of current state, should not rush to transform the entire system. AI tools like Amazon Q Developer can boost productivity if integrated into current development workflows.",
    "description": "Event Summary: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Event Objectives Bring together enterprises, developers, and leaders to explore innovations in cloud computing and AI. Help Vietnam accelerate digital transformation, explore Gen AI technologies, cloud, and new digital solutions for the future Explore advanced solutions from Amazon Web Services (AWS) experts and partners Provide opportunities to connect with the technology community Speakers Eric Yeo : Country General Manager, Vietnam, Cambodia, Laos \u0026 Myanmar, AWS Dr. Jens Lottner: CEO, Techcombank Ms. Trang Phung: CEO \u0026 Co-Founder, U2U Network Jaime Valles: Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson: Managing Director, ASEAN, AWS Vu Van: Co-founder \u0026 CEO, ELSA Corp Nguyen Hoa Binh: Chairman, Nexttech Group' Dieter Botha: CEO, TymeX Hung Nguyen Gia: Head of Solutions Architect, AWS Son Do: Technical Account Manager, AWS Nguyen Van Hai: Director of Software Engineering, Techcombank Phuc Nguyen: Solutions Architect, AWS Alex Tran: AI Director, OCB Nguyen Minh Ngan: AI Specialist, OCB Nguyen Manh Tuyen: Head of Data Application, LPBank Securities Vinh Nguyen: Co-Founder \u0026 CTO, Ninety Eight Hung Hoang: Customer Solutions Manager, AWS Taiki Dang: Solutions Architect, AWS Key Highlights Leveraging AWS Q to Improve and Enhance Developer Productivity Improve code quality Accelerate solution deployment Provide intelligent programming assistant Maintain strict compliance, privacy, and data security Unlocking Vietnam’s AI Potential AI adoption growth rate reaches 39% 55% of Vietnamese enterprises consider limited digital skills as the main barrier to AI adoption and scaling. Migrate, Modernize and Build on AWS Learn about large-scale migration and modernization strategies on AWS through real-world case studies from Techcombank. Enhance knowledge about application modernization using Generative AI-powered tools, with practical insights from VPBank Gain deep insights from leading industry experts through panel discussions on application modernization Learn about AI-driven cloud modernization specifically for VMware environments Understand AWS security best practices from development to production environments Connect and learn directly from AWS Solutions Architects and industry experts Key Takeaways Why is modernization matter ? Challenges from legacy systems: slowing innovation and increasing costs Benefits of modernization: enabling flexibility, providing deep insights, and driving customer-centric innovation. Impact on business: efficiency, scalability, resilience, competitiveness, and sustainability Modernization Journey Assess: Inventory current environment, identify weaknesses. Mobilize: Establish Center of Excellence (CCoE), build guiding principles, and develop cloud capabilities. Migrate \u0026 Modernize: Prioritize high-impact workloads for migration and modernization. Reinvent: Apply artificial intelligence (AI), automation, data products, and new business models. AWS Transform (First agentic AI service for large-scale migration and modernization) Assessment: Build business case for migrating to AWS. VMware: Supporting tools to help migrate from VMware environment to Amazon EC2. Mainframe: Agent to modernize IBM z/OS (mainframe) applications. .NET: Agent to modernize Windows-based .NET applications to Linux platform. Cloud Security Challenges Innovation roadblocks: Traditional security approaches can make security teams seen as barriers to innovation. Expertise \u0026 resource shortages: Current security teams are operating at maximum capacity and spending most of their time on repetitive, low-value tasks. Scale and complexity: Managing security at scale in diverse and complex cloud environments is a major challenge. Evolving threat landscapes: New attack tactics and technologies continuously emerge, creating an ever-changing risk environment. Data protection \u0026 privacy demands: To maintain customer trust, organizations must meet strict data protection and privacy requirements according to industry regulations (e.g., GDPR, PCI-DSS…). Threat Intelligence is the foundation that helps AWS services respond quickly and proactively to new threats. AWS Shield Amazon GuardDuty Amazon S3 AWS WAF (Web Application Firewall) AWS Network Firewall Amazon Route 53 Resolver DNS Firewall Amazon VPC (Virtual Private Cloud) Event Experience Participating in the workshop “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” was a very beneficial experience, helping me gain a comprehensive view of application modernization and modern tools. Some notable experiences:",
    "tags": [],
    "title": "Event 1",
    "uri": "/4-eventparticipated/4.1-event1/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow",
    "content": "Architectural model Domain: Route 53 (DNS) + ACM (SSL Certificate).\nCDN: CloudFront (Global Edge Network).\nStorage (Primary): S3 Singapore (ap-southeast-1).\nStorage (Failover): S3 N. Virginia (us-east-1).\nReplication: Automatically copy code from Singapore -\u003e Virginia.\nSecurity: OAC (Origin Access Control) - Private Bucket.\nTable of Contents Prerequisites\nS3 and Replication\nRoute 53 and ACM\nClouFront and Failover\nS3 Policy\nDNS Record\nDeploy and Test",
    "description": "Architectural model Domain: Route 53 (DNS) + ACM (SSL Certificate).\nCDN: CloudFront (Global Edge Network).\nStorage (Primary): S3 Singapore (ap-southeast-1).\nStorage (Failover): S3 N. Virginia (us-east-1).\nReplication: Automatically copy code from Singapore -\u003e Virginia.\nSecurity: OAC (Origin Access Control) - Private Bucket.\nTable of Contents Prerequisites\nS3 and Replication\nRoute 53 and ACM\nClouFront and Failover\nS3 Policy\nDNS Record\nDeploy and Test",
    "tags": [],
    "title": "Frontend Deploy",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase establishes the foundational network infrastructure and security boundaries for the microservices deployment.\nVPC and Subnet Configuration Step 1: Create VPC Infrastructure\nNavigate to VPC Console → Create VPC Select VPC and more option Configure VPC parameters: Parameter Value Rationale Name tag auto-generation SGU-Microservices Naming convention IPv4 CIDR block 10.0.0.0/16 Standard private network range Number of Availability Zones 2 (ap-southeast-1a, ap-southeast-1b) High availability across AZs Number of public subnets 2 For internet-facing resources Number of private subnets 2 For data layer isolation NAT gateways None Cost optimization (~$30/month savings) VPC endpoints None Cost optimization DNS options Enable DNS hostnames + Enable DNS resolution Required for service discovery Click Create VPC Network Architecture Result:\nVPC: 10.0.0.0/16\r├── Public Subnet 1: 10.0.0.0/20 (ap-southeast-1a)\r├── Public Subnet 2: 10.0.16.0/20 (ap-southeast-1b)\r├── Private Subnet 1: 10.0.128.0/20 (ap-southeast-1a)\r└── Private Subnet 2: 10.0.144.0/20 (ap-southeast-1b)\rSecurity Groups Configuration Security groups act as virtual firewalls controlling inbound and outbound traffic for AWS resources.\nStep 2: Create Security Groups\nNavigate to VPC → Security Groups → Create security group. Create four security groups with the following specifications:\nSecurity Group 1: public-alb-sg (Application Load Balancer) Parameter Value Name public-alb-sg Description Security group for SGUTODOLIST ALB VPC SGU-Microservices-VPC Inbound Rules:\nType Protocol Port Source Purpose HTTPS TCP 443 0.0.0.0/0 Public HTTPS access HTTP TCP 80 0.0.0.0/0 Public HTTP access (redirect to HTTPS) Security Group 2: ecs-app-sg (ECS Application Containers) Parameter Value Name ecs-app-sg Description Security group for SGUTODOLIST Service Container VPC SGU-Microservices-VPC Inbound Rules - Phase 1 (ALB to Services):\nType Protocol Port Source Purpose Custom TCP TCP 8080 public-alb-sg ALB to API Gateway Custom TCP TCP 8081 public-alb-sg ALB to User Service Custom TCP TCP 8082 public-alb-sg ALB to Taskflow Service Custom TCP TCP 9998 public-alb-sg ALB to Notification Service Custom TCP TCP 9999 public-alb-sg ALB to Auth Service Custom TCP TCP 9092 public-alb-sg Services call to Kafka Custom TCP TCP 9997 public-alb-sg ALB to AI Service Important: Click Create security group before proceeding to Phase 2.\nInbound Rules - Phase 2 (Inter-service Communication):\nSelect the newly created ecs-app-sg Edit inbound rules → Add rule Configure self-referencing rule: Type: All TCP Port range: 0-65535 Source: Select ecs-app-sg (self-reference) Purpose: Allow containers to communicate with each other Security Group 3: private-db-sg (Data Layer) Parameter Value Name private-db-sg Description Security group for SGUTODOLIST RDS \u0026 Redis \u0026 Kafka VPC SGU-Microservices-VPC Inbound Rules:\nType Protocol Port Source Purpose MySQL/Aurora TCP 3306 ecs-app-sg RDS database access Custom TCP TCP 6379 ecs-app-sg Redis cache access Custom TCP TCP 9092 ecs-app-sg Kafka broker access MySQL/Aurora TCP 3306 bastion-sg Access Database from the Bastion Host (Admin/Debug) Custom TCP TCP 6379 bastion-sg Access Redis from the Bastion Host (Admin/Debug) MySQL/Aurora TCP 3306 14.186.212.182/32 Direct Access from a fixed IP address (Personal/Debug) Security Group 4: bastion-sg (SSH Jump Host) Parameter Value Name bastion-sg Description Security group for SGUTODOLIST bastion VPC SGU-Microservices-VPC Inbound Rules:\nType Protocol Port Source Purpose SSH TCP 22 My IP Secure shell access from operator’s IP Security Note: Replace “My IP” with your actual public IP address for enhanced security.\nNetwork Validation Checklist Before proceeding to the next phase, verify:\nVPC created with CIDR 10.0.0.0/16 2 Public subnets across different AZs 2 Private subnets across different AZs DNS hostnames and resolution enabled All 4 security groups created with correct rules ecs-app-sg includes self-referencing rule STEP 2: Infrastructure \u0026 ALB Setup ➡",
    "description": "This phase establishes the foundational network infrastructure and security boundaries for the microservices deployment.\nVPC and Subnet Configuration Step 1: Create VPC Infrastructure\nNavigate to VPC Console → Create VPC Select VPC and more option Configure VPC parameters: Parameter Value Rationale Name tag auto-generation SGU-Microservices Naming convention IPv4 CIDR block 10.0.0.0/16 Standard private network range Number of Availability Zones 2 (ap-southeast-1a, ap-southeast-1b) High availability across AZs Number of public subnets 2 For internet-facing resources Number of private subnets 2 For data layer isolation NAT gateways None Cost optimization (~$30/month savings) VPC endpoints None Cost optimization DNS options Enable DNS hostnames + Enable DNS resolution Required for service discovery Click Create VPC Network Architecture Result:",
    "tags": [],
    "title": "Network \u0026 Security Preparation (VPC, SG)",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.1-network--security-preparation-vpc-sg/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "Preparation Requirements A registered domain name (e.g. sgutodolist.com)\nWith AWS Free Tier accounts, Route 53 domain registration is not supported.\nTherefore, in this project, the domain is registered with a third-party domain provider and then hosted on Amazon Route 53. An AWS account\nA successfully built ReactJS project\nAWS CLI (optional, for testing and validation)\nNode.js and npm installed on the local machine\nRequired Knowledge Basic understanding of Amazon S3, CloudFront, and Route 53\nFamiliarity with using the AWS Management Console\nAbility to build a ReactJS project\nSTEP 2: S3 and Replication ➡",
    "description": "Preparation Requirements A registered domain name (e.g. sgutodolist.com)\nWith AWS Free Tier accounts, Route 53 domain registration is not supported.\nTherefore, in this project, the domain is registered with a third-party domain provider and then hosted on Amazon Route 53. An AWS account\nA successfully built ReactJS project\nAWS CLI (optional, for testing and validation)\nNode.js and npm installed on the local machine\nRequired Knowledge Basic understanding of Amazon S3, CloudFront, and Route 53",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.1-prerequisites/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console management. Create a team to work on the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit - Watch the video lecture on module 1 - Learn about the terms: AWS Regions, AWS Availability Zones, AWS Points of Presence (Edge Locations) - Create a team to work on the project. 8/9/2025 8/9/2025 https://policies.fcjuni.com/ 3 - Practice: + lab01: Create first account, use IAM, create MFA + lab07: Create budgets 9/9/2025 9/9/2025 https://000001.awsstudygroup.com/vi/ https://000007.awsstudygroup.com/vi/ 4 - Learn how to do personal workshops using hugo.io - Translate AWS blogs 10/9/2025 10/9/2025 https://mcshelby.github.io/hugo-theme-relearn/index.html https://gohugo.io/getting-started/quick-start/ original blog’s link 5 - Write logs and configuration for the workshop 11/9/2025 11/9/2025 Week 1 Achievements: Understand what AWS is and have a basic understanding of AWS’s global infrastructure:\nCompute Storage IAM … Practice initializing aws account:\nCreate a new account in aws Set up MFA for the new account Create an IAM User as Admin to use in daily tasks Learn about AWS Support Get familiar with the AWS Management Console and know how to find, access, and use services from the web interface.\nManage budgets by creating budgets and setting alerts.\nConfigure a hugo project to write a workshop.\nGet familiar with the working environment and form a team to develop the project.",
    "description": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console management. Create a team to work on the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit - Watch the video lecture on module 1 - Learn about the terms: AWS Regions, AWS Availability Zones, AWS Points of Presence (Edge Locations) - Create a team to work on the project. 8/9/2025 8/9/2025 https://policies.fcjuni.com/ 3 - Practice: + lab01: Create first account, use IAM, create MFA + lab07: Create budgets 9/9/2025 9/9/2025 https://000001.awsstudygroup.com/vi/ https://000007.awsstudygroup.com/vi/ 4 - Learn how to do personal workshops using hugo.io - Translate AWS blogs 10/9/2025 10/9/2025 https://mcshelby.github.io/hugo-theme-relearn/index.html https://gohugo.io/getting-started/quick-start/ original blog’s link 5 - Write logs and configuration for the workshop 11/9/2025 11/9/2025 Week 1 Achievements: Understand what AWS is and have a basic understanding of AWS’s global infrastructure:",
    "tags": [],
    "title": "Week 1",
    "uri": "/1-worklog/1.1-week1/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "The content of the tasks carried out in each week is as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Building VPC and Hybrid DNS with Route53\nWeek 3: Learn about Compute VM services on AWS\nWeek 4: Learn about Amazon S3 storage service\nWeek 5: Learn about security services on AWS\nWeek 6: Learn about database services on AWS Cloud\nWeek 7: Practice deploying applications to aws with docker\nWeek 8: Learn how to optimize costs on AWS.\nWeek 9: Complete the interface and functionality for Task UpComing. Practice with Serverless\nWeek 10: Completed the comment interface in tasks, implemented real-time notifications, and built APIs for comment attachments\nWeek 11: Built UI and integrated APIs for the comment attachment interface. Fixed issues related to WebSocket\nWeek 12: Added features for customizing email notifications and responding to project invitations on the notification page\nWeek 13: Finalized the project and prepared the project report",
    "description": "The content of the tasks carried out in each week is as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Building VPC and Hybrid DNS with Route53\nWeek 3: Learn about Compute VM services on AWS\nWeek 4: Learn about Amazon S3 storage service\nWeek 5: Learn about security services on AWS\nWeek 6: Learn about database services on AWS Cloud\nWeek 7: Practice deploying applications to aws with docker\nWeek 8: Learn how to optimize costs on AWS.",
    "tags": [],
    "title": "Worklog",
    "uri": "/1-worklog/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop",
    "content": "Project Introduction SGU TodoList is a task management application built using a Microservices architecture on the AWS Cloud platform. The project was initially designed to be deployed as a Multi-Region SaaS model to ensure high availability and disaster recovery. However, due to budget constraints and AWS Free Tier limitations, the team optimized the architecture for a Single-Region Deployment with a Cross-Region Failover mechanism for the frontend.\nOverall architecture Backend Architecture (Single-Region: Singapore) ┌──────────────────────────────────────────────────────┐\r│ Internet Users │\r└───────────────────────┬──────────────────────────────┘\r│\r▼\r┌────────────────┐\r│ Route 53 │ ◄── DNS Management\r│ (Global DNS) │\r└───────┬────────┘\r│\r▼\r┌──────────────────────┐\r│ Application Load │\r│ Balancer (ALB) │ ◄── SSL/TLS Termination\r│ + ACM Certificate │ (sgutodolist.com)\r└──────────┬───────────┘\r│\r┌──────────────┼──────────────┐\r│ │ │\r▼ ▼ ▼\r┌─────────┐ ┌─────────┐ ┌──────────┐\r│ Auth │ │ User │ │Taskflow │ ◄── ECS Fargate\r│ Service │ │ Service │ │ Service │ Microservices\r└────┬────┘ └────┬────┘ └────┬─────┘ (Public Subnets)\r│ │ │\r└────────────┼─────────────┘\r│\r┌────────┴────────┐\r│ API Gateway │ ◄── Central Entry Point\r│ (Port 8080) │ + CORS + Rate Limiting\r└────────┬────────┘\r│\r┌───────────┼─────────────┐\r│ │ │\r▼ ▼ ▼\r┌────────────┐ ┌─────────┐ ┌──────────┐\r│Notification│ │ Kafka │ │ AI Model │\r│ Service │ │ Cluster │ │ Service │\r└──────┬─────┘ └────┬────┘ └────┬─────┘\r│ │ │\r└────────────┼────────────┘\r│\r┌──────────┴──────────┐\r│ │\r▼ ▼\r┌─────────┐ ┌───────────┐\r│ RDS │ │ Redis │ ◄── Data Layer\r│ MySQL │ │ElastiCache│ (Private Subnets)\r└─────────┘ └───────────┘\rFrontend Architecture (Multi-Region Failover) ┌─────────────────────────────────────────────────────────┐\r│ Internet Users │\r└──────────────────────────┬──────────────────────────────┘\r│\r▼\r┌──────────────┐\r│ Route 53 │ ◄── DNS: sgutodolist.com\r└────────┬─────┘\r│\r▼\r┌────────────────┐\r│ CloudFront │ ◄── Global CDN\r│ Distribution │ SSL Certificate\r└────────┬───────┘ Custom Domain\r│\r┌─────────────┴─────────────┐\r│ Origin Group (HA) │\r│ ┌─────────────────┐ │\r│ │ Primary Origin │ │\r│ │ S3 Singapore │ ◄───┼── Main Region\r│ └─────────────────┘ │\r│ │ │\r│ ┌───────▼───────────┐ │\r│ │ Failover Origin │ │\r│ │ S3 N.Virginia │ ◄─┼── Backup Region\r│ └───────────────────┘ │\r└───────────────────────────┘\r│\r┌──────────┴──────────┐\r│ S3 Cross-Region │\r│ Replication (CRR) │ ◄── Auto Sync\r└─────────────────────┘\rMain components 1. Backend Services (ECS Fargate) Service Port Chức năng Dependencies API Gateway 8080 - Điều phối routing\n- CORS handling\n- Rate limiting Redis, All Services Auth Service 9999 - Xác thực JWT\n- OAuth2 (Google)\n- Token management RDS, Redis, Kafka User Service 8081 - Quản lý user profile\n- User CRUD RDS, Redis, Kafka Taskflow Service 8082 - Quản lý tasks\n- Workflows RDS, Redis, Kafka Notification Service 9998 - WebSocket real-time\n- Push notifications RDS, Redis, Kafka AI Model Service 9997 - Task prioritization\n- Smart suggestions Python Flask 2. Infrastructure Components Networking VPC: 10.0.0.0/16 at Singapore (ap-southeast-1) Public Subnets (2 AZs): ECS Tasks, ALB, Bastion Private Subnets (2 AZs): RDS, Redis (data) Security Strategy: Eliminated NAT to save cost (~$30/month) Data Storage RDS MySQL: Primary database, Free Tier (db.t3.micro) ElastiCache Redis: Caching + Rate limiting (cache.t3.micro) Kafka (ECS): Event streaming, Service Discovery (kafka.sgu.local) Load Balancing \u0026 SSL Application Load Balancer: HTTPS termination, path-based routing ACM Certificate: sgutodolist.com Target Groups: Each service has their onw target group with health checks Service Discovery AWS Cloud Map: Namespace sgu.local Internal DNS: auth.sgu.local:9999 user.sgu.local:8081 taskflow.sgu.local:8082 notification.sgu.local:9998 ai-model.sgu.local:9997 kafka.sgu.local:9092 Cost \u0026 Optimization Important Architectural Decisions Factors Initial Choice Final Choice Savings NAT Gateway 2 NAT (HA) ❌ Not used ~$60/month ECS Compute EC2 Instances ✅ Fargate Spot (0.5 vCPU) ~$40/month RDS Multi-AZ ✅ Single-AZ Free Tier ~$30/month Redis Cluster Mode ✅ Single Node ~$20/month Backend Multi-Region Active-Active ✅ Single Region ~$200/month Frontend HA Multi-Region Active ✅ Failover Only ~$50/month Public Subnet Strategy for ECS Instead of using expensive NAT Gateways, all ECS Tasks are deployed on Public Subnets with Public IP enabled. This allows:\n✅ Load Docker images from ECR over the Internet ✅ Outbound connectivity to AWS services ✅ Save 100% on NAT Gateway costs ⚠️ Trade-off: Need to manage Security Groups carefully Domain \u0026 SSL Strategy Production Domains Frontend: https://sgutodolist.com (CloudFront + ACM us-east-1) Backend API: https://sgutodolist.com (ALB + ACM ap-southeast-1) SSL Certificates Certificate 1 (us-east-1): For CloudFront (Must be in Virginia) Domain: sgutodolist.com, *.sgutodolist.com Certificate 2 (ap-southeast-1): For ALB Singapore Domain: sgutodolist.com Security Architecture Security Groups Matrix SG Name Purpose Inbound Rules public-alb-sg ALB public facing 0.0.0.0/0:443, 0.0.0.0/0:80 ecs-app-sg ECS Tasks ALB:8080-8082, 9998-9999; Self: all ports (inter-service) private-db-sg RDS + Redis + Kafka ecs-app-sg:3306, 6379, 9092 bastion-sg SSH Jump Host My IP:22 Authentication Flow User → CloudFront → API Gateway → Auth Service\r↓\rJWT + Redis Session\r↓\rGoogle OAuth2 (Optional)\rTeam Information Project: SGU TodoList - Task Management System Architecture: Single-Region Microservices (Cost-Optimized) Primary Region: Asia Pacific (Singapore) - ap-southeast-1 Failover Region: US East (N. Virginia) - us-east-1 (Frontend only) Deployment Model: AWS Free Tier Optimized",
    "description": "Project Introduction SGU TodoList is a task management application built using a Microservices architecture on the AWS Cloud platform. The project was initially designed to be deployed as a Multi-Region SaaS model to ensure high availability and disaster recovery. However, due to budget constraints and AWS Free Tier limitations, the team optimized the architecture for a Single-Region Deployment with a Cross-Region Failover mechanism for the frontend.\nOverall architecture Backend Architecture (Single-Region: Singapore) ┌──────────────────────────────────────────────────────┐\r│ Internet Users │\r└───────────────────────┬──────────────────────────────┘\r│\r▼\r┌────────────────┐\r│ Route 53 │ ◄── DNS Management\r│ (Global DNS) │\r└───────┬────────┘\r│\r▼\r┌──────────────────────┐\r│ Application Load │\r│ Balancer (ALB) │ ◄── SSL/TLS Termination\r│ + ACM Certificate │ (sgutodolist.com)\r└──────────┬───────────┘\r│\r┌──────────────┼──────────────┐\r│ │ │\r▼ ▼ ▼\r┌─────────┐ ┌─────────┐ ┌──────────┐\r│ Auth │ │ User │ │Taskflow │ ◄── ECS Fargate\r│ Service │ │ Service │ │ Service │ Microservices\r└────┬────┘ └────┬────┘ └────┬─────┘ (Public Subnets)\r│ │ │\r└────────────┼─────────────┘\r│\r┌────────┴────────┐\r│ API Gateway │ ◄── Central Entry Point\r│ (Port 8080) │ + CORS + Rate Limiting\r└────────┬────────┘\r│\r┌───────────┼─────────────┐\r│ │ │\r▼ ▼ ▼\r┌────────────┐ ┌─────────┐ ┌──────────┐\r│Notification│ │ Kafka │ │ AI Model │\r│ Service │ │ Cluster │ │ Service │\r└──────┬─────┘ └────┬────┘ └────┬─────┘\r│ │ │\r└────────────┼────────────┘\r│\r┌──────────┴──────────┐\r│ │\r▼ ▼\r┌─────────┐ ┌───────────┐\r│ RDS │ │ Redis │ ◄── Data Layer\r│ MySQL │ │ElastiCache│ (Private Subnets)\r└─────────┘ └───────────┘\rFrontend Architecture (Multi-Region Failover) ┌─────────────────────────────────────────────────────────┐\r│ Internet Users │\r└──────────────────────────┬──────────────────────────────┘\r│\r▼\r┌──────────────┐\r│ Route 53 │ ◄── DNS: sgutodolist.com\r└────────┬─────┘\r│\r▼\r┌────────────────┐\r│ CloudFront │ ◄── Global CDN\r│ Distribution │ SSL Certificate\r└────────┬───────┘ Custom Domain\r│\r┌─────────────┴─────────────┐\r│ Origin Group (HA) │\r│ ┌─────────────────┐ │\r│ │ Primary Origin │ │\r│ │ S3 Singapore │ ◄───┼── Main Region\r│ └─────────────────┘ │\r│ │ │\r│ ┌───────▼───────────┐ │\r│ │ Failover Origin │ │\r│ │ S3 N.Virginia │ ◄─┼── Backup Region\r│ └───────────────────┘ │\r└───────────────────────────┘\r│\r┌──────────┴──────────┐\r│ S3 Cross-Region │\r│ Replication (CRR) │ ◄── Auto Sync\r└─────────────────────┘\rMain components 1. Backend Services (ECS Fargate) Service Port Chức năng Dependencies API Gateway 8080 - Điều phối routing\n- CORS handling\n- Rate limiting Redis, All Services Auth Service 9999 - Xác thực JWT\n- OAuth2 (Google)\n- Token management RDS, Redis, Kafka User Service 8081 - Quản lý user profile\n- User CRUD RDS, Redis, Kafka Taskflow Service 8082 - Quản lý tasks\n- Workflows RDS, Redis, Kafka Notification Service 9998 - WebSocket real-time\n- Push notifications RDS, Redis, Kafka AI Model Service 9997 - Task prioritization\n- Smart suggestions Python Flask 2. Infrastructure Components Networking VPC: 10.0.0.0/16 at Singapore (ap-southeast-1) Public Subnets (2 AZs): ECS Tasks, ALB, Bastion Private Subnets (2 AZs): RDS, Redis (data) Security Strategy: Eliminated NAT to save cost (~$30/month) Data Storage RDS MySQL: Primary database, Free Tier (db.t3.micro) ElastiCache Redis: Caching + Rate limiting (cache.t3.micro) Kafka (ECS): Event streaming, Service Discovery (kafka.sgu.local) Load Balancing \u0026 SSL Application Load Balancer: HTTPS termination, path-based routing ACM Certificate: sgutodolist.com Target Groups: Each service has their onw target group with health checks Service Discovery AWS Cloud Map: Namespace sgu.local Internal DNS: auth.sgu.local:9999 user.sgu.local:8081 taskflow.sgu.local:8082 notification.sgu.local:9998 ai-model.sgu.local:9997 kafka.sgu.local:9092 Cost \u0026 Optimization Important Architectural Decisions Factors Initial Choice Final Choice Savings NAT Gateway 2 NAT (HA) ❌ Not used ~$60/month ECS Compute EC2 Instances ✅ Fargate Spot (0.5 vCPU) ~$40/month RDS Multi-AZ ✅ Single-AZ Free Tier ~$30/month Redis Cluster Mode ✅ Single Node ~$20/month Backend Multi-Region Active-Active ✅ Single Region ~$200/month Frontend HA Multi-Region Active ✅ Failover Only ~$50/month Public Subnet Strategy for ECS Instead of using expensive NAT Gateways, all ECS Tasks are deployed on Public Subnets with Public IP enabled. This allows:",
    "tags": [],
    "title": "Workshop Overview",
    "uri": "/5-workshop/5.1-workshop_overview/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow",
    "content": "Architecture Model This model illustrates the core services used for high-availability, low-latency content delivery and security:\nDomain \u0026 SSL: Route 53 (DNS) is used for domain management, and ACM (SSL Certificate) secures the traffic.\nCDN: CloudFront acts as the Global Edge Network for content delivery.\nStorage (Primary): S3 in Singapore (ap-southeast-1) holds the main static assets.\nStorage (Failover): S3 in N. Virginia (us-east-1) serves as the secondary storage location.\nReplication: Automated replication copies files from Singapore to Virginia for redundancy.\nSecurity: OAC (Origin Access Control) is implemented to keep the S3 buckets private and ensure traffic only flows via CloudFront.\nTable of Contents Network \u0026 Security Preparation (VPC, SG)\nInfrastructure \u0026 ALB Setup (RDS, Redis, Cloud Map, ALB Routing)\nCode Update \u0026 Image Build (Create new Docker Image)\nTask Definitions Creation (Configure settings, FIX environment variables)\nServices Deployment Completion \u0026 Verification (Route 53, Google Console, Final Test)",
    "description": "Architecture Model This model illustrates the core services used for high-availability, low-latency content delivery and security:\nDomain \u0026 SSL: Route 53 (DNS) is used for domain management, and ACM (SSL Certificate) secures the traffic.\nCDN: CloudFront acts as the Global Edge Network for content delivery.\nStorage (Primary): S3 in Singapore (ap-southeast-1) holds the main static assets.\nStorage (Failover): S3 in N. Virginia (us-east-1) serves as the secondary storage location.\nReplication: Automated replication copies files from Singapore to Virginia for redundancy.",
    "tags": [],
    "title": "Backend Deploy",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Tech-savvy savings: innovative ways to cut costs in your small businessn Henrique Trevisan, Jonathan Woods, and Vince Anderson | 23/4/2024 | in Best Practices, Permalink\nRunning a small business means making the most of every dollar while maintaining high-quality services. As your business grows and your cloud usage expands, you must find ways to efficiently manage technology resources to protect your bottom line. In today’s challenging economic environment, cost optimization has become a top priority for business owners who want to reduce expenses without sacrificing performance, security, or customer experience.\nThis blog post explores practical strategies — in the short, medium, and long term — for small businesses to optimize their cloud costs while continuing to leverage the powerful capabilities that cloud platforms like Amazon Web Services (AWS) offer. Whether you’re just starting your cloud journey or looking to refine your existing setup, these insights will provide valuable guidance on making smart technology investments while keeping costs in check.\nQuick wins for immediate cost savings 1. Modernize your storage strategy Many small businesses overpay for data storage because they haven’t optimized their configuration. Smart storage management involves three key strategies:\nImplement tiering by moving rarely accessed data to lower-cost storage options, keeping only frequently used information in premium storage. Right-size your storage allocations—many businesses pay for significantly more space than they actually use. Configure your storage performance settings to match your actual needs rather than using default configurations. This balanced approach can reduce storage costs while maintaining or even improving performance. These optimizations require minimal technical effort but deliver immediate and ongoing savings to your monthly bill.\n2. Eliminate unnecessary certificate expenses Why pay a third party for Secure Sockets Layer/Transport Layer Security(SSL/TLS) certificates when you can get them for free? Many businesses continue paying annual fees to certificate providers out of habit, often spending hundreds of dollars annually for something now available at no cost. By moving your website security certificates to Amazon Route 53 using AWS Certificate Manager (ACM), you eliminate these recurring expenses while gaining automatic certificate renewal. Small businesses that make this switch remove the administrative burden of tracking expiration dates and manually renewing certificates. This straightforward change reduces both costs and security risks with minimal implementation effort.\n3. Optimize your log data retention Many businesses unknowingly store log data longer than necessary, increasing their cloud storage costs month after month. Many log management tools allow you to implement automated retention policies that align with your actual business requirements. By configuring appropriate log retention periods—rather than using unlimited default settings—you can substantially reduce your storage footprint while maintaining compliance. On AWS, Amazon CloudWatch makes this process straightforward with simple controls to retain critical security logs for 90 days while reducing operational logs to just 30 days. This customized approach typically reduces log storage costs compared to indefinite retention, and the automation keeps your team from having to manually delete outdated logs. For most compliance frameworks, 30-90 days of logs is sufficient rather than retaining years of rarely-accessed data.\nMid-term cost optimization strategies 1. Consolidate network resources while maintaining security As your business grows to serve multiple clients or departments, creating completely separate cloud environments for each seems like the safest approach. While this works initially, it multiplies your costs unnecessarily as you scale. Smart businesses are finding ways to share core network infrastructure while using virtual boundaries using a service such as Amazon Virtual Private Cloud (Amazon VPC) to keep client data securely separated. This balanced approach can help you accomplish your security and compliance goals while reducing redundant network components. Companies implementing this network consolidation strategy can experience a reduction in their monthly infrastructure costs without compromising client data isolation. The savings become increasingly significant as your business adds more clients, creating a more efficient growth model that maintains security while eliminating wasteful duplication.\n2. Optimize your content delivery costs Content delivery networks help websites load faster by caching content closer to users, but many small businesses pay for global coverage when their customers are concentrated in just a few regions. Reviewing your actual user geography and aligning your content delivery strategy accordingly can yield significant savings. For example, if your business primarily serves North American and European customers, you can select regional coverage options in Amazon CloudFront rather than the default global setting. This approach helps reduce your content delivery costs while maintaining fast performance in the markets that matter to your business. Geographic optimization creates immediate savings without any negative impact on your actual customer experience.\n3. Automate routine tasks with AI Time is money, especially for small businesses where teams wear multiple hats. Generative AI tools can now automate many repetitive tasks, freeing team capacity to focus on growth activities. For example, sales teams can use AI assistance to generate customized proposals based on previous successful bids, dramatically reducing the time spent on repetitive documentation. Similarly, customer service teams can deploy AI to handle routine inquiries, reducing response times while maintaining your brand voice across all customer interactions. By identifying these high-volume, repeatable tasks in your business, AI automation reduces labor costs while improving consistency and allowing your valuable human resources to focus on strategy and relationship-building.\nThis approach has proven effective for companies likeCreative Realities, Inc. Bart Massey, their EVP of Software Development, reports, “Using Amazon Q Business to build a private model for our product information has cut our RFP and RFI response times by over 50%, allowing us to respond faster to client requests from day one.” Their experience demonstrates how generative AI can significantly streamline business processes, leading to improved efficiency and responsiveness in customer interactions.\nBuilding cost efficiency in the long-term 1. Reduce software development costs Software development expenses can quickly consume your technology budget. Modern code assistance tools accelerate development by suggesting completions, identifying bugs early, and automating routine coding tasks. Amazon Q Developer exemplifies this approach, reducing costly rework by catching issues before they reach production and providing real-time guidance throughout the development process. This AI-powered assistant helps developers implement best practices, write secure code, and troubleshoot issues faster. When coupled with Infrastructure as Code (IaC) practices, businesses can standardize environments from development through production, eliminating time-consuming manual configurations and reducing provisioning time from days to minutes. Companies implementing these approaches can experience lower development costs while simultaneously accelerating their time to market.\n2. Automate infrastructure management Create resources when needed and remove them when not in use. Use AWS Lambda with Amazon EventBridge schedules to automatically shut down development environments after hours and restart them before the workday begins, thereby reducing non-production environment costs by running resources only during working hours. Another effective approach is automating scaling actions for predictable business cycles by employing AWS Auto Scaling groups to automatically increase capacity during busy periods and scale down during slower periods. These automation strategies can deliver cost reductions for targeted workloads.\n3. Rethink your system architecture Not every part of your business technology needs premium “always-on” protection. Apply a layered approach: invest in redundancy only for critical customer-facing components while using standard setups for internal tools. For example, a business can maintain high-availability only for client-facing systems using flexible deployment options from Amazon Relational Database Service (Amazon RDS). Similarly, moving completed client projects to archival tiers fromAmazon Simple Storage Service (Amazon S3) while maintaining accessibility can also reduce costs. For businesses with predictable busy periods, services like Amazon Aurora Serverless automatically adjust resources during peak times and scale down during quieter periods, generating savings compared to maintaining constant maximum capacity. By matching system reliability to actual business needs, most small businesses can reduce cloud costs without affecting critical\nConclusion Optimizing your cloud costs doesn’t have to mean compromising quality or capabilities. By implementing these strategies, small businesses can reduce expenses while continuing to leverage powerful technology to drive growth and innovation. Start with quick wins like optimizing storage and certificate management, then progress to more sophisticated approaches like network resource sharing and AI-powered automation. Each step you take will contribute to long-term savings and efficiency.\nRemember that cost optimization is an ongoing journey. As your business grows and evolves, continue to reassess your technology needs and adjust your strategy accordingly. With thoughtful planning and implementation of these approaches, you can build a cost-efficient, scalable technology foundation that supports your business objectives both today and in the future.\nTo get started on your cost-cutting journey with AWS, reach out to a sales specialist.",
    "description": "Tech-savvy savings: innovative ways to cut costs in your small businessn Henrique Trevisan, Jonathan Woods, and Vince Anderson | 23/4/2024 | in Best Practices, Permalink\nRunning a small business means making the most of every dollar while maintaining high-quality services. As your business grows and your cloud usage expands, you must find ways to efficiently manage technology resources to protect your bottom line. In today’s challenging economic environment, cost optimization has become a top priority for business owners who want to reduce expenses without sacrificing performance, security, or customer experience.",
    "tags": [],
    "title": "Blog 2",
    "uri": "/3-blogstranslated/3.2-blog2/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e EventParticipated",
    "content": "Event Summary: “AWS CLOUD MASTERY SERIES #3 – Security on AWS” Event Objectives A deep-dive sharing session that helps participants understand the 5 security pillars of the AWS Well-Architected Security Pillar and how to protect cloud systems against increasing threats.\nSharing about security-supporting services and system auditing methods to protect AWS accounts and systems from attacks.\nAn opportunity to connect with the First Cloud Journey community.\nSpeakers Le Vu Xuan An: AWS Cloud Club Captain HCMUTE, First Cloud AI Journey Tran Duc Anh: AWS Cloud Club Captain SGU, First Cloud AI Journey, Cloud Security Engineer Trainee Tran Doan Cong Ly: AWS Cloud Club Captain PTIT, First Cloud AI Journey Dang Hoang Hieu Nghi: AWS Cloud Club Captain HUFLIT, First Cloud AI Journey Nguyen Tuan Thinh: First Cloud AI Journey, Cloud Engineer Trainee Huynh Hoang Long: First Cloud AI Journey, Cloud Engineer Trainee Dinh Le Hoang Anh: First Cloud AI Journey, Cloud Engineer Trainee Nguyen Do Thanh Dat: First Cloud AI Journey, Cloud Engineer Trainee Van Hoang Kha: First Cloud AI Journey, Cloud Engineer Trainee Thinh Lam: First Cloud AI Journey Viet Nguyen: First Cloud AI Journey Mendel Grabski (Long): ex Head of Security \u0026 DevOps, Cloud Security Solutions Architect Tinh Truong: AWS Community Builder, Platform Engineer at TymeX Key Highlights Identity \u0026 Access Management Service Understand what IAM is used for Best practices for using IAM in system security SCP and permission boundaries for multi-account setups MFA, credential rotation, Access Analyzer Detection \u0026 Continuous Monitoring CloudTrail (org-level), GuardDuty, Security Hub Logging at all layers: VPC Flow Logs, ALB/S3 logs Alerting \u0026 automation with EventBridge Detection-as-Code (infrastructure + rules) Infrastructure Protection VPC segmentation, private vs public placement Security Groups vs NACLs: usage models WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics Data Protection KMS: key policies, grants, rotation Encryption at-rest \u0026 in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026 Parameter Store — rotation patterns Data classification \u0026 access guardrails Incident Response AWS Incident Response lifecycle Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response using Lambda/Step Functions Key Takeaways How to protect a personal AWS account Apply the Least Privilege Principle for IAM Users Do not use the root account for daily tasks Avoid using “*” in policy actions/resources Enable MFA for enhanced security Overview of GuardDuty An intelligent threat detection service that continuously monitors for malicious activities in AWS accounts and workloads. GuardDuty analyzes data from multiple sources including: AWS CloudTrail logs, VPC Flow Logs, DNS logs (Route 53 Resolver DNS query logs). Can use CloudFormation to build EventBridge rules that trigger Lambda functions and send SNS notifications when GuardDuty detects threats. Principles for building secure systems How to use Security Groups and NACLs to defend against external and internal threats. Understand common attack vectors to react and mitigate in time. Use services like AWS WAF, AWS Network Firewall, and AWS Shield Advanced (for DDoS protection) to strengthen system security both externally and internally. Using KMS to encrypt and protect data Create and manage encryption keys Control who can use or manage these keys Monitor all KMS-related activities with CloudTrail Event Experience Participating in the “AWS CLOUD MASTERY SERIES #3 – Security on AWS” workshop was a highly valuable experience, giving me a broader understanding of how to protect accounts and systems from basic to advanced levels. Some notable experiences:\nLearning from highly experienced speakers Speakers from the First Cloud Journey team and major tech organizations shared best practices for designing secure applications and preventing unwanted attacks. Through real-world case studies, I gained deeper insight into applying Cloud Security in large-scale projects. Networking and discussions The workshop provided opportunities to directly interact with mentors and peers from the First Cloud Journey program. Key takeaways When building a system, aside from infrastructure and architecture, it is crucial to focus on security from the smallest steps to prevent external threats and internal vulnerabilities. Tools like Amazon GuardDuty can be applied to optimize workload, reduce manual effort, and increase automation in existing systems.",
    "description": "Event Summary: “AWS CLOUD MASTERY SERIES #3 – Security on AWS” Event Objectives A deep-dive sharing session that helps participants understand the 5 security pillars of the AWS Well-Architected Security Pillar and how to protect cloud systems against increasing threats.\nSharing about security-supporting services and system auditing methods to protect AWS accounts and systems from attacks.\nAn opportunity to connect with the First Cloud Journey community.\nSpeakers Le Vu Xuan An: AWS Cloud Club Captain HCMUTE, First Cloud AI Journey Tran Duc Anh: AWS Cloud Club Captain SGU, First Cloud AI Journey, Cloud Security Engineer Trainee Tran Doan Cong Ly: AWS Cloud Club Captain PTIT, First Cloud AI Journey Dang Hoang Hieu Nghi: AWS Cloud Club Captain HUFLIT, First Cloud AI Journey Nguyen Tuan Thinh: First Cloud AI Journey, Cloud Engineer Trainee Huynh Hoang Long: First Cloud AI Journey, Cloud Engineer Trainee Dinh Le Hoang Anh: First Cloud AI Journey, Cloud Engineer Trainee Nguyen Do Thanh Dat: First Cloud AI Journey, Cloud Engineer Trainee Van Hoang Kha: First Cloud AI Journey, Cloud Engineer Trainee Thinh Lam: First Cloud AI Journey Viet Nguyen: First Cloud AI Journey Mendel Grabski (Long): ex Head of Security \u0026 DevOps, Cloud Security Solutions Architect Tinh Truong: AWS Community Builder, Platform Engineer at TymeX Key Highlights Identity \u0026 Access Management Service Understand what IAM is used for Best practices for using IAM in system security SCP and permission boundaries for multi-account setups MFA, credential rotation, Access Analyzer Detection \u0026 Continuous Monitoring CloudTrail (org-level), GuardDuty, Security Hub Logging at all layers: VPC Flow Logs, ALB/S3 logs Alerting \u0026 automation with EventBridge Detection-as-Code (infrastructure + rules) Infrastructure Protection VPC segmentation, private vs public placement Security Groups vs NACLs: usage models WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics Data Protection KMS: key policies, grants, rotation Encryption at-rest \u0026 in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026 Parameter Store — rotation patterns Data classification \u0026 access guardrails Incident Response AWS Incident Response lifecycle Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response using Lambda/Step Functions Key Takeaways How to protect a personal AWS account Apply the Least Privilege Principle for IAM Users Do not use the root account for daily tasks Avoid using “*” in policy actions/resources Enable MFA for enhanced security Overview of GuardDuty An intelligent threat detection service that continuously monitors for malicious activities in AWS accounts and workloads. GuardDuty analyzes data from multiple sources including: AWS CloudTrail logs, VPC Flow Logs, DNS logs (Route 53 Resolver DNS query logs). Can use CloudFormation to build EventBridge rules that trigger Lambda functions and send SNS notifications when GuardDuty detects threats. Principles for building secure systems How to use Security Groups and NACLs to defend against external and internal threats. Understand common attack vectors to react and mitigate in time. Use services like AWS WAF, AWS Network Firewall, and AWS Shield Advanced (for DDoS protection) to strengthen system security both externally and internally. Using KMS to encrypt and protect data Create and manage encryption keys Control who can use or manage these keys Monitor all KMS-related activities with CloudTrail Event Experience Participating in the “AWS CLOUD MASTERY SERIES #3 – Security on AWS” workshop was a highly valuable experience, giving me a broader understanding of how to protect accounts and systems from basic to advanced levels. Some notable experiences:",
    "tags": [],
    "title": "Event 2",
    "uri": "/4-eventparticipated/4.2-event2/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase provisions the data layer components, service discovery mechanism, and load balancing infrastructure.\nSSL Certificate Provisioning Request ACM Certificate\nSwitch region to Singapore (ap-southeast-1) Navigate to Certificate Manager (ACM) Select Request a certificate → Request a public certificate Configure certificate request: Domain name: sgutodolist.com Validation method: DNS validation Click Request Access the certificate details → Under Domains section → Click Create records in Route 53 Wait for status to change to Issued (typically 5-10 minutes) RDS MYSQL (Database) Goal: Create MySQL 8.0 Database located in Private Subnet.\nGo to RDS \u003e Create database.\nChoose a database creation method: Full Configuration (To customize).\nEngine options: Select MySQL.\nEngine Version: Select 8.0.x (For example 8.0.35 or 8.0.39) to match Docker Compose.\nTemplates: Select Free tier.\nSettings:\nDB Instance identifier: sgu-todolist-db\nMaster username: root\nMaster password: 12345678 (Example).\nInstance configuration: Select db.t3.micro (or t2.micro if the account is old).\nConnectivity (IMPORTANT):\nCompute resource: Don’t connect to an EC2 compute resource.\nVPC: Select SGU-Microservices-vpc.\nDB Subnet group: Select Create new (or select the existing one pointing to 2 Private Subnets).\nPublic access: NO (Because the DB is in the Private area).\nVPC security group: Select private-db-sg (Uncheck default).\nAvailability Zone: Select ap-southeast-1a.\nAdditional configuration: Initial database name: aws_todolist_database (Enter it here to avoid having to run the CREATE DATABASE command manually, if desired). However, it’s best to leave it blank and use the Bastion created later to be sure.\nUncheck Enable automated backups (to save storage space if only testing).\nClick Create database. 👉 After creating (Status: Available), copy Endpoint.\nELASTICACHE REDIS Goal: Create a simple, cheap Redis t3.micro, without using Cluster mode.\nGo to ElastiCache \u003e Redis OSS caches \u003e Create Redis OSS cache.\nCluster settings:\nEngine: Redis OSS.\nDeployment option: Node-based cluster (This must be selected to adjust the configuration).\nCreation method: Cluster cache.\nCluster info: Cluster mode: Disabled.\nName: sgu-redis.\nCache settings: Node type: cache.t3.micro.\nNumber of replicas: 0.\nConnectivity: Subnet groups: Select Create new (if not already there).\nName: sgu-redis-subnet-group.\nVPC: SGU-Microservices-vpc.\nSubnets: Select 2 Private Subnets.\nVPC security groups: Select private-db-sg.\nAvailability Zone placements: Select Specify Availability Zones -\u003e ap-southeast-1a.\nClick Next.\nAdvanced settings:\nEnable automatic backups: Uncheck (Save money).\nLogs: Disable all.\nScroll to the bottom and click Create. After creating, copy Primary Endpoint.\nAWS CLOUD MAP (Service Discovery) Goal: Create an internal .local domain for Kafka and AI Service to talk to each other without a Load Balancer.\nGo to Cloud Map service.\nClick Create namespace.\nNamespace name: sgu.local.\nDescription: SGU Internal Network.\nInstance discovery: Select API calls and DNS queries in VPCs.\nVPC: Select SGU-Microservices-vpc.\nClick Create namespace.\nALB ROUTING (Target Groups \u0026 Load Balancer) This is the most important routing part for users to access the system.\n4.1 Create 5 Target Groups (Repeat 5 times) Go to EC2 \u003e Target Groups \u003e Create target group.\nCommon configuration for all 5:\nTarget type: IP addresses (Required for ECS Fargate).\nProtocol: HTTP.\nIP address type: IPv4.\nVPC: SGU-Microservices-vpc.\nHealth check path: /actuator/health.\nNote: In the next “Register targets” step, DO NOT enter any IP, click Create.\nList of 5 Target Groups to create:\nName Port Health Check Path auth-tg 9999 /api/auth/actuator/health user-tg 8081 /api/user/actuator/health task-tg 8082 /api/taskflow/actuator/health noti-tg 9998 /api/notification/actuator/health 4.2 Create Application Load Balancer (ALB) Go to EC2 \u003e Load Balancers \u003e Create load balancer \u003e Application Load Balancer.\nBasic configuration: Name: sgu-alb.\nScheme: Internet-facing.\nIP address type: IPv4.\nNetwork mapping: VPC: SGU-Microservices-vpc.\nMappings: Select 2 Public Subnets (For example ...public1... and ...public2...). ⚠️ If you get this wrong, the whole thing will collapse.\nSecurity groups: Select public-alb-sg. Listeners and routing (Create 2): HTTP:80: Forward to api-gateway-tg.\nHTTPS:443 (Click Add listener):\nDefault action: Forward to api-gateway-tg.\nSecure listener settings: Select ACM certificate sgutodolist.com\nImportant: Để mọi service phải thông qua api-gateway thì phải set Listener rules của Protocol:Port HTTPS:443 là: /api/auth/* (vì auth thì có thể đi 1 luồng riêng) và Default\nEC2 \u003e Load balancers \u003e sgu-alb \u003e HTTPS:443 listener Ở mục Listener rules xóa các Rule có điều kiện Path là /api/\u003cservice\u003e/* ⬅ STEP 1: Network \u0026 Security Preparation\rSTEP 3: Code Update \u0026 Image Build ➡",
    "description": "This phase provisions the data layer components, service discovery mechanism, and load balancing infrastructure.\nSSL Certificate Provisioning Request ACM Certificate\nSwitch region to Singapore (ap-southeast-1) Navigate to Certificate Manager (ACM) Select Request a certificate → Request a public certificate Configure certificate request: Domain name: sgutodolist.com Validation method: DNS validation Click Request Access the certificate details → Under Domains section → Click Create records in Route 53 Wait for status to change to Issued (typically 5-10 minutes) RDS MYSQL (Database) Goal: Create MySQL 8.0 Database located in Private Subnet.",
    "tags": [],
    "title": "Infrastructure \u0026 ALB Setup (RDS, Redis, Cloud Map, ALB Routing)",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.2-infrastructure--alb-setup-rds-redis-cloud-map-alb-routing/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop",
    "content": "Preamble: Prerequisites for Backend Deployment Before initiating the deployment steps outlined in the Table of Contents, the following foundational tools, configurations, and AWS service setups must be completed to ensure a smooth, secure, and cost-optimized deployment into the ap-southeast-1 (Singapore) region.\nStep Requirement Purpose 1 AWS Account \u0026 CLI Access and command-line management. 2 Domain \u0026 SSL Securing traffic and global content delivery. 3 Source Code \u0026 Artifacts Building the deployable microservice images. 4 AWS Network Foundation Setting up the isolated, highly available VPC structure. I. Tools \u0026 Credentials AWS Account Setup: A valid AWS Account is required, and initial billing alerts (AWS Budgets) must be configured to monitor Free Tier usage.\nAWS CLI: AWS Command Line Interface must be installed and configured with appropriate IAM User credentials granting necessary permissions (EC2, RDS, S3, CloudFront, Route 53, IAM, CloudWatch).\nDocker: Docker Engine must be installed locally to build the container images for the Spring Boot microservices.\nJava/Maven: Local environment must be set up to compile and package the Spring Boot application code.\nII. Domain, SSL, and DNS Preparation Registered Domain: A domain name must be registered (e.g., via Route 53 or an external registrar).\nRoute 53 Hosted Zone: The domain must be managed within an Amazon Route 53 Hosted Zone.\nACM Certificate: An AWS Certificate Manager (ACM) certificate must be provisioned for the domain (e.g., *.taskmanager.com) in two regions:\nus-east-1 (N. Virginia): Required for CloudFront (Global Service).\nap-southeast-1 (Singapore): Required for the Application Load Balancer (ALB).\nIII. Network Foundation (VPC \u0026 Subnets) A VPC must be created in ap-southeast-1 with the following structure for High Availability (HA) across at least two Availability Zones (AZs):\nVPC: One primary VPC (e.g., CIDR 10.0.0.0/16).\nPublic Subnets (Multi-AZ): To host the Application Load Balancer (ALB) and Internet Gateway (IGW).\nPrivate Subnets (Multi-AZ): To host the application instances (EC2/ECS), RDS, and ElastiCache.\nNAT Gateway: Deployed in at least one Public Subnet to allow resources in the Private Subnets (EC2 instances) to securely access the internet for updates and communication with AWS services (like S3/ECR).\nSecurity Groups (SG): Initial Security Groups must be defined following the principle of least privilege:\nALB SG: Allows inbound traffic on HTTP (80) and HTTPS (443) from the internet.\nEC2/ECS SG: Allows inbound traffic only from the ALB SG on the application port (e.g., 8080).\nRDS/ElastiCache SG: Allows inbound traffic only from the EC2/ECS SG on their respective ports (e.g., MySQL 3306, Redis 6379).\nIV. Data \u0026 Storage Pre-Configuration S3 Primary Bucket (ap-southeast-1): An S3 bucket must be created in ap-southeast-1 to hold static assets and application deployment artifacts (JARs/Docker Images).\nS3 DR Bucket (us-east-1): A secondary S3 bucket must be created in us-east-1 (N. Virginia) to serve as the Disaster Recovery target.\nCross-Region Replication (CRR): S3 CRR must be configured to automatically replicate objects from the primary bucket in ap-southeast-1 to the DR bucket in us-east-1.\nV. Compute \u0026 Service Preparation Docker Images: The Spring Boot microservices must be built into production-ready Docker images.\nECR Repository (Optional): If using ECS, a repository must be prepared in ECR (Elastic Container Registry) to store the Docker images.\nIAM Roles: Necessary IAM Roles must be created for:\nEC2/ECS Tasks: Grants permissions to access S3, CloudWatch, and the RDS database.\nALB: Allows the load balancer to perform its function.",
    "description": "Preamble: Prerequisites for Backend Deployment Before initiating the deployment steps outlined in the Table of Contents, the following foundational tools, configurations, and AWS service setups must be completed to ensure a smooth, secure, and cost-optimized deployment into the ap-southeast-1 (Singapore) region.\nStep Requirement Purpose 1 AWS Account \u0026 CLI Access and command-line management. 2 Domain \u0026 SSL Securing traffic and global content delivery. 3 Source Code \u0026 Artifacts Building the deployable microservice images. 4 AWS Network Foundation Setting up the isolated, highly available VPC structure. I. Tools \u0026 Credentials AWS Account Setup: A valid AWS Account is required, and initial billing alerts (AWS Budgets) must be configured to monitor Free Tier usage.",
    "tags": [],
    "title": "Prerequisite",
    "uri": "/5-workshop/5.2-prerequisite/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "Cost-Optimized SaaS Task Management Platform Single-Region High Availability with ECS Fargate 1. Executive Summary The SaaS Task Management Platform is designed to deliver a Todoist-like collaborative experience with High Availability (HA) and Cost Efficiency, specifically optimized for AWS Free Tier constraints.\nInitially, the project targeted a Multi-Region deployment to achieve the high levels of global performance, disaster recovery (DR), and uptime. However, due to the cost limitations and the constraints of the AWS Free Tier (particularly concerning cross-region data transfer and running multiple primary/replica database instances), the architecture was strategically consolidated into a Single AWS Region (ap-southeast-1 - Singapore).\nBuilt on Spring Boot microservices deployed as ECS Fargate containers, the platform now leverages Multi-AZ deployment within Singapore for resilience while eliminating server management overhead.\nKey Architecture Highlights:\nServerless Compute: ECS Fargate eliminates EC2 instance management\nRegional High Availability: Multi-AZ deployment for compute (ECS), database (RDS), and caching (ElastiCache)\nLow Latency in SEA: Optimized performance for Southeast Asian users\nGlobal Content Delivery: CloudFront CDN for worldwide static asset distribution\nCost Predictability: Architecture designed to maximize AWS Free Tier benefits\nDisaster Recovery: S3 Cross-Region Replication to us-east-1 for data backup\nThe result is a production-ready, cost-effective platform that balances performance, availability, and operational simplicity—ideal for MVP development and regional deployment, while maintaining a clear roadmap for future Multi-Region expansion.\n2. Problem Statement \u0026 Solution 2.1. The Challenge: Balancing Performance, Availability, and Cost Traditional SaaS platforms face critical trade-offs when operating on limited budgets:\nOperational Complexity:\nEC2-based deployments require constant server management (patching, monitoring, capacity planning)\nManual scaling decisions lead to over-provisioning (wasted money) or under-provisioning (poor performance)\nComplex deployment processes prone to human error and downtime\nCost Challenges:\nMulti-region architectures introduce expensive cross-region data transfer\nRunning RDS Read Replicas in multiple regions quickly exhausts Free Tier limits\nNAT Gateway charges ($35+/month) consume significant portions of small budgets\nAlways-on EC2 instances waste capacity during off-peak hours\nAvailability vs. Budget:\nSingle-region deployments risk total outage during regional failures\nMulti-AZ configurations increase costs but are essential for production workloads\nBalancing HA requirements with Free Tier constraints requires careful architecture\n2.2. The Single-Region ECS Solution This project adopts a Single-Region (ap-southeast-1) High Availability architecture using ECS Fargate for serverless compute, optimizing for cost while maintaining production-grade reliability.\nCore Architecture Decisions:\nCompute Layer (Serverless):\nECS Fargate for containerized Spring Boot microservices (no EC2 management)\nApplication Load Balancer for traffic distribution and health checks\nAWS Cloud Map for service discovery between microservices\nAuto Scaling based on CPU/memory metrics\nData Layer (Regional + Multi-AZ):\nRDS MySQL (db.t3.micro) with Multi-AZ for automated failover\nElastiCache Redis (cache.t3.micro) for session management and caching\nS3 with Cross-Region Replication to us-east-1 for disaster recovery\nNetwork Layer (Global + Regional):\nVPC with public/private subnets across 2 Availability Zones\nCloudFront CDN for global static asset delivery\nRoute 53 for DNS management and domain hosting\nVPC Endpoints to eliminate NAT Gateway costs ($35/month savings)\nSecurity \u0026 Observability:\nSecurity Groups for network-level access control\nIAM Roles for service-to-service authentication (no hardcoded credentials)\nOrigin Access Control (OAC) to secure S3 buckets\nCloudWatch for centralized logging, metrics, and alarms\nKey Advantages Over Traditional Architecture:\nAspect Traditional EC2 ECS Fargate (This Project) Server Management Manual patching, monitoring, SSH access Fully managed, no server access needed Scaling Manual ASG configuration Automatic based on metrics Deployment Complex, multi-step process Rolling updates with zero downtime Cost Model Pay for running instances (24/7) Pay only for container runtime Free Tier 750 EC2 hours/month 20 GB-hours + 50 GB data transfer Startup Time 2-5 minutes (AMI boot) 30-60 seconds (container start) 2.3. Benefits and ROI Technical Benefits:\n80% reduction in operational overhead (no server management)\n99.5%+ uptime with Multi-AZ deployment for RDS and ECS\n\u003c100ms API response time for cached requests\n2-5 minute recovery time for AZ failures (automatic)\nCost Benefits:\n~$25-30/month operational cost (vs $50-90 with EC2)\nVPC Endpoints eliminate $35/month NAT Gateway charges\nPay-per-use pricing—only charged for actual container runtime\nFree Tier optimized to stay within budget constraints\nLearning \u0026 Career Value:\nHands-on experience with modern cloud-native architecture\nContainer orchestration expertise (Docker, ECS Fargate)\nDevOps practices: IaC, CI/CD, monitoring, incident response\nPortfolio project demonstrating AWS Solutions Architect competencies\n3. Solution Architecture Figure 1. Single-Region ECS Fargate Architecture\r3.1. Architecture Overview Primary Deployment Region: ap-southeast-1 (Singapore)\nCompute \u0026 Orchestration:\nECS Fargate Cluster running 5 containerized Spring Boot microservices\nApplication Load Balancer distributing traffic across ECS tasks (Multi-AZ)\nAWS Cloud Map for internal service discovery (private DNS namespace)\nAuto Scaling policies targeting 70% CPU, 80% memory utilization\nData \u0026 Caching:\nRDS MySQL (db.t3.micro) with Multi-AZ deployment for automatic failover\nElastiCache Redis (cache.t3.micro) for session storage and hot data caching\nS3 bucket for user files, attachments, and static assets\nNetwork Infrastructure:\nVPC (10.0.0.0/16) with public and private subnets across 2 Availability Zones\nInternet Gateway for public subnet (ALB) internet connectivity\nVPC Endpoints for S3, ECR, CloudWatch (eliminates NAT Gateway cost)\nSecurity Groups controlling all traffic flows between components\nGlobal Services:\nContent Delivery:\nCloudFront CDN with global edge locations for static asset distribution\nOrigin: Primary S3 bucket in Singapore, failover to us-east-1\nOrigin Access Control (OAC) ensuring S3 is only accessible via CloudFront\nDisaster Recovery:\nS3 Cross-Region Replication (CRR) to us-east-1 (N. Virginia)\nRDS Automated Snapshots (7-day retention, daily backups)\nDNS \u0026 Security:\nRoute 53 for domain hosting and DNS management\nACM (AWS Certificate Manager) for free SSL/TLS certificates\nCloudWatch for centralized logs, metrics, and alarms\n3.2. Microservices Architecture The platform implements 5 core Spring Boot microservices deployed as ECS Fargate tasks:\nService Port Responsibilities Container Resources API Gateway 8080 API Traffic Routing, Request Aggregation/Validation 0.5 vCPU, 1GB RAM User Service 8081 User profiles, settings, avatar management 0.5 vCPU, 1GB RAM Taskflow Service 8082 Board/workspace creation, Task CRUD, assignments, comments, attachments 0.5 vCPU, 1GB RAM Notification Service 9998 Real-time notifications, email (SES), WebSockets 0.5 vCPU, 1GB RAM Auth Service 9999 User authentication, JWT tokens, OAuth2 (Google) 0.5 vCPU, 1GB RAM Service Communication Patterns:\nExternal Traffic (Users):\nUser → Route 53 → CloudFront (static) OR ALB (API)\r→ ALB Path-Based Routing:\r/api/* → API Gateway Target Group (Port 8080)\rAPI Gateway Internal Routing (Port-based, via Cloud Map):\r/auth/* → Auth Service (Port 9999)\r/users/* → User Service (Port 8081)\r/taskflow/* → Taskflow Service (Port 8082)\r/notifications/* → Notification Service (Port 9998)\rInternal Service-to-Service:\nService A → AWS Cloud Map (service-b.local:PORT)\r→ Direct container-to-container communication\r→ Response cached in Redis (5-minute TTL)\rContainer Configuration:\nBase Image: eclipse-temurin:21-jre-alpine (lightweight JRE)\nHealth Check: Spring Actuator /actuator/health/liveness endpoint (used by ALB)\nLogging: CloudWatch Logs with 7-day retention\nEnvironment Variables: Database credentials, Redis endpoint, S3 bucket name\nIAM Task Role: Permissions for S3, SES, CloudWatch access\n3.3. AWS Services Used Category Services Purpose Cost Optimization Compute ECS Fargate, ALB Serverless container orchestration, load balancing Free Tier: 750 ALB hours, 20 GB Fargate hours Database RDS MySQL (db.t3.micro) Primary database with Multi-AZ HA Fixed cost: ~$15/month (exceeds Free Tier) Caching ElastiCache Redis (cache.t3.micro) Session store, API response caching Free Tier: 750 node hours Storage S3 Standard + CRR Object storage with DR replication Free Tier: 5GB storage, 20k GET, 2k PUT CDN CloudFront Global content delivery Free Tier: 1TB data transfer, 10M requests Container Registry ECR Docker image storage Free Tier: 500MB storage Service Discovery AWS Cloud Map Microservices service registry $0.50/namespace + $0.10/instance/month Networking VPC, VPC Endpoints Network isolation, private AWS service access VPC free, Endpoints ~$7/month (replaces $35 NAT) DNS Route 53 Domain hosting $0.50/hosted zone Security ACM, IAM, Security Groups SSL certificates, access control Free Observability CloudWatch Logs, Metrics, Alarms Monitoring, logging, alerting Free Tier: 5GB logs, 10 custom metrics 4. Service Roles Overview AWS Service Role in Architecture Configuration Details Route 53 DNS hosting for custom domain, SSL certificate validation Hosted zone: sgutodolist.com, Health checks for ALB CloudFront Global CDN serving static assets from edge locations Origin: S3 Singapore (primary), S3 Virginia (failover) ACM Free SSL/TLS certificates for HTTPS Wildcard cert: *.sgutodolist.com, auto-renewal enabled VPC Isolated network for all resources CIDR: 10.0.0.0/16, 2 public + 2 private subnets across 2 AZs Internet Gateway Enables ALB to receive traffic from internet Attached to public subnets only VPC Endpoints Private connections to AWS services (no NAT Gateway) S3 Gateway Endpoint, Interface Endpoints for ECR/CloudWatch Application Load Balancer Layer 7 routing, SSL termination, health checks Multi-AZ, single routing rule to API Gateway Target Group ECS Fargate Serverless container runtime (no EC2 management) Runs Spring Boot containers with varying ports ECS Cluster Logical grouping of ECS services and tasks Single cluster: sgutodolist-cluster ECS Service Maintains desired task count, integrates with ALB Desired count: 2 tasks/service, rolling deployment strategy ECS Task Definition Blueprint for containers (image, resources, environment) 5 task definitions (one per microservice) AWS Cloud Map Service discovery via private DNS Namespace: local, services: api-gateway.local, auth-service.local, etc. RDS MySQL Primary relational database with Multi-AZ db.t3.micro, 20GB storage, automated backups enabled ElastiCache Redis In-memory cache for sessions and API responses cache.t3.micro, cluster mode disabled S3 (Primary) User files, attachments, static frontend assets Bucket: sgutodolist-assets-sg (ap-southeast-1) S3 (Secondary) Disaster recovery replica Bucket: sgutodolist-assets-us (us-east-1) S3 CRR Automated async replication Singapore → Virginia Replication rule for all objects, versioning enabled Origin Access Control Restricts S3 access to CloudFront only Blocks direct public access to S3 buckets Security Groups Virtual firewall for ALB, ECS tasks, RDS, Redis Least-privilege rules, source-based restrictions IAM Task Execution Role Allows ECS to pull images from ECR, write logs Permissions: ECR pull, CloudWatch Logs write IAM Task Role Allows containers to access S3, SES, etc. Permissions: S3 read/write, SES send, CloudWatch metrics CloudWatch Logs Centralized application logs from ECS containers 7-day retention, 5 log groups (one per service) CloudWatch Metrics Performance metrics (CPU, memory, request count) Custom metrics for business KPIs CloudWatch Alarms Alerting for high CPU, failed tasks, budget thresholds SNS integration for email notifications 5. Service Flow 5.1. User Request Flow Static Assets (Frontend - HTML, CSS, JS, Images):\nUser accesses https://sgutodolist.com\nRoute 53 resolves DNS to CloudFront distribution\nCloudFront checks nearest edge location cache:\nCache HIT: Returns asset immediately (latency \u003c20ms)\nCache MISS: Fetches from S3 origin (Singapore), caches for 24 hours\nBrowser loads frontend application\nStatic assets served with low latency globally via edge locations\nAPI Requests (Backend Microservices):\nFrontend makes API call: https://sgutodolist.com/api/task\nRoute 53 resolves to Application Load Balancer in Singapore\nALB performs SSL termination and routes all /api/* traffic to the API Gateway Target Group (Port 8080).\nAPI Gateway (running on ECS) receives the request and internally routes via AWS Cloud Map based on the path:\nRequests to /api/auth/** are routed to Auth Service (Port 9999).\nRequests to /api/user/** are routed to User Service (Port 8081).\nRequests to /api/taskflow/** are routed to Taskflow Service (Port 8082).\nRequests to /api/notifications/** are routed to Notification Service (Port 9998).\nThe target microservice processes the request.\nData Access Pattern (90% Reads, 10% Writes):\nRead Operations:\nSpring Boot service receives request\nCheck ElastiCache Redis for cached data:\nCache HIT: Return immediately (latency \u003c5ms)\nCache MISS: Query RDS MySQL (Multi-AZ), cache result with TTL\nResponse returned through ALB to user\nTotal latency: 5-10ms (cached) or 20-50ms (database query)\nWrite Operations:\nSpring Boot service validates request\nWrite to RDS MySQL primary database\nRDS synchronously replicates to standby instance (same region, different AZ)\nInvalidate/update related cache entries in Redis\nAsync: Trigger notifications, update search index\nSuccess response to user\nTotal latency: 50-100ms\nService-to-Service Communication:\nExample: API Gateway needs to validate a JWT token via Auth Service\nAPI Gateway queries AWS Cloud Map: http://auth-service.local:9999/validate\nCloud Map resolves to healthy Auth Service task private IP\nDirect container-to-container communication within VPC (no ALB overhead)\nResponse cached in Redis for 5 minutes to reduce repeated calls\nLatency: 10-20ms for first call, \u003c5ms for subsequent cached calls\n5.2. Developer Deployment Flow CI/CD Pipeline (Manual Process, Automatable with CodePipeline):\nPhase 1: Local Development\nDeveloper updates Spring Boot microservice code\nRun unit tests: ./mvnw test\nRun integration tests with Docker Compose (local MySQL + Redis)\nCommit changes to Git repository\nPhase 2: Container Build\nBuild Spring Boot JAR: ./mvnw clean package\nBuild Docker image:\nFROM eclipse-temurin:21-jre-alpineWORKDIR /appCOPY target/task-service.jar app.jarEXPOSE 8084HEALTHCHECK CMD curl -f http://localhost:8084/actuator/health || exit 1ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\rTag image: task-service:v1.2.3\nPhase 3: Push to ECR\n# Authenticate to ECR\raws ecr get-login-password --region ap-southeast-1 |\\\rdocker login --username AWS --password-stdin {account-id}.dkr.ecr.ap-southeast-1.amazonaws.com\r# Tag and push\rdocker tag task-service:v1.2.3\\\r{account-id}.dkr.ecr.ap-southeast-1.amazonaws.com/task-service:v1.2.3\rdocker push {account-id}.dkr.ecr.ap-southeast-1.amazonaws.com/task-service:v1.2.3\rPhase 4: Update ECS Task Definition\nNavigate to ECS console → Task Definitions → task-service\nCreate new revision:\nUpdate container image to :v1.2.3\nReview environment variables (DB_HOST, REDIS_HOST, etc.)\nVerify resource allocation (0.5 vCPU, 1GB RAM)\nRegister new task definition revision\nPhase 5: Deploy to ECS Service (Rolling Update)\nNavigate to ECS Service → task-service\nUpdate service to use new task definition revision\nECS performs automatic rolling update:\nLaunch 2 new tasks with updated image\nWait for tasks to pass ALB health checks (3 consecutive passes)\nALB starts routing 50% traffic to new tasks\nDrain connections from old tasks (30-second timeout)\nStop old tasks once fully drained\nTotal deployment time: 3-5 minutes (zero downtime)\nPhase 6: Verification \u0026 Monitoring\nCheck ECS Service events: aws ecs describe-services\nMonitor CloudWatch Logs for errors: /ecs/task-service\nTest API endpoints via ALB: curl https://sgutodolist.com/api/task/health\nCheck CloudWatch Metrics: CPU, memory, request count, error rate\nMonitor ALB Target Health in console\nRollback Procedure (If Issues Detected):\nIdentify last known good task definition revision (e.g., v1.2.2)\nUpdate ECS Service to use previous revision\nECS performs automatic rollback deployment (3-5 minutes)\nVerify health checks pass and logs show no errors\nInvestigate issue in lower environments before next deployment\nFuture Automation (Phase 2 Roadmap):\nGitHub Actions: Trigger builds on push to main branch\nAWS CodePipeline: Source (GitHub) → Build (CodeBuild) → Deploy (ECS)\nBlue/Green Deployments: Use CodeDeploy for safer production rollouts\nAutomated Testing: Integration tests run before deployment\n5.3. Data Replication \u0026 Storage Strategy Data Storage \u0026 Backup Matrix:\nData Type Primary Storage Replication Method Backup RPO RTO Transactional Data (users, boards, tasks) RDS MySQL Multi-AZ Synchronous (within AZs) Automated snapshots (daily, 7-day retention) \u003c1 minute 10-20 minutes (AZ failure), 2-4 hours (region failure) Session Tokens ElastiCache Redis None (ephemeral) None (regenerate on failure) N/A Immediate (users re-authenticate) User Files (attachments, avatars) S3 Singapore CRR to S3 Virginia (async) Versioning (30-day retention) 5-15 minutes Immediate (CloudFront failover) Static Assets (frontend code) S3 Singapore CRR to S3 Virginia (async) Versioning + git repository 5-15 minutes Immediate (CloudFront failover) Application Logs CloudWatch Logs Regional replication (managed by AWS) 7-day retention N/A N/A Docker Images ECR Singapore Manual push to other regions if needed Image versioning N/A Minutes (pull from ECR) S3 Cross-Region Replication Configuration:\nSource Bucket: sgutodolist-assets-sg (ap-southeast-1)\nDestination Bucket: sgutodolist-assets-us (us-east-1)\nReplication Rules:\nReplicate all objects (prefix: /)\nReplicate metadata, tags, and object ACLs\nDo NOT replicate delete markers (prevent accidental data loss)\nPriority: 1 (highest)\nVersioning: Enabled on both source and destination\nExpected Latency: 5-15 minutes for most objects (\u003c1MB)\nUse Case: Disaster recovery + CloudFront origin failover\n5.4. High Availability \u0026 Disaster Recovery Failure Scenarios \u0026 Automated Response:\nScenario 1: Single ECS Task Failure\nDetection: ALB health check fails (3 consecutive failures at 10-second intervals)\nAutomatic Response:\nALB stops routing new requests to unhealthy task\nExisting connections drained (30-second timeout)\nECS Service Controller launches replacement task automatically\nImpact: Zero user-facing downtime (other tasks handle load)\nRecovery Time: 2-3 minutes (container start + health check pass)\nUser Experience: No interruption\nScenario 2: Complete Service Failure (All Tasks Unhealthy)\nDetection: All 2 tasks for a service fail health checks, ALB returns 503 errors\nAutomatic Response: ECS attempts to launch replacement tasks\nManual Intervention Required:\nCheck CloudWatch Logs for root cause (database connection timeout, memory leak, bad deployment)\nIf bad deployment: Rollback to previous task definition revision\nIf infrastructure issue: Check RDS/Redis connectivity, Security Groups\nImpact: Service unavailable until new tasks healthy or issue resolved\nRecovery Time: 5-15 minutes (depends on issue complexity)\nMitigation: Implement circuit breakers, graceful degradation\nScenario 3: Availability Zone Failure (e.g., AZ-1a Goes Down)\nDetection: All tasks in AZ-1a become unreachable, ALB marks them unhealthy\nAutomatic Response:\nALB immediately routes all traffic to tasks in healthy AZ (AZ-1b)\nECS Service launches replacement tasks in healthy AZs to restore desired count\nRDS Multi-AZ: If primary DB in failed AZ, RDS automatically fails over to standby (60-120 seconds)\nImpact:\n30-50% capacity reduction for 5-10 minutes (temporary latency increase)\nPossible 60-120 second database unavailability during RDS failover\nRecovery Time: 5-10 minutes for full capacity restoration\nUser Experience: Slight performance degradation, no data loss\nScenario 4: RDS Primary Database Failure\nDetection:\nRDS health checks fail\nApplication logs show connection timeouts to database\nCloudWatch alarm triggers: DatabaseConnections metric drops to 0\nAutomatic Response:\nRDS Multi-AZ automatically promotes standby instance to primary\nDNS endpoint (sgutodolist-db.xxxxx.ap-southeast-1.rds.amazonaws.com) updated to point to new primary\nApplication connection pools automatically reconnect (Spring Boot retry logic)\nImpact: 60-120 seconds of database write unavailability\nRecovery Time: 1-2 minutes (fully automated)\nData Loss: Zero (synchronous replication to standby)\nScenario 5: Complete Regional Failure (Singapore Outage)\nDetection:\nRoute 53 health checks fail for Singapore ALB\nCloudWatch alarms trigger: All ECS tasks unreachable\nManual verification: AWS Service Health Dashboard confirms regional issue\nManual DR Process:\nOption A: Read-Only Mode (15-30 minutes)\nUpdate CloudFront origin to use S3 Virginia bucket (static assets still work)\nDisplay maintenance page to users: “Service temporarily unavailable”\nWait for AWS to restore Singapore region\nOption B: Full Recovery to New Region (2-4 hours)\nRestore latest RDS snapshot to new region (us-east-1):\naws rds restore-db-instance-from-db-snapshot \\ --db-instance-identifier sgutodolist-dr \\ --db-snapshot-identifier rds:sgutodolist-db-2024-12-07-00-00 \\ --db-instance-class db.t3.micro \\ --availability-zone us-east-1a\rCreate new ECS Fargate cluster in us-east-1\nDeploy all 5 microservices using existing task definitions (update DB endpoint)\nCreate new ALB in us-east-1, configure target groups\nUpdate Route 53 to point to new ALB\nUpdate CloudFront origin to use new ALB for API calls\nImpact: Full service outage during recovery\nRecovery Time: 2-4 hours (manual process)\nData Loss: Last 5-60 minutes (RDS automated backups every 5 minutes, snapshots hourly)\nCost: Additional resources in us-east-1 (~$30/month if kept running)\nDisaster Recovery Metrics:\nRPO (Recovery Point Objective): \u003c1 hour (RDS automated backups)\nRTO (Recovery Time Objective):\nAZ failure: 5-10 minutes (automatic)\nRegional failure: 2-4 hours (manual)\nAvailability Target: 99.5% (43 minutes downtime/month allowance)\n6. Budget Estimation 6.1. Monthly Cost Breakdown (Free Tier Optimized) AWS Service Specification Free Tier Allocation Expected Usage Cost/Month ECS Fargate 5 services x 2 tasks x 0.5 vCPU, 1GB RAM 20 GB-hours/month 10 tasks x 24h x 30d = 7,200 GB-hours $0 (Month 1), ~$36 after Free Tier RDS MySQL db.t3.micro, Multi-AZ, 20GB storage 750 hours Single-AZ only 744 hours Multi-AZ $15.00 (fixed cost) ElastiCache Redis cache.t3.micro, single node 750 node hours 744 hours $0 (Free Tier) Application Load Balancer Standard ALB, minimal LCUs 750 hours + 15 LCUs 744 hours, 10 LCUs $0 (Free Tier) S3 Storage Standard class + CRR 5GB storage, 20k GET, 2k PUT 10GB storage, 30k GET, 5k PUT, CRR $2.00 CloudFront Global edge locations 1TB data transfer, 10M requests 50GB data transfer, 2M requests $0 (Free Tier) Route 53 Hosted zone + queries First 25 zones discounted 1 hosted zone, 1M queries $0.50 VPC Endpoints S3 Gateway + ECR/CloudWatch Interface None 3 endpoints x 24h x 30d $7.00 ECR Docker image storage 500MB storage/month 2GB storage (5 images x 400MB) $0.20 AWS Cloud Map Service discovery namespace None 1 namespace + 5 service instances $1.00 CloudWatch Logs, metrics, alarms 5GB logs, 10 custom metrics 3GB logs (7-day retention), 15 metrics, 5 alarms $3.00 Data Transfer Inter-AZ, internet egress 100GB out to internet 30GB out (API responses, ALB traffic) $3.00 VPC, Security Groups, IAM, ACM Networking and security Free N/A $0.00 Month 1 Total: ~$31.70 Month 2+ Total: ~$67.70 6.2. Cost Optimization Strategies Immediate Savings (Implemented):\n✅ VPC Endpoints Instead of NAT Gateway (-$28/month)\nBefore: NAT Gateway ($32/month) + data processing ($3/month) = $35/month\nAfter: VPC Endpoints ($7/month for 3 endpoints)\nSavings: $28/month ($336/year)\nTrade-off: None—endpoints are more secure and faster\n✅ Single-Region Deployment (-$50+/month)\nBefore: Multi-region (Singapore + Sydney) with cross-region data transfer\nAfter: Single region with S3 CRR for DR only\nSavings: No second RDS instance ($15), no second ElastiCache ($12), no second ALB ($16), no cross-region data transfer ($10-20)\nTrade-off: Users outside SEA experience higher API latency (acceptable for MVP)\n✅ ECS Fargate with Minimal Resources (-$10-15/month vs larger EC2)\nConfiguration: 0.5 vCPU, 1GB RAM per task (Fargate minimum)\nSavings: Efficient resource allocation, pay only for what you use\nTrade-off: Monitor performance, scale up if needed\nAdditional Optimizations (Consider if Budget Exceeded):\nReduce ECS Task Count During Off-Peak Hours\nCurrent: 2 tasks per service (10 total) running 24/7\nOptimization: Scale down to 1 task per service during 12am-8am SGT\nPotential Savings: ~$6/month\nRisk: Reduced redundancy during off-hours\nOptimize CloudWatch Log Retention\nCurrent: 7-day retention (3GB logs)\nOptimization: Reduce to 3-day retention\nPotential Savings: ~$1-2/month\nTrade-off: Shorter log history for debugging\nUse S3 Intelligent-Tiering\nAutomatically moves infrequently accessed objects to cheaper storage tiers\nPotential Savings: $0.50-1/month\nTrade-off: Minimal retrieval delay for cold objects\nDisable RDS Multi-AZ Temporarily (NOT RECOMMENDED)\nSavings: ~$7.50/month (50% reduction)\nCRITICAL RISK: No automatic failover, accept downtime during DB failures\nUse Case: Only for development/testing environments\nRevised Budget Scenarios:\nScenario Monthly Cost Annual Cost Use Case Current (Free Tier - Month 1) $31.70 - Initial launch with Free Tier benefits Standard (Post Free Tier) $67.70 $812/year Production after Free Tier expires Optimized (Off-peak scaling) $61.70 $740/year Budget-conscious production Development (No Multi-AZ) $60.20 $722/year Testing environment only 6.3. Free Tier Monitoring \u0026 Budget Alerts Critical Free Tier Limits to Track:\nService Free Tier Limit Monthly Allowance Alert Threshold Monitoring Method ECS Fargate 20 GB-hours First month only 16 GB-hours (80%) CloudWatch custom metric + AWS Budgets RDS 750 hours Single-AZ only (Multi-AZ NOT covered) N/A (always paid) N/A ElastiCache 750 node-hours Monthly (cache.t2.micro or t3.micro) 700 hours (93%) CloudWatch billing metric ALB 750 hours + 15 LCUs Monthly 700 hours (93%) AWS Cost Explorer S3 5GB storage, 20k GET, 2k PUT Monthly 4GB, 18k GET, 1.8k PUT S3 Storage Lens CloudFront 1TB data transfer, 10M requests Monthly 900GB, 9M requests CloudFront usage reports Data Transfer 100GB out to internet Monthly 90GB (90%) CloudWatch billing alarm AWS Budgets Configuration:\nBudget 1: Overall Monthly Budget\nName: “Production Infrastructure Budget”\nAmount: $70/month\nAlert Thresholds:\n50% ($35) - Email to team\n80% ($56) - Email to admin + Slack notification\n100% ($70) - Email + SMS to admin\n110% ($77) - Email + trigger cost reduction script\nBudget 2: ECS Fargate Specific\nName: “ECS Fargate Monitor”\nAmount: $40/month\nFiltered by: Service = ECS\nAlert Thresholds: 80%, 100%\nBudget 3: Data Transfer Watch\nName: “Data Transfer Overage Prevention”\nAmount: $10/month\nFiltered by: Usage Type Group = Data Transfer\nAlert Thresholds: 50%, 80%, 100%\nDaily Monitoring Routine (5 minutes):\nCheck AWS Cost Explorer → Daily spend trend\nReview ECS Service metrics → Task count hasn’t scaled unexpectedly\nVerify S3 data transfer → No unusual spikes from CRR\nCheck CloudWatch alarms → No billing alerts triggered\nReview top 5 cost drivers → Identify any anomalies\nWeekly Cost Review (15 minutes):\nGenerate cost report by service (last 7 days)\nCompare to previous week → Identify trends\nReview CloudFront data transfer → Validate CDN efficiency\nCheck RDS Performance Insights → Optimize slow queries\nUpdate cost forecast for end of month\n7. Risk Assessment 7.1. Risk Matrix Risk Category Specific Risk Impact Probability Priority Mitigation Strategy Cost Free Tier exhaustion before month end High High CRITICAL Daily monitoring, budget alarms at 50%/80%/100%, auto-scaling limits Cost ECS Fargate over-scaling during traffic spike High Medium HIGH Set maximum task count to 3 per service, configure target tracking conservatively Cost Unexpected data transfer charges Medium Medium MEDIUM VPC Endpoints eliminate most charges, monitor S3 CRR costs Availability RDS primary failure during peak hours High Low MEDIUM Multi-AZ enabled, automatic failover in 60-120 seconds, test monthly Availability Complete regional failure (Singapore) Critical Very Low HIGH Documented DR runbook, quarterly DR drills, maintain S3 CRR to us-east-1 Availability All ECS tasks fail after bad deployment High Medium HIGH Implement blue/green deployments, automated rollback, thorough testing in staging Performance ElastiCache eviction under load Medium Medium MEDIUM Monitor hit rate (target \u003e80%), increase instance size if needed Performance Database connection pool exhaustion Medium Medium MEDIUM Set max connections to 20 per task, monitor with Performance Insights Security Exposed RDS endpoint to internet Critical Low CRITICAL Security Group restricts to ECS tasks only, no public access, quarterly audit Security Leaked IAM credentials in code Critical Low CRITICAL Use IAM roles exclusively, never hardcode secrets, automated scanning with git-secrets Security S3 bucket misconfiguration (public access) High Low HIGH OAC enforced, S3 Block Public Access enabled, quarterly review Data Accidental database deletion High Very Low MEDIUM Enable RDS deletion protection, require MFA for destructive operations Data Data loss during regional failure Medium Very Low LOW RDS automated backups (7-day retention), test restore process monthly Operational Failed deployment with no rollback plan Medium Medium MEDIUM Document rollback procedure, keep previous 3 task definition revisions 7.2. Detailed Mitigation Plans Cost Control Measures:\nProactive Monitoring:\nDaily Checks:\r- AWS Cost Explorer: Current spend vs forecast\r- ECS Service: Task count = expected (2 per service)\r- CloudWatch Billing: No unexpected alarms\r- S3 metrics: CRR data transfer within normal range\rWeekly Reviews:\r- Top 5 cost drivers analysis\r- Comparison to previous week\r- Free Tier usage tracking\r- Forecast adjustment for end of month\rMonthly Actions:\r- Generate detailed cost report\r- Review and optimize resource allocation\r- Update budget alerts for next month\r- Document lessons learned\rEmergency Cost Reduction Plan (Execute if \u003e100% budget):\nAction Time to Execute Monthly Savings Impact 1. Reduce ECS tasks to 1 per service 5 minutes $18 Reduced redundancy, acceptable for emergency 2. Stop ElastiCache cluster temporarily 2 minutes $12 Slower performance, users may notice 3. Disable S3 CRR temporarily 5 minutes $1 No new DR backups, existing data safe 4. Reduce CloudWatch log retention to 1 day 2 minutes $2 Limited debugging history 5. Scale down to db.t3.micro Single-AZ (risky) 10 minutes $7.50 High risk of downtime, emergency only Total Potential Savings: 24 minutes $40.50 Acceptable for 1-2 weeks while investigating Security Hardening:\nNetwork Security Configuration:\nSecurity Group: ALB-SG\rInbound:\r- Port 443 (HTTPS) from 0.0.0.0/0\r- Port 80 (HTTP) from 0.0.0.0/0 (redirect to 443)\rOutbound:\r- All traffic to ECS-Tasks-SG\rSecurity Group: ECS-Tasks-SG\rInbound:\r- Ports 8080, 8081, 8082, 9998, 9999 from ALB-SG only\r- Ports 8080, 8081, 8082, 9998, 9999 from ECS-Tasks-SG (inter-service communication)\rOutbound:\r- Port 3306 to RDS-SG\r- Port 6379 to Redis-SG\r- Port 443 to VPC Endpoints (S3, ECR, CloudWatch)\rSecurity Group: RDS-SG\rInbound:\r- Port 3306 from ECS-Tasks-SG only\rOutbound:\r- None (no outbound connections)\rSecurity Group: Redis-SG\rInbound:\r- Port 6379 from ECS-Tasks-SG only\rOutbound:\r- None\rIAM Roles Configuration:\nTask Execution Role (ECS infrastructure):\rPermissions:\r- ecr:GetDownloadUrlForLayer\r- ecr:BatchGetImage\r- logs:CreateLogStream\r- logs:PutLogEvents\rTask Role (Application permissions):\rPermissions:\r- s3:GetObject on sgutodolist-assets-sg/*\r- s3:PutObject on sgutodolist-assets-sg/uploads/*\r- ses:SendEmail for notification service\r- cloudwatch:PutMetricData for custom metrics\rSecurity Audit Checklist (Monthly):\nReview all Security Group rules for unnecessary open ports\nVerify RDS has no public accessibility\nConfirm S3 Block Public Access is enabled\nCheck IAM roles for over-permissive policies\nReview CloudTrail logs for suspicious API calls\nVerify all data at rest encryption (RDS, S3)\nConfirm SSL/TLS for all data in transit\nRotate RDS master password (quarterly)\nReview and update security group descriptions\nPerformance Optimization:\nCaching Strategy:\nData Type Cache Location TTL Eviction Policy Monitoring Metric User sessions Redis 30 minutes TTL-based Session count, hit rate \u003e90% User profiles Redis 5 minutes LRU Hit rate \u003e85% Board metadata Redis 10 minutes LRU Hit rate \u003e80% Task lists Redis 2 minutes LRU Hit rate \u003e75% Static reference data Redis 1 hour TTL-based Hit rate \u003e95% Database Query Optimization:\n-- Add indexes for common queries\rCREATE INDEX idx_tasks_board_id ON tasks(board_id);\rCREATE INDEX idx_tasks_assignee_id ON tasks(assignee_id);\rCREATE INDEX idx_tasks_status ON tasks(status);\rCREATE INDEX idx_boards_user_id ON boards(user_id);\rCREATE INDEX idx_board_members_user_id ON board_members(user_id);\r-- Monitor slow queries (\u003e500ms)\rSET GLOBAL slow_query_log = 'ON';\rSET GLOBAL long_query_time = 0.5;\rApplication-Level Optimizations:\nPagination: Max 100 items per page, default 20\nLazy Loading: Use JPA fetch = FetchType.LAZY for relationships\nConnection Pooling: HikariCP with max 20 connections per task\nResponse Compression: gzip enabled on ALB (automatic)\nAPI Rate Limiting: 100 requests/minute per user (prevent abuse)\nDisaster Recovery Testing:\nMonthly DR Drill (30 minutes):\nTest Procedure Expected Result Pass/Fail 1. ECS Task Failure Manually stop one task ALB routes traffic to healthy task, new task launches in 2-3 min 2. Cache Failure Restart Redis cluster App reconnects automatically, regenerates cache 3. Deployment Rollback Deploy bad task definition, then rollback Service returns to previous version in 3-5 min 4. RDS Snapshot Restore Restore snapshot to new instance Database accessible, data integrity verified Quarterly Full DR Exercise (2 hours):\nSimulate Complete Regional Failure\nMark all Singapore resources as “unavailable”\nDocument current RTO/RPO baselines\nExecute DR Runbook:\nStep 1: Restore RDS snapshot to us-east-1 (30 minutes)\rStep 2: Create ECS cluster in us-east-1 (10 minutes)\rStep 3: Deploy all 5 services with updated DB endpoint (20 minutes)\rStep 4: Create and configure ALB in us-east-1 (15 minutes)\rStep 5: Update Route 53 to point to new ALB (5 minutes)\rStep 6: Update CloudFront origin for API calls (10 minutes)\rStep 7: End-to-end testing (20 minutes)\rVerify Complete Functionality:\nUser can log in and access their boards\nTasks can be created, updated, deleted\nFile uploads work correctly\nNotifications are sent\nDocument Findings:\nActual RTO achieved vs target (2-4 hours)\nIssues encountered and resolutions\nUpdates needed to DR runbook\nCost of maintaining DR environment\n8. Expected Outcomes 8.1. Technical Achievements Performance Metrics:\nMetric Target Measurement Method Baseline API Response Time (p95) \u003c100ms CloudWatch custom metrics 80-120ms API Response Time (p99) \u003c300ms CloudWatch custom metrics 250-400ms Page Load Time \u003c2 seconds CloudFront + browser metrics 1.5-2.5s Cache Hit Rate \u003e80% Redis INFO stats 75-85% Database Query Time (p95) \u003c50ms RDS Performance Insights 30-70ms Availability (Monthly) 99.5% CloudWatch uptime monitoring 99.4-99.7% ECS Task Health \u003e95% ALB target health checks 97-99% Scalability Capabilities:\nCurrent Capacity: 100-500 concurrent users with 10 ECS tasks (2 per service)\nMaximum Capacity (Same Cost): 1,000 users with optimized caching and query performance\nScaling Path: Add tasks linearly (3-4 per service for 2,000+ users, ~$30/month additional)\nOperational Improvements:\nBefore (Traditional EC2) After (ECS Fargate) Improvement 4-6 hours/week server management 30 min/week monitoring 85% time savings Manual scaling decisions Automatic target tracking Zero manual intervention 5-10 minute deployments 3-5 minute rolling updates 50% faster SSH access required No server access needed Improved security Complex AMI management Simple Docker images Easier version control 8.2. Long-term Value Skills Development:\nCloud Architecture:\nHands-on experience with AWS core services (ECS, Fargate, RDS, ElastiCache, VPC)\nUnderstanding of High Availability patterns (Multi-AZ, health checks, auto-scaling)\nCost optimization strategies for cloud infrastructure\nTrade-off analysis: Performance vs Cost vs Availability\nContainer \u0026 Microservices:\nDocker containerization best practices\nMicroservices communication patterns (service discovery, API gateways)\nContainer orchestration with ECS Fargate\nService mesh concepts (Cloud Map for DNS-based discovery)\nDevOps Practices:\nCI/CD pipeline design and implementation\nInfrastructure monitoring and observability\nIncident response and disaster recovery\nRolling deployments and zero-downtime updates\nSecurity:\nLeast-privilege IAM policies\nNetwork segmentation with Security Groups\nEncryption at rest (RDS, S3) and in transit (TLS/SSL)\nSecurity audit and compliance practices\nPortfolio Project Value:\nDemonstrates to Employers:\n✅ Production-ready system design (not just tutorials)\n✅ Cost consciousness and budget management\n✅ Modern cloud-native architecture patterns\n✅ End-to-end project delivery (planning → implementation → testing → documentation)\n✅ Ability to work within constraints (budget, technology, time)\nInterview Talking Points:\n“Reduced operational costs by 40% while maintaining 99.5% uptime using ECS Fargate”\n“Implemented Multi-AZ disaster recovery with \u003c2 minute RTO for AZ failures”\n“Designed microservices architecture serving 500+ concurrent users on $30/month budget”\n“Achieved \u003c100ms API response times through strategic caching and query optimization”\nBusiness Foundation:\nMonetization Potential:\nCurrent architecture supports 100-500 users at $30/month\nRevenue model: $5/user/month = $500-2,500/month potential\nBreak-even: 6-10 paying users\nProfit margin: 97-99% after break-even\nScaling Roadmap:\nUsers Monthly Cost Revenue ($5/user) Profit Required Changes 100 $30 $500 $470 None (current architecture) 500 $50 $2,500 $2,450 Increase tasks to 3 per service 1,000 $80 $5,000 $4,920 Upgrade RDS to db.t3.small, add CloudWatch dashboards 5,000 $300 $25,000 $24,700 Multi-region (add ap-southeast-2), Aurora RDS, managed Redis 10,000+ $800+ $50,000+ $49,200+ Multi-region active-active, Aurora Global DB, ECS auto-scaling Career Impact:\nAWS Certification Alignment:\nCloud Practitioner: Covers 70% of services used (EC2, RDS, S3, CloudFront)\nSolutions Architect Associate: Direct experience with 80% of exam topics (VPC, IAM, HA patterns)\nSysOps Administrator: Hands-on with monitoring, scaling, cost optimization\nOpen Source Contribution:\nTemplate repository for “Single-Region HA SaaS on AWS Free Tier”\nBlog series: “Building a Production SaaS for $30/month”\nConference talk: “Cost-Optimized Cloud Architecture Patterns”\nNext Project Ideas (Building on This Foundation):\nAdd real-time collaboration with WebSockets (AWS API Gateway WebSocket)\nImplement full-text search with Amazon OpenSearch\nBuild AI-powered task recommendations with Amazon Bedrock\nAdd mobile app with AWS Amplify and GraphQL API\n9. Future Development Roadmap The current Single-Region HA architecture provides a cost-optimized, production-ready foundation. As the platform grows in users and revenue, the following enhancements can be implemented incrementally.\n9.1. Phase 2: Multi-Region Expansion (6-12 months) When to Consider:\nUser base exceeds 1,000 active users\nSignificant user population outside Southeast Asia (\u003e20%)\nMonthly revenue justifies additional infrastructure cost ($300+)\nBusiness requires \u003c50ms API latency globally\nTarget Architecture:\nComponent Current (Phase 1) Future (Phase 2) Regions ap-southeast-1 (Singapore) + ap-southeast-2 (Sydney), us-west-2 (Oregon) Routing Route 53 (single endpoint) Route 53 Latency-Based Routing with health checks Database RDS Multi-AZ (single region) Amazon Aurora Global Database (multi-region) Compute ECS Fargate (Singapore only) ECS Fargate clusters in each region Caching ElastiCache (Singapore) Regional ElastiCache clusters + Global Datastore Storage S3 CRR (passive DR) S3 Multi-Region Access Points (active-active) Estimated Cost $30-70/month $200-300/month Benefits:\n✅ True global low latency (\u003c50ms for 95% of users)\n✅ Enhanced disaster recovery (automatic failover between regions)\n✅ Geographic compliance (data residency for EU, APAC, US)\n✅ Improved read performance (local read replicas)\n9.2. Phase 3: Enterprise Features (12-18 months) Advanced Security:\nAWS Cognito for SSO/SAML integration\nAWS WAF for API protection against attacks\nAWS Shield Standard for DDoS protection\nAWS GuardDuty for threat detection\nObservability \u0026 Analytics:\nAWS X-Ray for distributed tracing\nAmazon OpenSearch for log analytics\nCustom CloudWatch Dashboards for business metrics\nAWS Cost Explorer API for automated cost reporting\nPerformance Optimization:\nAmazon ElastiCache for Redis with Cluster Mode (horizontal scaling)\nAmazon Aurora Serverless v2 (auto-scaling database)\nAWS Global Accelerator for improved network performance\nCloudFront Functions for edge computing\nOperational Excellence:\nAWS Systems Manager for parameter management\nAWS Secrets Manager for credential rotation\nInfrastructure as Code with Terraform or CDK\nAutomated DR testing with AWS Backup\n9.3. Phase 4: AI/ML Integration (18-24 months) Amazon Bedrock for AI-powered task recommendations\nAmazon SageMaker for predictive analytics (task completion time)\nAmazon Comprehend for sentiment analysis on comments\nAmazon Rekognition for smart image tagging in attachments\n10. Conclusion This Cost-Optimized SaaS Task Management Platform demonstrates that production-grade applications can be built within AWS Free Tier constraints without sacrificing quality or reliability.\nKey Achievements:\n✅ 99.5% availability with Multi-AZ deployment\n✅ \u003c$30/month operational cost in first month\n✅ Zero server management with ECS Fargate\n✅ \u003c100ms API response times with strategic caching\n✅ Disaster recovery with S3 Cross-Region Replication\nCompetitive Advantages:\nModern cloud-native architecture ready for enterprise scale\nComprehensive documentation and operational runbooks\nClear scaling path from 100 to 100,000+ users\nFoundation for future AI/ML features\nNext Steps:\nComplete Phase 1 implementation (4-6 weeks)\nDeploy to production and gather metrics (2-3 months)\nDocument lessons learned and optimize based on real usage\nEvaluate Phase 2 expansion based on user growth and feedback\nThis architecture provides a solid foundation for learning, career development, and potential monetization while maintaining strict cost discipline and operational excellence.",
    "description": "Cost-Optimized SaaS Task Management Platform Single-Region High Availability with ECS Fargate 1. Executive Summary The SaaS Task Management Platform is designed to deliver a Todoist-like collaborative experience with High Availability (HA) and Cost Efficiency, specifically optimized for AWS Free Tier constraints.\nInitially, the project targeted a Multi-Region deployment to achieve the high levels of global performance, disaster recovery (DR), and uptime. However, due to the cost limitations and the constraints of the AWS Free Tier (particularly concerning cross-region data transfer and running multiple primary/replica database instances), the architecture was strategically consolidated into a Single AWS Region (ap-southeast-1 - Singapore).",
    "tags": [],
    "title": "Proposal",
    "uri": "/2-proposal/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 1: STORAGE PREPARATION (S3 \u0026 REPLICATION) This phase focuses on creating a durable storage layer for frontend resources and setting up an automated cross-region synchronization mechanism.\nStep 1.1: Create the Primary Bucket (Singapore) Open the Amazon S3 Console and select the Asia Pacific (Singapore) region.\nClick Create bucket.\nBucket name: sgutodolist-frontend-sg Object Ownership: ACLs disabled (Recommended) Block Public Access: ✅ Block all public access\n(The bucket must remain private because CloudFront OAC will be used.) Bucket Versioning: ✅ Enable\n(Required for cross-region replication.) Default encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3) Click Create bucket.\nStep 1.2: Create the Secondary Bucket (N. Virginia) Switch the region to US East (N. Virginia).\nClick Create bucket.\nBucket name: sgutodolist-frontend-us Block Public Access: ✅ Block all public access Bucket Versioning: ✅ Enable Click Create bucket.\nAfter completing Step 1.1 and Step 1.2, two S3 buckets should be available:\nPrimary bucket: Singapore (ap-southeast-1) Secondary bucket: N. Virginia (us-east-1) Step 1.3: Configure Replication (Automatic Sync) Go back to the Singapore bucket (sgutodolist-frontend-sg).\nNavigate to Management → Replication rules → Click Create replication rule.\nRule name: SyncToUS Status: Enabled Source bucket: Apply to all objects in the bucket Destination: Choose a bucket in this account\n→ Select sgutodolist-frontend-us\n(Ensure that the region us-east-1 is selected so the bucket is visible.) IAM Role: Select Create new role\n(AWS will automatically generate the required permissions.) Click Save.\nWhen prompted with “Replicate existing objects?”, select No\n(The bucket is currently empty.)\n⬅ STEP 1: Prerequisites\rSTEP 3: Route 53 and ACM ➡",
    "description": "PHASE 1: STORAGE PREPARATION (S3 \u0026 REPLICATION) This phase focuses on creating a durable storage layer for frontend resources and setting up an automated cross-region synchronization mechanism.\nStep 1.1: Create the Primary Bucket (Singapore) Open the Amazon S3 Console and select the Asia Pacific (Singapore) region.\nClick Create bucket.\nBucket name: sgutodolist-frontend-sg Object Ownership: ACLs disabled (Recommended) Block Public Access: ✅ Block all public access\n(The bucket must remain private because CloudFront OAC will be used.) Bucket Versioning: ✅ Enable\n(Required for cross-region replication.) Default encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3) Click Create bucket.",
    "tags": [],
    "title": "S3 and Replication",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.2-s3-and-replication/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 2 Objectives: Learn the basic VPC architecture and its important components Do the labs in module 2 Read the book Aws advanced network Discuss ideas for the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn more terms related to VPC practice 15/9/2025 15/9/2025 https://viblo.asia/p/tim-hieu-ve-aws-phan-1-vpc-virtual-private-cloud-924lJGv05PM 3 - Practice: + lab03: Amazon VPC and AWS Site-to-Site VPN Workshop 16/9/2025 16/9/2025 http://000003.awsstudygroup.com/ 4 - Learn how to do DNS setup exercises with route 53 Practice: + lab10: Set up Hybrid DNS with Route 53 Resolver 17/9/2025 17/9/2025 https://000010.awsstudygroup.com/ 5 - Join Cloud Day Vietnam in Ho Chi Minh City 18/09/2025 18/9/2025 6 - Translate blogs - Write worklogs and event reports 1 - Hold a team meeting to finalize the project idea 19/09/2025 19/9/2025 original link 7 - Practice: + lab20: Set up AWS Transit Gatewayy + lab19: Setting up VPC Peering - Update worklog 20/09/2025 20/9/2025 https://000020.awsstudygroup.com/vi/ https://000019.awsstudygroup.com/vi/ Week 2 Achievements: Understand VPC service and how to create a virtual private network on AWS\nPractice building a VPC virtual private network:\nCreate VPC, Subnet, Internet Gateway, Route Table, Security Group, Activate VPC Flow Logs Deploy public and private EC2 servers. Create NAT Gateway to connect private instances to the internet. Use Reachability Analyzer to test connectivity Practice setting up HYBRID DNS with ROUTE 53 RESOLVER:\nTake advantage of templates from AWS Quick Start to build a highly available and secure network infrastructure using AWS CloudFormation. Know how to log in to the server using RDP protocol Set up DNS Practice setting up VPC Peering to allow 2 VPCs to communicate with each other\nPractice using AWS Transit Gateway to optimize the system when there are many vpc that need to communicate with each other. Avoid creating too many VPC Peering\nParticipate in event cloud day vietnam and learn more about migration, modernization\nFinalized the project idea to build a Todoist task management system.",
    "description": "Week 2 Objectives: Learn the basic VPC architecture and its important components Do the labs in module 2 Read the book Aws advanced network Discuss ideas for the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn more terms related to VPC practice 15/9/2025 15/9/2025 https://viblo.asia/p/tim-hieu-ve-aws-phan-1-vpc-virtual-private-cloud-924lJGv05PM 3 - Practice: + lab03: Amazon VPC and AWS Site-to-Site VPN Workshop 16/9/2025 16/9/2025 http://000003.awsstudygroup.com/ 4 - Learn how to do DNS setup exercises with route 53 Practice: + lab10: Set up Hybrid DNS with Route 53 Resolver 17/9/2025 17/9/2025 https://000010.awsstudygroup.com/ 5 - Join Cloud Day Vietnam in Ho Chi Minh City 18/09/2025 18/9/2025 6 - Translate blogs - Write worklogs and event reports 1 - Hold a team meeting to finalize the project idea 19/09/2025 19/9/2025 original link 7 - Practice: + lab20: Set up AWS Transit Gatewayy + lab19: Setting up VPC Peering - Update worklog 20/09/2025 20/9/2025 https://000020.awsstudygroup.com/vi/ https://000019.awsstudygroup.com/vi/ Week 2 Achievements: Understand VPC service and how to create a virtual private network on AWS",
    "tags": [],
    "title": "Week 2",
    "uri": "/1-worklog/1.2-week2/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 3 Objectives: Learn about VM services on AWS Perform application development on EC2, create backups and use cloudwatch to monitor the system Survey the existing Todoist system and discuss the system architecture for the software to be developed. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures about Compute VM service on AWS - Translate blogs for week 3 22/9/2025 22/9/2025 original link 3 - Practice: + lab04: Initialize and learn basic features of Amazon EC2 - Survey the existing Todoist system and discuss the system to be developed. 23/9/2025 23/9/2025 https://000004.awsstudygroup.com/ 4 - Practice: + lab04 section 8: Limit resource usage using IAM service + lab02: Manage access rights with AWS Identity and Access Management (IAM) 24/9/2025 24/9/2025 https://000004.awsstudygroup.com/vi/8-costusagegovernance/ https://000002.awsstudygroup.com/vi/ 5 - Practice: + lab08: Using CloudWatch 25/9/2025 25/9/2025 https://000008.awsstudygroup.com/ 6 - Practice: + lab06: Deploy FCJ Management application with Auto Scaling Group + lab13: Deploy aws backup for the system - Update worklog 26/9/2025 26/9/2025 https://000006.awsstudygroup.com/ https://000013.awsstudygroup.com/ Week 3 Achievements: Understand the Compute VM service on AWS\nPractice deploying applications with EC2:\nInitialize instances and connect to initialized instances Know how to deploy a simple application to an AWS Linux server Limit access to resources using AWS IAM Practice managing access rights with AWS IDENTITY AND ACCESS MANAGEMENT (IAM):\nKnow how to create an IAM Role to temporarily grant permissions to an IAM User Practice using CloudWatch to monitor the status of the system.\nPractice deploying applications with Auto Scaling Group:\nSet up a Load Balancer to automatically distribute traffic across multiple targets Create an Auto Scaling Group to automatically initialize more instances according to different strategies Deploy AWS Backup for the system\nSurvey the existing system and finalize the system architecture:\nBackend: Spring Frontend: React Database: MySQL",
    "description": "Week 3 Objectives: Learn about VM services on AWS Perform application development on EC2, create backups and use cloudwatch to monitor the system Survey the existing Todoist system and discuss the system architecture for the software to be developed. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures about Compute VM service on AWS - Translate blogs for week 3 22/9/2025 22/9/2025 original link 3 - Practice: + lab04: Initialize and learn basic features of Amazon EC2 - Survey the existing Todoist system and discuss the system to be developed. 23/9/2025 23/9/2025 https://000004.awsstudygroup.com/ 4 - Practice: + lab04 section 8: Limit resource usage using IAM service + lab02: Manage access rights with AWS Identity and Access Management (IAM) 24/9/2025 24/9/2025 https://000004.awsstudygroup.com/vi/8-costusagegovernance/ https://000002.awsstudygroup.com/vi/ 5 - Practice: + lab08: Using CloudWatch 25/9/2025 25/9/2025 https://000008.awsstudygroup.com/ 6 - Practice: + lab06: Deploy FCJ Management application with Auto Scaling Group + lab13: Deploy aws backup for the system - Update worklog 26/9/2025 26/9/2025 https://000006.awsstudygroup.com/ https://000013.awsstudygroup.com/ Week 3 Achievements: Understand the Compute VM service on AWS",
    "tags": [],
    "title": " Week 3",
    "uri": "/1-worklog/1.3-week3/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Accelerating the next wave of generative AI startups Swami Sivasubramanian | 13/06/2024 | in AWS for Startups, Featured, Startup, Startup Spotlight | Permalink\nSince day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. Each year since 2020, we have provided startups nearly $1 billion in AWS Promotional Credits. It’s no coincidence then that 80% of the world’s unicorns use AWS. I am lucky to have had a front row seat to the development of so many of these startups over my time at AWS—companies like Netflix, Wiz, and Airtasker. And I’m enthusiastic about the rapid pace at which startups are adopting generative artificial intelligence (AI) and how this technology is creating an entirely new generation of startups.\nThese generative AI startups have the ability to transform industries and shape the future, which is why today we announced a commitment of $230 million to accelerate the creation of generative AI applications by startups around the world. We are excited to collaborate with visionary startups, nurture their growth, and unlock new possibilities. In addition to this monetary investment, today we’re also announcing the second-annual AWS Generative AI Accelerator in partnership with NVIDIA. This global 10-week hybrid program is designed to propel the next wave of generative AI startups. This year, we’re expanding the program 4x to serve 80 startups globally. Selected participants will each receive up to $1 million in AWS Promotional Credits to fuel their development and scaling needs. The program also provides go-to-market support as well as business and technical mentorship. Participants will tap into a network that includes domain experts from AWS as well as key AWS partners such as NVIDIA, Meta, Mistral AI, and venture capital firms investing in generative AI.\nBuilding in the cloud with generative AI In addition to these programs, AWS is committed to making it possible for startups of all sizes and developers of all skill levels to build and scale generative AI applications with the most comprehensive set of capabilities across the three layers of the generative AI stack. At the bottom layer of the stack, we provide infrastructure to train large language models (LLMs) and foundation models (FMs) and produce inferences or predictions. This includes the best NVIDIA GPUs and GPU-optimized software, custom, machine learning (ML) chips including AWS Trainium and AWS Inferentia, as well as Amazon SageMaker, which greatly simplifies the ML development process. In the middle layer, Amazon Bedrock makes it easier for startups to build secure, customized, and responsible generative AI applications using LLMs and other FMs from leading AI companies. And at the top layer of the stack, we have Amazon Q, the most capable generative AI-powered assistant for accelerating software development and leveraging companies’ internal data.\nCustomers are innovating using technologies across the stack. For instance, during my time at the VivaTech conference in Paris last month, I sat down Michael Chen, VP of Strategic Alliances at PolyAI, which offers customized voice AI solutions for enterprises. PolyAI develops natural-sounding text-to-speech models using Amazon SageMaker. And they build on Amazon Bedrock to ensure responsible and ethical AI practices. They use Amazon Connect to integrate their voice AI into customer service operations.\nAt the bottom layer of the stack, NinjaTech uses Trainium and Inferentia2 chips, along with Amazon SageMaker, to build, train, and scale custom AI agents. From conducting research to scheduling meetings, these AI agents save time and money for NinjaTech’s users by bringing the power of generative AI into their everyday workflows. I recently sat down with Sam Naghshineh, Co-founder and CTO, to discuss how this approach enables them to save time and resources for their users.\nLeonardo.AI, a startup from the 2023 AWS Generative AI Accelerator cohort, is also harnessing the capabilities of AWS Inferentia2 to enable artists and professionals to produce high-quality visual assets with unmatched speed and consistency. By reducing their inference costs without sacrificing performance, Leonardo.AI can offer their most advanced generative AI features at a more accessible price point.\nLeading generative AI startups, including Perplexity, Hugging Face, AI21 Labs, Articul8, Luma AI, Hippocratic AI, Recursal AI, and DatologyAI are building, training, and deploying their models on Amazon SageMaker. For instance, Hugging Face used Amazon SagaMaker HyperPod, a feature that accelerates training by up to 40%, to create new open-source FMs. The automated job recovery feature helps minimize disruptions during the FM training process, saving them hundreds of hours of training time a year.\nAt the middle layer, Perplexity leverages Amazon Bedrock with Anthropic Claude 3 to build their AI-powered search engine. Bedrock ensures robust data protection, ethical alignment through content filtering, and scalable deployment of Claude 3. While Nexxiot, an innovator in transportation and supply chain solutions, quickly moved its Scope AI assistant solution to Amazon Bedrock with Anthropic Claude in order to give their customers the best real-time, conversational insights into their transport assets.\nAt the top layer, Amazon Q Developer helps developers at startups build, test, and deploy applications faster and more efficiently, allowing them to focus their valuable energy on driving innovation. Ancileo, an insurance SaaS provider for insurers, re-insurers, brokers, and affinity partners, uses Amazon Q Developer to reduce the time to resolve coding-related issues by 30%, and is integrating ticketing and documentation with Amazon Q to speed up onboarding and allow anyone in the company to quickly find their answers. Amazon Q Business enables everyone at a startup to be more data-driven and make better, faster decisions using the organization’s collective knowledge. Brightcove, a leading provider of cloud video services, deployed Amazon Q Business to streamline their customer support workflow, allowing the team to expedite responses, provide more personalized service, and ultimately enhance the customer experience.\nResources for generative AI startups The future of generative AI belongs to those who act now. The application window for the AWS Generative AI Accelerator program is open from June 13 to July 19, 2024, and we’ll be selecting a global cohort of the most promising generative AI startups. Don’t miss this unique chance to redefine what’s possible with generative AI, and apply now!\nOther helpful resources include:\nYou can use your AWS Activate credits for Amazon Bedrock to experiment with FMs, along with a broad set of capabilities needed to build responsible generative AI applications with security and privacy. Dive deeper by exploring our Generative AI Community space for technical content, insights, and connections with fellow builders. AWS also provides free training to help the current and future workforce take advantage of Amazon’s generative AI tools. For those interested in learning to build with generative AI on AWS, explore the comprehensive Generative AI Learning Plan for Developers to gain the skills you need to create cutting-edge applications NVIDIA offers NVIDIA Inception, a free program designed to help startups evolve faster through cutting-edge technology, opportunities to connect with venture capitalists, and access to the latest technical resources from NVIDIA. Apply now, explore the resources, and join the generative AI revolution with AWS. Additional Resources Twitch series: Let’s Ship It – with AWS! Generative AI\nAWS Generative AI Accelerator Program: Apply now\nTAGS: Accelerators, AWS Startups",
    "description": "Accelerating the next wave of generative AI startups Swami Sivasubramanian | 13/06/2024 | in AWS for Startups, Featured, Startup, Startup Spotlight | Permalink\nSince day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. Each year since 2020, we have provided startups nearly $1 billion in AWS Promotional Credits. It’s no coincidence then that 80% of the world’s unicorns use AWS. I am lucky to have had a front row seat to the development of so many of these startups over my time at AWS—companies like Netflix, Wiz, and Airtasker. And I’m enthusiastic about the rapid pace at which startups are adopting generative artificial intelligence (AI) and how this technology is creating an entirely new generation of startups.",
    "tags": [],
    "title": "Blog 3",
    "uri": "/3-blogstranslated/3.3-blog3/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "Blog 1 - Technology that teaches empathy? How mpathic uses AI to help us listen to each other What if artificial intelligence (AI) could augment our ability to really listen and truly relate to others? What if technology could draw upon our collective lived experiences and help us be more human to each other? These are the questions Dr. Grin Lord, clinical psychologist and founder of conversation analytics company mpathic, has spent the last 15 years chasing.\nBlog 2 - Tech-savvy savings: innovative ways to cut costs in your small business Discover practical strategies to reduce your small business costs through smart cloud management, storage optimization, and AI automation. Learn how to save money while maintaining quality.\nBlog 3 - Tech-savvy savings: innovative ways to cut costs in your small business Since day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. These startups have the ability to transform industries and shape the future, which is why today we announced a commitment of $230 million to accelerate the creation of generative AI applications by startups around the world. Read to learn how to apply to become a member of this global program.\nBlog 4 - Scaling Cloudera’s development environment: Leveraging Amazon EKS, Karpenter, Bottlerocket, and Cilium for hybrid cloud This post is co-written with Shreelola Hegde,Sriharsha Devineni and Lee Watterworth from Cloudera. Cloudera is a global leader in enterprise data management, analytics, and AI. The Cloudera platform enables organizations to manage, process, and analyze massive datasets, helping businesses across industries like finance, healthcare, manufacturing, and telecommunications accelerate AI/ML adoption and unlock real-time insights. A […]\nBlog 5 - Creating a Multi-Region Application with AWS Services – Part 1, Compute, Networking, and Security Many AWS services have features to help you build and manage a multi-Region architecture, but identifying those capabilities across 200+ services can be overwhelming. In this 3-part blog series, we filter through those 200+ services and focus on those that have specific features to assist you in building multi-Region applications. In Part 1, we’ll build a foundation with AWS security, networking, and compute services.\nBlog 6 - GitOps continuous delivery with ArgoCD and EKS using natural language ArgoCD is a leading GitOps tool that empowers teams to manage Kubernetes deployments declaratively, using Git as the single source of truth. Its robust feature set, including automated sync, rollback support, drift detection, advanced deployment strategies, RBAC integration, and multi-cluster support, makes it a go-to solution for Kubernetes application delivery. However, as organizations scale, several pain points and operational challenges become apparent.\nBlog 7 - Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple Today we are excited to announce version 1.0 of the Strands Agents SDK, marking a significant milestone in our journey to make building AI agents simple, reliable, and production-ready. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production.\nBlog 8 -Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances The analysis of ever-increasing amounts of genomic and multiomic data demands efficient, scalable, and cost-effective computational solutions. Amazon Web Services (AWS) continues to support these workloads through FPGA accelerated compute offerings such as Amazon EC2 F2 instances.",
    "description": "Blog 1 - Technology that teaches empathy? How mpathic uses AI to help us listen to each other What if artificial intelligence (AI) could augment our ability to really listen and truly relate to others? What if technology could draw upon our collective lived experiences and help us be more human to each other? These are the questions Dr. Grin Lord, clinical psychologist and founder of conversation analytics company mpathic, has spent the last 15 years chasing.",
    "tags": [],
    "title": "BlogsTranslated",
    "uri": "/3-blogstranslated/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase focuses on adapting the application source code for the cloud environment. We will configure Cross-Origin Resource Sharing (CORS) for the reactive API Gateway, refactor application configurations to use dynamic environment variables, and finally build and push all microservice Docker images to the Amazon Elastic Container Registry (ECR).\nObjectives:\nConfigure the code to work seamlessly in both Local (Docker Compose) and Cloud (AWS ECS) environments.\nCreate Repositories on AWS ECR.\nBuild and Push all 6 Docker images to AWS.\n1. CREATE REPOSITORIES ON AWS ECR Before pushing images, you must create a “repository” for each service. Run the following commands in your Terminal (PowerShell or Git Bash):\n# Auth Service aws ecr create-repository --repository-name auth-service --region ap-southeast-1 # User Service aws ecr create-repository --repository-name user-service --region ap-southeast-1 # Taskflow Service aws ecr create-repository --repository-name taskflow-service --region ap-southeast-1 # Notification Service aws ecr create-repository --repository-name notification-service --region ap-southeast-1 # API Gateway aws ecr create-repository --repository-name api-gateway --region ap-southeast-1 # AI Model Service aws ecr create-repository --repository-name ai-model-service --region ap-southeast-1\r2. BUILD AND PUSH IMAGES Execute these commands from the root directory of your project (todolist-backend).\n1. Login Docker to AWS: (Replace 031133710884 with your AWS Account ID if it’s different)\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com\r2. Build \u0026 Push each of the 6 Services:\nService 1: API Gateway\ncd api-gateway docker build -t api-gateway:latest . docker tag api-gateway:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/api-gateway:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/api-gateway:latest cd ..\rService 2: Auth Service\ncd auth-service docker build -t auth-service:latest . docker tag auth-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/auth-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/auth-service:latest cd ..\rService 3: User Service\ncd user-service docker build -t user-service:latest . docker tag user-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/user-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/user-service:latest cd ..\rService 4: Taskflow Service\ncd taskflow-service docker build -t taskflow-service:latest . docker tag taskflow-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/taskflow-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/taskflow-service:latest cd ..\rService 5: Notification Service\ncd notification-service docker build -t notification-service:latest . docker tag notification-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/notification-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/notification-service:latest cd ..\rService 6: AI Model Service (Note: The folder name is model)\ncd model docker build -t ai-model-service:latest . docker tag ai-model-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/ai-model-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/ai-model-service:latest cd ..\rEXPECTED RESULT After running all the commands, you can verify by running:\naws ecr describe-repositories --region ap-southeast-1\rIf you see a list of 6 repositories and no errors during the push process, you have successfully completed this phase.\nYou can now proceed to creating the Task Definitions.\n⬅ STEP 2: Infrastructure \u0026 ALB Setup\rSTEP 4: Task Definitions Creation ➡",
    "description": "This phase focuses on adapting the application source code for the cloud environment. We will configure Cross-Origin Resource Sharing (CORS) for the reactive API Gateway, refactor application configurations to use dynamic environment variables, and finally build and push all microservice Docker images to the Amazon Elastic Container Registry (ECR).\nObjectives:\nConfigure the code to work seamlessly in both Local (Docker Compose) and Cloud (AWS ECS) environments.\nCreate Repositories on AWS ECR.",
    "tags": [],
    "title": "Code Update \u0026 Image Build (Create new Docker Image)",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.3-code-update--image-build-create-new-docker-image/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop",
    "content": "Table of Contents This section provides step-by-step guidance for deploying both Frontend and Backend components onto the cloud infrastructure.\nIt covers the complete deployment workflow, from build artifacts preparation to production-ready release on AWS services.\n5.3.1. Frontend Deploy\nGuidelines for building, configuring, and deploying the frontend to Amazon S3 + CloudFront with cross-region failover.\n5.3.2. Backend Deploy\nInstructions for deploying backend microservices to AWS ECS/Fargate, including network configuration, load balancing, and service integration.\nBy the end of this section, the application will be fully deployed on AWS with a secure, scalable, and cost-optimized architecture.",
    "description": "Table of Contents This section provides step-by-step guidance for deploying both Frontend and Backend components onto the cloud infrastructure.\nIt covers the complete deployment workflow, from build artifacts preparation to production-ready release on AWS services.\n5.3.1. Frontend Deploy\nGuidelines for building, configuring, and deploying the frontend to Amazon S3 + CloudFront with cross-region failover.\n5.3.2. Backend Deploy\nInstructions for deploying backend microservices to AWS ECS/Fargate, including network configuration, load balancing, and service integration.",
    "tags": [],
    "title": "Deploy Flow",
    "uri": "/5-workshop/5.3-deploy_flow/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 2: DOMAIN \u0026 SECURITY PREPARATION (ROUTE 53 \u0026 ACM) Before creating the CloudFront distribution, the domain name system (DNS) and SSL certificates must be properly prepared.\nStep 2.1: Create a Hosted Zone (If Not Exists) Navigate to Amazon Route 53 → Hosted zones.\nIf the domain sgutodolist.com does not exist in the list:\nClick Create hosted zone. Domain name: sgutodolist.com Type: Public hosted zone Click Create. Update Name Servers at the domain provider\n(This project uses a domain purchased from a third-party registrar.)\nAfter opening the newly created Hosted Zone, you will see four Name Servers in the following format: ns-1538.awsdns-00.co.uk. ns-1374.awsdns-43.org. ns-172.awsdns-21.com. ns-547.awsdns-04.net.\nyaml Copy code\nCopy all four Name Servers and update them in the domain management panel of your domain registrar. Step 2.2: Request an SSL Certificate (ACM) – IMPORTANT ⚠️ IMPORTANT NOTE:\nSSL certificates used with Amazon CloudFront MUST be created in the US East (N. Virginia) region.\nSwitch the AWS Console region to US East (N. Virginia).\nGo to AWS Certificate Manager (ACM) → Request certificate.\nSelect Request a public certificate → Next.\nDomain names:\nsgutodolist.com *.sgutodolist.com\n(Wildcard domain for subdomains such as www.) Validation method: DNS validation (Recommended).\nClick Request.\nIn the Certificates list, click the newly created certificate\n(Status: Pending validation).\nUnder the Domains section, click Create records in Route 53.\nClick Create records.\nWait a few minutes until the certificate status changes to Issued (Green).\n---\r⬅ STEP 2: S3 and Replication\rSTEP 4: CloudFront and Failover ➡",
    "description": "PHASE 2: DOMAIN \u0026 SECURITY PREPARATION (ROUTE 53 \u0026 ACM) Before creating the CloudFront distribution, the domain name system (DNS) and SSL certificates must be properly prepared.\nStep 2.1: Create a Hosted Zone (If Not Exists) Navigate to Amazon Route 53 → Hosted zones.\nIf the domain sgutodolist.com does not exist in the list:\nClick Create hosted zone. Domain name: sgutodolist.com Type: Public hosted zone Click Create. Update Name Servers at the domain provider\n(This project uses a domain purchased from a third-party registrar.)",
    "tags": [],
    "title": "Route 53 and ACM",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.3-route-53-and-acm/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Scaling Cloudera’s development environment: Leveraging Amazon EKS, Karpenter, Bottlerocket, and Cilium for hybrid cloud Harpreet Virk and Padma Iyer | 26/09/2025 | in Advanced (300), Amazon Elastic Kubernetes Service, Best Practices, Containers, Graviton, Migration, Migration Acceleration Program (MAP) | Permalink |\nThis post is co-written with Shreelola Hegde,Sriharsha Devineni and Lee Watterworth from Cloudera.\nCloudera is a global leader in enterprise data management, analytics, and AI. The Cloudera platform enables organizations to manage, process, and analyze massive datasets, helping businesses across industries like finance, healthcare, manufacturing, and telecommunications accelerate AI/ML adoption and unlock real-time insights.\nA key element driving the success of the platform is the development environment, where developers can build and test new features for release. The environment, built on Kubernetes, faced multiple challenges on-premises especially in the scaling and utilization of resources.\nThis post covers how Cloudera modernized its development operations by adopting a hybrid cloud approach built on Amazon Elastic Kubernetes Service (Amazon EKS). This strategy balances the existing capacity of on-premises environments with the elasticity of the cloud while leveraging enhancements such as Karpenter, Bottlerocker, and Cilium to optimize scaling, security, and cost efficiency.\nOperational Challenges faced On-Premises The development environment faced challenges running on-premises: The environment was required to scale up and down frequently but was constrained by fixed capacity. Build and test processes for services like Apache Spark, Hive, and HBase required containers as large as 64 GB RAM and 32 vCPUs, which quickly exceeded available resources. Pull requests during intensive coding sprints surged as high as 300% leading to 45-minute build time increases, creating a bottleneck in the CI/CD pipeline that significantly slowed development velocity. This, in turn, delayed feature delivery, and risked release schedules while increasing infrastructure costs and developer idle time. The application design also involved retrieving and saving artifacts and datasets from Amazon S3, which introduced latency for on-premises agents, further extending build and test cycles. These constraints created bottlenecks in the development pipeline and highlighted the need for elasticity beyond on-premises clusters. Solution Overview Cloudera addressed these challenges by adopting a hybrid cloud model where predictable workloads remained on-premises and dynamic workloads moved to AWS. The architecture brought together the elasticity of AWS with Cloudera’s established on-premises infrastructure, delivering seamless scaling, reduced latency, and optimized costs.\nHigh-level architecture of the development environment in AWS\nThe modernized Kubernetes architecture provided the following benefits:\nElasticity with Amazon EKS and Karpenter: Cloudera, using Karpenter, was able to scale its workloads from a handful of nodes to thousands within minutes and also contract when demand dropped. This ensured efficient scaling during surges while eliminating idle resource waste during freezes. This also enabled multiple pull requests to run in parallel without waiting for capacity, giving developers faster turnaround times and improving release velocity. Intelligent provisioning ensured that compute instances were always aligned with workload requirements, which improved utilization rates and reduced costs by up to 40%.\nHandling large containers with Karpenter: Build and test jobs requiring massive containers were matched instantly with optimized compute through Karpenter’s intelligent node provisioning. This real-time elasticity ensured no delays or resource contention.\nStrengthening security with Bottlerocket: The linux-based OS further enhanced the scaling process by delivering a container-optimized environment with minimal operating system overhead. Its immutable filesystem strengthened security by preventing unauthorized changes, while atomic updates simplified system patching and reduced maintenance downtime. This change reduced the development environment’s attack surface by 60%, streamlined patching through atomic updates, and improved compute efficiency by 35%.\nReducing build delays with Bottlerocket and Amazon Elastic Block Store (Amazon EBS) (Amazon EBS) snapshots: od launch times fell from 30 minutes to seconds using Bottlerocket and Amazon EBS snapshots to pre-cache large images. This improvement gave developers the ability to start new builds almost instantly, transforming productivity. Pull request spikes no longer created bottlenecks.\nScaling network with Cilium: Networking was modernized with Cilium, which provided identity-based security, advanced pod-level observability, and eBPF-driven networking. By introducing flexible IP address management, Cilium allowed Cloudera to scale beyond 10,000 workloads without encountering IP exhaustion issues, all while offering clear visibility into pod-level networking.\nEliminating idle resource waste with AWS Graviton and Flex instances: Graviton và Flex instances đóng vai trò quan trọng trong tối ưu chi phí và hiệu năng. Graviton mang lại lợi ích hiệu năng-giá tốt cho workloads dựa trên ARM, trong khi Flex instances tăng hiệu quả cho các tác vụ biên dịch x64. Kết hợp lại, các tùy chọn tính toán này giảm gần một phần ba chi phí vận hành và tối đa 40% chi phí hạ tầng, đảm bảo Cloudera cân bằng giữa hiệu suất và chi phí cho các nhu cầu workload đa dạng.\nResolving S3 latency with cloud-native integration: Running builds in Amazon EKS, dropped latency to Amazon S3 to milliseconds, accelerating artifact retrieval. This had the additional benefit of lowering network transfer costs by 30%.\nThis holistic solution not only addressed each bottleneck but also created a foundation that scales elastically, operates securely, and optimizes costs across hybrid environments.\nBusiness Outcomes The adoption of this hybrid Kubernetes environment transformed Cloudera’s development operations. Build and test cycle times improved by 50%, enabling faster delivery of new features and improvements. The ability to scale from 10 to more than 1,000 nodes in minutes gave developers reliable access to the resources they needed, eliminating bottlenecks during high demand. By optimizing data transfers with Amazon S3, network costs were reduced by 30% and latency was cut to milliseconds. Intelligent scaling and workload-aligned compute selection lowered infrastructure costs by 40%. Bottlerocket reduced the attack surface by 60% and improved compute efficiency by 35%.\nThese advances not only strengthened security but also delivered freed engineers from infrastructure management and allowing them to focus on core development.\nConclusion Cloudera’s successful implementation showcases the transformative power of AWS’s container stack – Amazon EKS, Karpenter, and Bottlerocket. The modernized Kubernetes environment resulted in seamless scaling, enhanced security, and optimized cost management while delivering peak performance for dynamic workloads. Cloudera’s journey proves how the integration of purpose-built AWS solutions can dramatically improve infrastructure management, reduce operational overhead, and accelerate developer productivity.\nThrough automated node provisioning, intelligent workload placement, and streamlined operations, Cloudera demonstrates how organizations can achieve efficiency in container environments. Following Cloudera’s proven architecture, enterprises can build a robust, scalable, and cost-effective Kubernetes environment that meets today’s demanding development needs while preparing for future growth. Speak with your AWS account team to take the next steps to building a modernized Amazon EKS environment.",
    "description": "Scaling Cloudera’s development environment: Leveraging Amazon EKS, Karpenter, Bottlerocket, and Cilium for hybrid cloud Harpreet Virk and Padma Iyer | 26/09/2025 | in Advanced (300), Amazon Elastic Kubernetes Service, Best Practices, Containers, Graviton, Migration, Migration Acceleration Program (MAP) | Permalink |\nThis post is co-written with Shreelola Hegde,Sriharsha Devineni and Lee Watterworth from Cloudera.\nCloudera is a global leader in enterprise data management, analytics, and AI. The Cloudera platform enables organizations to manage, process, and analyze massive datasets, helping businesses across industries like finance, healthcare, manufacturing, and telecommunications accelerate AI/ML adoption and unlock real-time insights.",
    "tags": [],
    "title": " Blog 4",
    "uri": "/3-blogstranslated/3.4-blog4/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop",
    "content": "Cleanup Guide This guide provides a comprehensive instruction to completely terminate all deployed AWS resources across the ap-southeast-1 (Singapore) and us-east-1 (N. Virginia) regions. The sequence is optimized to avoid orphaned resources and subsequent charges.\nI. Deployed AWS Services Summary Before proceeding, verify the existence of the following active services in your account:\nService Category Service/Resource Name Region Purpose Compute/Scaling ECS Cluster/ASG, EC2 Instances ap-southeast-1 Application Hosting Database/Caching RDS DB Instance (Multi-AZ), ElastiCache Redis Cluster ap-southeast-1 Data Persistence \u0026 Session Management Networking Application Load Balancer (ALB), Target Groups, NAT Gateway, VPC ap-southeast-1 Traffic Distribution \u0026 Internet Access Storage/Artifacts S3 Primary Bucket, S3 DR Bucket, ECR Repositories ap-southeast-1, us-east-1 File Storage \u0026 Image Hosting Global/DNS/Security CloudFront Distribution, ACM Certificates, Route 53 Hosted Zone Global (Edge), us-east-1, ap-southeast-1 CDN \u0026 SSL Security II. Cleanup Procedure (Step-by-Step) 1. Clean Up Application Layer (ap-southeast-1) We start by eliminating resources that host the application and connect to the database.\nStop ECS/EC2 Compute:\nIf using ECS: Go to Amazon ECS → Clusters. Select Cluster → Tab Services. Select all Services → Click Update. Set Desired tasks to 0 → Next → Update Service. Wait for tasks to stop. Then, select Services again → Delete → Confirm with delete me. Delete Load Balancer (ALB) Components:\nGo to EC2 → Target Groups. Select Target Groups → Click Actions → Delete.\nGo to EC2 → Load Balancers. Select ALB → Click Actions → Delete.\n2. Delete Database and Caching Services Delete ElastiCache (Redis):\nGo to ElastiCache → Redis Clusters. Select Cluster → Click Actions → Delete. Delete RDS Database (CRITICAL COST STEP):\nGo to RDS → Databases. Select Instance.\nIf Deletion protection is Enabled, click Modify → Scroll to Deletion protection → Uncheck the box → Continue → Apply immediately.\nSelect Instance → Click Actions → Delete.\nUncheck Create final snapshot → Type the DB name to confirm → Click Delete.\n3. Clean Up Global Services and Certificates These steps involve resources spanning multiple regions.\nDelete CloudFront Distribution:\nGo to CloudFront. Select Distribution → Click Disable. (Wait 10-15 minutes for status change).\nOnce Disabled, select Distribution → Click Delete.\nDelete ACM Certificates (Must be done in both regions):\nus-east-1 (N. Virginia): Switch Region → Go to ACM. Select Certificate → Click Delete.\nap-southeast-1 (Singapore): Switch Region → Go to ACM. Select Certificate → Click Delete.\n4. Clean Up Storage and Artifacts Delete ECR Repositories:\nGo to Elastic Container Registry (ECR). Select Repositories → Click Delete → Type delete to confirm. Clean and Delete S3 Buckets:\nRemove CRR: Go to S3 → Select Primary Bucket (ap-southeast-1) → Tab Management → Replication Rules → Delete the rule.\nEmpty: For both S3 Buckets (ap-southeast-1 and us-east-1), go to Tab Objects → Select all → Click Delete → Type permanently delete to confirm.\nDelete: After emptying, go to Tab Properties → Click Delete → Type the Bucket name to confirm.\n5. Delete Networking and DNS Delete NAT Gateway (CRITICAL COST STEP):\nGo to VPC → NAT Gateways. Select Gateway → Click Actions → Delete NAT Gateway. Delete Internet Gateway (IGW):\nGo to VPC → Internet Gateways. Select IGW → Click Actions → Detach from VPC.\nSelect IGW → Click Actions → Delete Internet Gateway.\nDelete VPC Components:\nDelete Security Groups → Delete Subnets → Delete VPC itself. Delete Route 53 Hosted Zone:\nGo to Route 53 → Hosted Zones. Delete all custom records → Click Delete Hosted Zone. III. Final Verification AWS Billing/Cost Explorer: Immediately check to ensure your estimated hourly cost has dropped to $0.00.\nDashboards: Verify the EC2, ECS, and RDS dashboards in ap-southeast-1 show zero running resources.",
    "description": "Cleanup Guide This guide provides a comprehensive instruction to completely terminate all deployed AWS resources across the ap-southeast-1 (Singapore) and us-east-1 (N. Virginia) regions. The sequence is optimized to avoid orphaned resources and subsequent charges.\nI. Deployed AWS Services Summary Before proceeding, verify the existence of the following active services in your account:\nService Category Service/Resource Name Region Purpose Compute/Scaling ECS Cluster/ASG, EC2 Instances ap-southeast-1 Application Hosting Database/Caching RDS DB Instance (Multi-AZ), ElastiCache Redis Cluster ap-southeast-1 Data Persistence \u0026 Session Management Networking Application Load Balancer (ALB), Target Groups, NAT Gateway, VPC ap-southeast-1 Traffic Distribution \u0026 Internet Access Storage/Artifacts S3 Primary Bucket, S3 DR Bucket, ECR Repositories ap-southeast-1, us-east-1 File Storage \u0026 Image Hosting Global/DNS/Security CloudFront Distribution, ACM Certificates, Route 53 Hosted Zone Global (Edge), us-east-1, ap-southeast-1 CDN \u0026 SSL Security II. Cleanup Procedure (Step-by-Step) 1. Clean Up Application Layer (ap-southeast-1) We start by eliminating resources that host the application and connect to the database.",
    "tags": [],
    "title": "Clean Up",
    "uri": "/5-workshop/5.4-clean_up/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "STAGE 3: CONFIGURATION CLOUDFRONT (CDN \u0026 FAILOVER) STEP 3.1: Create Distribution Step 1: Get started Distribution name: sgutodolist-frontend-cloudfront\nDistribution type: Keep Single website or app.\nDomain:\nIn the Route 53 managed domain box, enter: sgutodolist.com.\n(If there is an Alternate domain names box, enter www.sgutodolist.com if the interface allows, otherwise we will add it later).\nClick Next.\nStep 2: Specify origin Origin type: Select Amazon S3.\nOrigin:\nClick on the search box, select bucket Singapore: sgutodolist-frontend-sg....\nAllow private S3 bucket access to CloudFront:\nSelect: Allow private S3 bucket access to CloudFront - Recommended.\nCache settings: Leave “Use recommended cache settings…” unchanged.\nClick Next.\nStep 3: Enable security Web Application Firewall (WAF):\nCheck the box on the right: Do not enable security protections (to save costs).\nClick Next.\nStep 4: Get TLS certificate TLS certificate:\nSelect the generated ACM certificate: sgutodolist.com (...).\nClick Next.\nStep 5: Review and create Scroll down to the bottom and click the orange button Create distribution. STEP 3.1 (Additional): Post-creation configuration (Required) Origin access control: This step is done after successfully creating a Distribution, go to the Origin tab of the newly created Distribution, click on origin and click edit\nClick Create new OAC.\nName: S3-OAC-HA.\nSigning behavior: Sign requests.\nClick Create.\nNext:\n1. Copy Policy (Important):\nClick on the newly created Distribution\nGo to the Origin tab of the newly created Distribution\nClick on the origin in the Origins list and click the Edit button\nClick the Copy policy button in the Origin access control. Go to the S3 Console tab \u003e Bucket Singapore \u003e Permissions \u003e Bucket Policy \u003e Paste \u003e Save. 2. Add Default Root Object (Fix white screen error):\nIn the newly created Distribution details screen, select the General tab (first tab).\nScroll down to the Settings section, click the Edit button (located to the right of the Settings section).\nFind the Default root object box.\nEnter: index.html.\n(By the way, check the Alternate domain names (CNAMEs) section: Make sure both sgutodolist.com and www.sgutodolist.com are present. If missing, add the item).\nScroll down and click Save changes.\nStep 3.2: Add Secondary Origin (Virginia) Go to the newly created Distribution \u003e Origins tab.\nClick Create origin.\nOrigin domain: Select the Virginia bucket (sgutodolist-frontend-us.s3...).\nOrigin access: Reselect the S3-OAC-HA created earlier.\nName: Failover-US.\nClick Create origin. Step 3.3: Create Origin Group (Enable High Availability) Still on the Origins tab \u003e Click Create origin group.\nName: HighAvailability-Group.\nOrigins:\nAdd Primary-SG (Up - Priority 1).\nAdd Failover-US (Down - Priority 2).\nFailover criteria: Select: 500, 502, 503, 504.\nClick Create origin group.\nWe will have 2 origins and 1 origin group:\nStep 3.4: Update Behavior Behaviors Tab \u003e Select Default (*) \u003e Edit.\nOrigin and origin groups: Change from Primary-SG to HighAvailability-Group.\nClick Save changes.\nStep 3.5: Configure SPA Routing (Handle 404 React errors) Tab Error pages \u003e Create custom error response.\nRule 1 (For OAC):\nHTTP error code: 403.\nCustomize error response: Yes.\nResponse page path: /index.html.\nHTTP response code: 200.\nRule 2 (For React Router): Create another similar one for the code 404. (Path is still /index.html, code 200). ⬅ STEP 3: Route 53 and ACM\rSTEP 5: S3 Policy ➡",
    "description": "STAGE 3: CONFIGURATION CLOUDFRONT (CDN \u0026 FAILOVER) STEP 3.1: Create Distribution Step 1: Get started Distribution name: sgutodolist-frontend-cloudfront\nDistribution type: Keep Single website or app.\nDomain:\nIn the Route 53 managed domain box, enter: sgutodolist.com.\n(If there is an Alternate domain names box, enter www.sgutodolist.com if the interface allows, otherwise we will add it later).\nClick Next.\nStep 2: Specify origin Origin type: Select Amazon S3.\nOrigin:\nClick on the search box, select bucket Singapore: sgutodolist-frontend-sg....",
    "tags": [],
    "title": "ClouFront and Failover",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.4-cloufront-and-failover/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026 Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS CLOUD MASTERY SERIES #3 – Security on AWS\nDate \u0026 Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee",
    "description": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026 Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS CLOUD MASTERY SERIES #3 – Security on AWS",
    "tags": [],
    "title": "EventParticipated",
    "uri": "/4-eventparticipated/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "Task Definitions serve as blueprints that define how ECS should run containers, including resource allocation (RAM/CPU) and essential environment variables for service connectivity.\nPreparation Record the following values before starting:\nRDS Endpoint: sgu-todolist-db.[random].ap-southeast-1.rds.amazonaws.com\nRedis Endpoint: sgu-redis.[random].cache.amazonaws.com (Without :6379)\nECR Image URIs: The URIs of the 6 repositories from the Image Build step.\nGoogle OAuth Credentials: Client ID and Client Secret.\nStandard Task Definition Process For each service, follow this standard configuration process:\n1. Base Configuration:\nNavigate to Amazon ECS → Task definitions → Create new task definition.\nInfrastructure:\nLaunch type: AWS Fargate\nOperating system: Linux/X86_64\nTask Execution Role: ecsTaskExecutionRole\nContainer Details:\nProtocol: TCP\nPort Mapping: Refer to the specific service details below.\nLog collection: Turned on (Essential for debugging).\n1. Kafka Server (Infrastructure Backbone) Note: Kafka must be deployed first so other services can connect upon startup.\nParameter Value Family Name kafka-server-td Task Memory 2 GB (Critical) Task CPU 1 vCPU Container Port 9092 Environment Variables (Copy exactly):\nKey Value ALLOW_PLAINTEXT_LISTENER yes KAFKA_CFG_ADVERTISED_LISTENERS PLAINTEXT://kafka.sgu.local:9092 KAFKA_CFG_CONTROLLER_LISTENER_NAMES CONTROLLER KAFKA_CFG_CONTROLLER_QUORUM_VOTERS 0@127.0.0.1:9093 KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT KAFKA_CFG_LISTENERS PLAINTEXT://:9092,CONTROLLER://:9093 KAFKA_CFG_LOG_DIRS /tmp/kafka-logs KAFKA_CFG_NODE_ID 0 KAFKA_CFG_PROCESS_ROLES controller,broker KAFKA_HEAP_OPTS -Xmx512m 2. Auth Service (Critical Security Service) Note: This service requires high RAM to handle Login processes and Encryption.\nParameter Value Family Name auth-service-td Task Memory 2 GB (Mandatory to prevent Exit Code 137) Task CPU 0.5 vCPU Container Port 9999 Environment Variables:\nKey Value Notes SERVER_PORT 9999 JAVA_OPTS -Xmx768m Must include the hyphen -. SPRING_JPA_HIBERNATE_DDL_AUTO none CRITICAL: Prevents 500 Errors caused by DB schema conflicts. SERVER_FORWARD_HEADERS_STRATEGY native Fixes HTTP/HTTPS redirect issues behind ALB. SPRING_DATASOURCE_URL jdbc:mysql://[RDS-ENDPOINT]:3306/aws_todolist_database?allowPublicKeyRetrieval=true\u0026useSSL=false\u0026serverTimezone=UTC SPRING_DATASOURCE_USERNAME root SPRING_DATASOURCE_PASSWORD [DB_PASSWORD] SPRING_DATASOURCE_HIKARI_MAXIMUM_POOL_SIZE 5 SPRING_DATA_REDIS_HOST [REDIS-ENDPOINT] SPRING_DATA_REDIS_PORT 6379 SPRING_DATA_REDIS_SSL_ENABLED true Mandatory for AWS ElastiCache. SPRING_KAFKA_BOOTSTRAP_SERVERS kafka.sgu.local:9092 DOMAIN_FRONTEND https://sgutodolist.com APP_OAUTH2_REDIRECT_URI https://sgutodolist.com/oauth2/redirect SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_GOOGLE_CLIENT_ID [GOOGLE_CLIENT_ID] SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_GOOGLE_CLIENT_SECRET [GOOGLE_CLIENT_SECRET] SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_GOOGLE_REDIRECT_URI https://sgutodolist.com/api/auth/login/oauth2/code/google Note: Include /api/auth in path. 3. User Service \u0026 Taskflow Service These are business logic services requiring permission to create DB tables on first run.\nParameter Value Family Name user-service-td / taskflow-service-td Task Memory 1 GB Task CPU 0.5 vCPU Container Port 8081 (User) / 8082 (Taskflow) Environment Variables (Common for both):\nKey Value Notes SERVER_PORT 8081 (User) or 8082 (Taskflow) JAVA_OPTS -Xmx512m SPRING_JPA_HIBERNATE_DDL_AUTO update Allows service to create tables on startup. SPRING_DATASOURCE_URL (Same as Auth Service) SPRING_DATASOURCE_USERNAME root SPRING_DATASOURCE_PASSWORD [DB_PASSWORD] SPRING_DATASOURCE_HIKARI_MAXIMUM_POOL_SIZE 5 SPRING_DATA_REDIS_HOST [REDIS-ENDPOINT] SPRING_DATA_REDIS_PORT 6379 SPRING_DATA_REDIS_SSL_ENABLED true SPRING_KAFKA_BOOTSTRAP_SERVERS kafka.sgu.local:9092 4. API Gateway (The Orchestrator) The most critical service for internal routing.\nParameter Value Family Name api-gateway-td Task Memory 1 GB Task CPU 0.5 vCPU Container Port 8080 Environment Variables:\nKey Value Explanation SERVER_PORT 8080 CORS_ALLOWED_ORIGINS https://sgutodolist.com,https://www.sgutodolist.com Allows Frontend API access. AUTH_SERVICE_URL http://auth.sgu.local:9999 Internal routing via Service Discovery. USER_SERVICE_URL http://user.sgu.local:8081 Note the name: user (not user-service). TASKFLOW_SERVICE_URL http://taskflow.sgu.local:8082 Note the name: taskflow. NOTIFICATION_SERVICE_URL http://notification.sgu.local:9998 Note the name: notification. SPRING_DATA_REDIS_HOST [REDIS-ENDPOINT] Used for Rate Limiting. SPRING_DATA_REDIS_PORT 6379 SPRING_DATA_REDIS_SSL_ENABLED true Task Definition Validation Checklist After creation, please verify:\nRAM Allocation: Auth Service must be 2GB, others at least 1GB.\nRedis SSL: Variable SPRING_DATA_REDIS_SSL_ENABLED = true is set for all services connecting to Redis.\nInternal URLs: All _SERVICE_URL variables in Gateway must end with .sgu.local.\nDDL Auto: Auth Service must be none, User/Taskflow should be update.\n⬅ STEP 3: Code Update \u0026 Image Build\rSTEP 5: Services Deployment ➡",
    "description": "Task Definitions serve as blueprints that define how ECS should run containers, including resource allocation (RAM/CPU) and essential environment variables for service connectivity.\nPreparation Record the following values before starting:\nRDS Endpoint: sgu-todolist-db.[random].ap-southeast-1.rds.amazonaws.com\nRedis Endpoint: sgu-redis.[random].cache.amazonaws.com (Without :6379)\nECR Image URIs: The URIs of the 6 repositories from the Image Build step.\nGoogle OAuth Credentials: Client ID and Client Secret.\nStandard Task Definition Process For each service, follow this standard configuration process:",
    "tags": [],
    "title": "Task Definitions Creation",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.4-task-definitions-creation-configure-settings-fix-environment-variables/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 4 Objectives: Learn about Amazon S3 Storage Service Learn about Security Services on AWS Implement static web application deployment through S3 and cloudfront Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures on S3 storage service - Translate blogs for week 4 - Hold weekly team meetings to update work progress. 9/29/2025 9/29/2025 original link 3 - Practice: + lab57: STARTING WITH AMAZON S3 + lab13: Deploy AWS Backup to the System + lab 14: VM Import/Export - Research and make a list of APIs needed for the TaskFlow Service. 9/30/2025 9/30/2025 https://000057.awsstudygroup.com https://000014.awsstudygroup.com https://000013.awsstudygroup.com/ 4 - Watch video lectures on security services on AWS 10/1/2025 10/1/2025 5 - Practice: + lab2: AWS Identity and Access Management (IAM) Access Control + lab44: IAM Role \u0026 Condition + lab 48: Granting authorization for an application to access AWS services with an IAM role. 10/2/2025 10/2/2025 https://000048.awsstudygroup.com https://000002.awsstudygroup.com https://000044.awsstudygroup.com 6 - Practice: + lab30: LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY - Update the worklog. 3/10/2025 3/10/2025 https://000030.awsstudygroup.com Week 4 Achievements: Understand about storage services on S3\nPractice hosting a static website using Amazon S3:\nKnow how to initialize an S3 and upload data of the static website to it Practice configuring AWS CloudFront to host a static website on S3 without having to public information about the bucket Learn about the Bucket Versioning to preserve and restore versions of all objects stored in the bucket Learn about the Amzon S3 Cross-Region Replication (CRR) to automatically copy objects across different AWS regions Practice implementing backups for the system, understand the two main concepts of RPO and RTO\nLearn how to import virtual machines from VMware to Amazon EC2 instances and migrate EC2 instances back to on-premises VMware environments.\nPractice managing system access using AWS IAM by:\nCreating an IAM user with permissions to access Amazon S3, and using the user’s access keys to upload files from a server to an S3 bucket. Creating an IAM role for EC2 instances that grants S3 upload permissions, enabling secure access to S3 without embedding access keys (thus avoiding credential exposure). Practice using IAM PERMISSION BOUNDARY to limit user permissions, thereby simplifying permission management and avoiding the assignment of unnecessary privileges.\nResearch and create a list of APIs required for the Taskflow service.",
    "description": "Week 4 Objectives: Learn about Amazon S3 Storage Service Learn about Security Services on AWS Implement static web application deployment through S3 and cloudfront Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video lectures on S3 storage service - Translate blogs for week 4 - Hold weekly team meetings to update work progress. 9/29/2025 9/29/2025 original link 3 - Practice: + lab57: STARTING WITH AMAZON S3 + lab13: Deploy AWS Backup to the System + lab 14: VM Import/Export - Research and make a list of APIs needed for the TaskFlow Service. 9/30/2025 9/30/2025 https://000057.awsstudygroup.com https://000014.awsstudygroup.com https://000013.awsstudygroup.com/ 4 - Watch video lectures on security services on AWS 10/1/2025 10/1/2025 5 - Practice: + lab2: AWS Identity and Access Management (IAM) Access Control + lab44: IAM Role \u0026 Condition + lab 48: Granting authorization for an application to access AWS services with an IAM role. 10/2/2025 10/2/2025 https://000048.awsstudygroup.com https://000002.awsstudygroup.com https://000044.awsstudygroup.com 6 - Practice: + lab30: LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY - Update the worklog. 3/10/2025 3/10/2025 https://000030.awsstudygroup.com Week 4 Achievements: Understand about storage services on S3",
    "tags": [],
    "title": "Week 4",
    "uri": "/1-worklog/1.4-week4/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Creating a Multi-Region Application with AWS Services – Part 1, Compute, Networking, and Security Joe Chapman và Sebastian Leks | 08/12/2021 | Amazon CloudFront, Amazon EC2, Amazon Elastic Block Store (Amazon EBS), Amazon Route 53, Amazon Simple Storage Service (S3), Amazon VPC, Architecture, AWS CloudTrail, AWS Global Accelerator, AWS Identity and Access Management (IAM), AWS Secrets Manager, AWS Security Hub, AWS Transit Gateway, AWS Well-Architected | Permalink\nMany AWS services have features to help you build and manage a multi-Region architecture, but identifying those capabilities across 200+ services can be overwhelming.\nIn this 3-part blog series, we filter through those 200+ services and focus on those that have specific features to assist you in building multi-Region applications. In Part 1, we’ll build a foundation with AWS security, networking, and compute services. In Part 2, we’ll add in data and replication strategies. Finally, in Part 3, we’ll look at the application and management layers. As we go through each part, we’ll build up an example application to display one way of combining these services to create a multi-Region application.\nConsiderations before getting started AWS Region are built with multiple isolated and physically separate Availbility Zone. . This approach allows you to create highly available Well-Architected workloads that span AZs to achieve greater fault tolerance. This satisfies the availability goals for most applications, but there are some general reasons that you may be thinking about expanding beyond a single Region:\nExpansion to a global audience as an application grows and its user base becomes more geographically dispersed, there can be a need to reduce latencies for different parts of the world.\nReducing Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) as part of a multi-Region disaster recovery (DR) plan.\nLocal laws and regulationsmay have strict data residency and privacy requirements that must be followed.\nIf you’re building a new multi-Region application, you may want to consider focusing on AWS services that have built-in functionality to assist. Existing applications will need to be further examined to determine the most expandable architecture to support its growth. The following sections review these services, and highlight use cases and best practices.\nIdentity and access across Regions Creating a security foundation starts with setting proper authentication and authorization rules. The system handling these requests must be highly resilient to verify and authorize requests quickly and reliably. AWS Identity and Access Management (IAM) accomplishes this by creating a reliable mechanism for you to manage access to AWS services and resources. IAM has multi-Region availability automatically, with no configuration required on your part.\nFor help managing Windows users, devices, and applications on a multi-Region network, you can set up AWS Directory Service for Microsoft Active Directory Enterprise Edition to automatically replicate directory data across Regions..This reduces directory lookup latencies by using the closest directory and creates durability by spanning multiple Regions. Note that this will also introduce a shared fate across domain controllers for multi-Region topologies, because group policy changes will be propagated to all member servers.\nApplications that need to securely store, rotate, and audit secrets, such as database passwords, should use AWS Secrets Manager. This service encrypts secrets with AWS Key Management Service (AWS KMS) keys and can replicate secrets to secondary Regions to ensure applications are able to quickly retrieve a secret in the closest Region.\nEncryption across Regions AWS KMS can be used to encrypt data at rest, and is used extensively for encryption across AWS services. By default, keys are confined to a single Region. AWS services such as AWS như Amazon Simple Storage Service (Amazon S3) scross-Region replication and Amazon Aurora Global Database (both covered in phần 2), simplify the process of encryption and decryption with different keys in each Region. For other parts of your multi-Region application that rely on KMS keys, you can set up AWS KMS multi-Region keys to replicate the key material and key ID to a second Region. This eliminates the need to decrypt and re-encrypt data with a different key in each Region. For example, multi-Region keys can be used to reduce the complexity of a multi-Region application’s encryption operations for data that is stored across Regions.\nAuditing and observability across Regions It is a best practice to configure AWS CloudTrail to keep a record of all relevant AWS API activity in your account for auditing purposes. When you utilize multiple Regions or accounts, these CloudTrail logs should be aggregated into a single Amazon S3 bucket for easier analysis. To prevent misuse, the centralized logs should be treated with higher severity, with only limited access to key systems and personnel.\nTo stay on top of AWS Security Hub findings, you can aggregate and link findings from multiple locations to a single Region. This is an easy way to create a centralized view of Security Hub findings across accounts and Regions. Once set up, the findings are continuously synced between Regions to keep you updated on global results in a single dashboard.\nWe put these features together in Figure 1. We used IAM to grant fine-grained access to AWS services and resources, Directory Service for Microsoft AD for authentication to Microsoft applications, and Secrets Manager to store sensitive database credentials. Our data, which moves freely between Regions, is encrypted with KMS multi-Region keys, and all AWS API access is logged with CloudTrail and aggregated to a central S3 bucket that only our security team has access to.\nFigure 1. Multi-Region security, identity, and compliance services\nBuilding a global network For resources launched into virtual networks in different Regions, Amazon Virtual Cloud (Amazon VPC) allows private routing between Regions and accounts with VPC peering. These resources can communicate using private IP addresses and do not require an internet gateway, VPN, or separate network appliances. This feature works well for smaller networks that only require a few peering connections. However, transitive routing is not allowed, and as the number of peered virtual private cloud (VPCs) increases, the mesh of peered connections can become difficult to manage and troubleshoot.\nAWS Transit Gateway reduces these difficulties by creating a network transit hub that connects your VPCs and on-premises networks. A Transit Gateway’s routing capabilities can expand to additional Regions with Transit Gateway inter-Region peering to create a globally distributed, private network for your resources.\nBuilding a reliable, cost-effective way to route users to distributed Internet applications requires highly available and scalable Domain Name System (DNS) records. Amazon Route 53 does exactly that.\nRoute 53 includes many routing policies. For example, you can route a request to a record with the lowest network latency, or send users in a specific geolocation to a localized application endpoint. For DR, Route 53 Application Recovery Controller (Route 53 ARC) offers a comprehensive failover solution with minimal dependencies. Route 53 ARC routing policies, safety checks, and readiness checks help you to failover across Regions, AZs, and on-premises reliably.\nRoute 53 includes many routing policies. For example, you can route a request to a record with the lowest network latency, or send users in a specific geolocation to a localized application endpoint. For DR, Route 53 Application Recovery Controller (Route 53 ARC) offers a comprehensive failover solution with minimal dependencies. Route 53 ARC routing policies, safety checks, and readiness checks help you to failover across Regions, AZs, and on-premises reliably.\nThe Amazon CloundFront content delivery network is global, built across 300+ points of presence (PoP) spread throughout the world. Applications that have multiple possible origins, such as across Regions, can use CloudFront origin failover to automatically fail over to a recovery origin when the primary is not available. CloudFront’s capabilities expand beyond serving content, with the ability to run compute at the edge. CloudFront Functions make it easy to run lightweight JavaScript code, and AWS Lambda@Edge enables you to run Node.js and Python functions closer to users of your application, which improves performance and reduces latency. By placing compute at the edge, you can take load off of your origin and provide quicker responses for your global end users.\nBuilt on the AWS global network, AWS Global Accelerator provides two static anycast IPs to give a single-entry point for internet-facing applications. You can seamlessly add or remove origins while continuing to automatically route traffic to the closest healthy Regional endpoint. If a failure is detected, Global Accelerator will automatically redirect traffic to a healthy endpoint within seconds, with no changes to the static IP.\nFigure 2 uses a Route 53 latency-based routing policy to route users to the quickest endpoint, CloudFront is used to serve static content such as videos and images, and Transit Gateway creates a global private network for our devices to talk securely across Regions.\nBuilding and managing the compute layer\nXây dựng và quản lý lớp tính toán (compute layer) Although Amazon Elastic Compute Cloud (Amazon EC2) instances and their associated Amazon Elastic Block Store (Amazon EBS) volumes reside in a single AZ Amazon Data Lifecycle Manager can automate the process of taking and copying EBS snapshots across Regions. This can enhance DR strategies by providing an easy cold backup-and-restore option for EBS volumes. If you need to back up more than just EBS volumes, AWS Backup provides a central place to do this across multiple services and is covered in part 2.\nAn Amazon EC2 instance is based on an Amazon Machine Image (AMI). An AMI specifies instance configurations such as the instance’s storage, launch permissions, and device mappings. When a new standard image needs to be created and released, EC2 Image Builder simplifies the building, testing, and deployment of new AMIs. It can also help with copying of AMIs to additional Regions to eliminate needing to manually copy source AMIs to target Regions.\nMicroservice-based applications that use containers benefit from quicker start-up times. Amazon Elastic Container Registry (Amazon ECR) can help ensure this happens consistently across Regions with private image replication at the registry level. An ECR private registry can be configured for either cross-Region or cross-account replication to ensure your images are ready in secondary Regions when needed.\nAs an architecture expands into multiple Regions, it can become difficult to track where resources are provisioned. Amazon EC2 Global View helps alleviate this by providing a centralized dashboard to see Amazon EC2 resources such as instances, VPCs, subnets, security groups, and volumes in all active Regions.\nWe bring these compute layer features together in Figure 3 by using EC2 Image Builder to copy our latest golden AMI across Regions for deployment. We also back up each EBS volume for 3 days and replicate it across Regions using Data Lifecycle Manager.\nFigure 3. AMI and EBS snapshot copy across Regions\nBringing it together At the end of each part of this blog series, we build on a sample application based on the services covered. This shows you how to bring these services together to build a multi-Region application with AWS services. We don’t use every service mentioned, just those that fit the use case.\nWe built this example to expand to a global audience. It requires high availability across Regions, and favors performance over strict consistency. We have chosen the following services covered in this post to accomplish our goals:\nA Route 53 latency routing policy that routes users to the deployment with the least latency.\nCloudFront is set up to serve our static content. Region 1 is our primary origin, but we’ve configured origin failover to Region 2 in case of a disaster.\nThe application relies on several third-party APIs, so Secrets Manager with cross-Region replication has been set up to store sensitive API key information.\nWe centralize our CloudTrail logs in Region 1 for easier analysis and auditing.\nSecurity Hub in Region 1 is where we have chosen to aggregate findings from all Regions.\nThis is a containers-based application, and we rely on Amazon ECR replication for each location to quickly pull the latest images locally.\nTo communicate using private IPs across Regions, a Transit Gateway is set up in each Region with intra-Region between them. VPC peering could have also worked, but we expect to expand to several more Regions in the future and decided this would be the better long-term choice.\nIAM is used to grant access to manage our AWS resources.\nFigure 4. Building an application with AWS multi-Region services using services covered in Part 1\nSummary It’s important to create a solid foundation when architecting a multi-Region application. These foundations lay the groundwork for you to move fast in a secure, reliable, and elastic way as you build out your application. Many AWS services include native features to help you build a multi-Region architecture. Your architecture will be different depending on the reason for expanding beyond a single Region. In this post, we covered specific features across AWS security, networking, and compute services that have built-in functionality to take away some of the undifferentiated heavy lifting. We’ll cover data, application, and management services in future posts.\nReady to get started? We’ve chosen some AWS Solutions and AWS Blogs để hỗ trợ bạn!\nLooking for more architecture content? AWS Architecture Center provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more!\nLink bài viết gốc: https://aws.amazon.com/blogs/architecture/creating-a-multi-region-application-with-aws-services-part-1-compute-and-security/",
    "description": "Creating a Multi-Region Application with AWS Services – Part 1, Compute, Networking, and Security Joe Chapman và Sebastian Leks | 08/12/2021 | Amazon CloudFront, Amazon EC2, Amazon Elastic Block Store (Amazon EBS), Amazon Route 53, Amazon Simple Storage Service (S3), Amazon VPC, Architecture, AWS CloudTrail, AWS Global Accelerator, AWS Identity and Access Management (IAM), AWS Secrets Manager, AWS Security Hub, AWS Transit Gateway, AWS Well-Architected | Permalink\nMany AWS services have features to help you build and manage a multi-Region architecture, but identifying those capabilities across 200+ services can be overwhelming.",
    "tags": [],
    "title": " Blog 5",
    "uri": "/3-blogstranslated/3.5-blog5/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop",
    "content": "1. Hugo Commands 1.1\nhugo build\r1.2\nhugo server -D\r2. Git Commands 2.1 Check repository status\ngit status\r2.2 List local branches\ngit branch\r2.3 List all branches (local + remote)\ngit branch -a\r2.4 Switch to another branch\ngit checkout \u003cbranch-name\u003e\r2.5 Create and switch to a new branch\ngit checkout -b \u003cbranch-name\u003e\r2.6 Pull the latest updates from remote\ngit pull\r2.7 Fetch updates without merging\ngit fetch\r2.8 Add files to staging\ngit add \u003cfile\u003e\rAdd all changes:\ngit add .\r2.9 Commit changes\ngit commit -m \"your message\"\r2.10 Push changes to remote\ngit push\rPush a new branch for the first time:\ngit push -u origin \u003cbranch-name\u003e\r2.11 View remote repositories\ngit remote -v\r2.12 Merge a branch into the current branch\ngit merge \u003cbranch-name\u003e # Example: git checkout main git merge developer\r2.13 Delete a branch\nDelete local branch:\ngit branch -d \u003cbranch-name\u003e\rDelete remote branch:\ngit push origin --delete \u003cbranch-name\u003e\r2.14 Undo changes\nDiscard changes in a file:\ngit checkout -- \u003cfile\u003e\rReset everything to the last commit:\ngit reset --hard\r3. PowerShell Commands 3.1 Automatically update _index.md for all weeks in a specific directory\n# Target directory path $targetDir = \"content/1-Worklog/1.3-DuongBinhMinh\" # Check if directory exists if (-not (Test-Path $targetDir)) { Write-Host \"Error: Directory not found $targetDir\" -ForegroundColor Red break } # Get subfolders $folders = Get-ChildItem -Path $targetDir -Directory foreach ($folder in $folders) { if ($folder.Name -match \"Week_(\\d+)\") { $weekNum = $matches[1] $filePath = Join-Path $folder.FullName \"_index.md\" if (Test-Path $filePath) { # Read file content $content = Get-Content -Path $filePath -Raw -Encoding UTF8 # Generate new pre value $newPreLine = \"pre = `\" \u003cb\u003e 1.3.$weekNum. \u003c/b\u003e `\"\" # Replace old pre line $newContent = $content -replace '(?m)^pre\\s*=\\s*\".*\"', $newPreLine # Overwrite file Set-Content -Path $filePath -Value $newContent -Encoding UTF8 Write-Host \"Updated Week $($weekNum): $newPreLine\" -ForegroundColor Green } else { Write-Host \"Skipped $folder.Name (_index.md not found)\" -ForegroundColor Yellow } } } Write-Host \"Update completed!\" -ForegroundColor Cyan\r3.2 Standardize folder structure, convert Leaf Bundles → Branch Bundles, fix frontmatter and internal links\n# --- CONFIGURATION --- $worklogPath = \"content/1-Worklog\" # --- HELPER FUNCTION: Rename index.md to _index.md --- Function Fix-IndexFileName ($dirPath) { $wrongFile = Join-Path $dirPath \"index.md\" $correctFile = Join-Path $dirPath \"_index.md\" if (Test-Path $wrongFile) { if (-not (Test-Path $correctFile)) { Rename-Item -Path $wrongFile -NewName \"_index.md\" Write-Host \" [File] Renamed index.md -\u003e _index.md\" -ForegroundColor Magenta } } } Write-Host \"=== STARTING HUGO DATA STANDARDIZATION ===\" -ForegroundColor Cyan # Get all user directories (e.g., 1.1, 1.2, 1.3 ...) $userFolders = Get-ChildItem -Path $worklogPath -Directory foreach ($userFolder in $userFolders) { if ($userFolder.Name -match \"^(\\d+\\.\\d+)-\") { $userPrefix = $matches[1] Write-Host \"`n--- Processing User: $($userFolder.Name) (Prefix: $userPrefix) ---\" -ForegroundColor Cyan # Fix parent index file Fix-IndexFileName $userFolder.FullName $parentIndexFile = Join-Path $userFolder.FullName \"_index.md\" if (Test-Path $parentIndexFile) { $pContent = Get-Content -Path $parentIndexFile -Raw -Encoding UTF8 # Fix Hugo shortcode format $pContent = $pContent -replace '\\{\\{\u003c\\s*relref', ('{' + '{% relref') $pContent = $pContent -replace '\u003e\\}\\}\\)', ('%' + '}})') # Fix broken internal links $pContent = [Regex]::Replace($pContent, 'relref\\s*\"[^\"]*?Week_(\\d+)\"', { param($m) $w = $m.Groups[1].Value return 'relref \"' + \"$userPrefix.$w-Week_$w\" + '\"' }) Set-Content -Path $parentIndexFile -Value $pContent -Encoding UTF8 Write-Host \" [Link] Updated links in parent _index.md\" -ForegroundColor Green } # Process week folders $weekFolders = Get-ChildItem -Path $userFolder.FullName -Directory | Where-Object { $_.Name -match \"Week_\" } foreach ($weekFolder in $weekFolders) { if ($weekFolder.Name -match \"Week_(\\d+)\") { $weekNum = $matches[1] $correctFolderName = \"$userPrefix.$weekNum-Week_$weekNum\" $currentFolderPath = $weekFolder.FullName if ($weekFolder.Name -ne $correctFolderName) { Rename-Item -Path $currentFolderPath -NewName $correctFolderName $currentFolderPath = Join-Path $userFolder.FullName $correctFolderName Write-Host \" [Folder] Renamed to: $correctFolderName\" -ForegroundColor Yellow } Fix-IndexFileName $currentFolderPath $childIndexFile = Join-Path $currentFolderPath \"_index.md\" if (Test-Path $childIndexFile) { $cContent = Get-Content -Path $childIndexFile -Raw -Encoding UTF8 $newPre = \"pre = `\" \u003cb\u003e $userPrefix.$weekNum. \u003c/b\u003e `\"\" if ($cContent -match '(?m)^pre\\s*=') { $cContent = $cContent -replace '(?m)^pre\\s*=\\s*\".*\"', $newPre } else { $cContent = $cContent -replace '(?m)^weight\\s*=\\s*(\\d+)', \"weight = `$1`n$newPre\" } Set-Content -Path $childIndexFile -Value $cContent -Encoding UTF8 Write-Host \" [Pre] Updated pre to: $userPrefix.$weekNum.\" -ForegroundColor Gray } } } } } Write-Host \"`n=== COMPLETED! PLEASE RUN: hugo server -D ===\" -ForegroundColor Green\r3.3 Create multiple folders at once\n$basePath = \"D:\\IT\\AWS-FCJ\\AWS-Workshop\\content\\5-Workshop\" $folders = @( \"5.1-Workshop_Overview\", \"5.2-Prerequisite\", \"5.3-Deploy_Flow\", \"5.4-Clean_Up\" ) foreach ($f in $folders) { $fullPath = Join-Path $basePath $f New-Item -ItemType Directory -Path $fullPath -Force | Out-Null New-Item -ItemType File -Path (Join-Path $fullPath \"_index.md\") -Force | Out-Null }\r3.4 Create a single folder\n$basePath = \"D:\\IT\\AWS-FCJ\\AWS-Workshop\\content\\5-Workshop\" $folderName = \"5.1-Workshop_Overview\" $fullPath = Join-Path $basePath $folderName New-Item -ItemType Directory -Path $fullPath -Force | Out-Null New-Item -ItemType File -Path (Join-Path $fullPath \"_index.md\") -Force | Out-Null\r3.5 Display the entire project structure\ntree /f /a\r3.6 Display a specific directory structure\ntree content/1-Worklog/1.1-PhanCanhTuanDat /F\r3.7 Recreate gh-pages worktree safely\n# 0. Remove current public folder Remove-Item -Recurse -Force .\\public # 1. Remove old gh-pages worktree (even if folder no longer exists) git worktree remove \"D:/IT/AWS-FCJ/AWS-Workshop/public/public\" --force # 2. Clean orphaned worktree metadata git worktree prune # 3. Verify remaining worktrees git worktree list # 4. Recreate gh-pages worktree at /public git worktree add -B gh-pages public origin/gh-pages # 5. Verify status cd public git status",
    "description": "1. Hugo Commands 1.1\nhugo build\r1.2\nhugo server -D\r2. Git Commands 2.1 Check repository status\ngit status\r2.2 List local branches\ngit branch\r2.3 List all branches (local + remote)\ngit branch -a\r2.4 Switch to another branch\ngit checkout \u003cbranch-name\u003e\r2.5 Create and switch to a new branch\ngit checkout -b \u003cbranch-name\u003e\r2.6 Pull the latest updates from remote\ngit pull\r2.7 Fetch updates without merging",
    "tags": [],
    "title": "Mystic Skills",
    "uri": "/5-workshop/5.5-mystic-skills/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "STAGE 4: S3 PERMITS (POLICY) CloudFront needs a “permission” to pull files from your 2 closed buckets.\nIn CloudFront \u003e Origins tab.\nSelect Origin Singapore \u003e Edit \u003e Copy policy.\nOpen a new tab \u003e S3 Console \u003e Bucket sgutodolist-frontend-sg.\nPermissions tab \u003e Bucket Policy \u003e Edit \u003e Paste \u003e Save.\nRepeat with Bucket Virginia:\nGo back to CloudFront \u003e Select Origin Virginia \u003e Edit \u003e Copy policy.\nGo to S3 sgutodolist-frontend-us \u003e Permissions \u003e Bucket Policy \u003e Paste \u003e Save.\n⬅ STEP 4: ClouFront and Failover\rSTEP 6: DNS Record ➡",
    "description": "STAGE 4: S3 PERMITS (POLICY) CloudFront needs a “permission” to pull files from your 2 closed buckets.\nIn CloudFront \u003e Origins tab.\nSelect Origin Singapore \u003e Edit \u003e Copy policy.\nOpen a new tab \u003e S3 Console \u003e Bucket sgutodolist-frontend-sg.\nPermissions tab \u003e Bucket Policy \u003e Edit \u003e Paste \u003e Save.\nRepeat with Bucket Virginia:\nGo back to CloudFront \u003e Select Origin Virginia \u003e Edit \u003e Copy policy.\nGo to S3 sgutodolist-frontend-us \u003e Permissions \u003e Bucket Policy \u003e Paste \u003e Save.",
    "tags": [],
    "title": "S3 Policy",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.5-s3-policy/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase deploys containerized applications to ECS Fargate following a specific order to ensure proper dependency resolution.\nCritical Network Configuration For ALL service deployments, the following network settings are mandatory:\n1. VPC: SGU-Microservices-VPC\n2. Subnets: Select both Public Subnets (required for image pulling)\n3. Security Group: ecs-app-sg (remove default)\n4. Public IP: Turned on (critical for ECR access)\nDeployment Priority Order Services must be deployed in the following sequence to respect dependencies:\nGroup 1: Business Logic Services (Deploy first - required by API Gateway)\n1. Auth Service\n2. User Service\n3. Taskflow Service\n4. Notification Service\nGroup 2: Internal Services (Deploy second) 5. AI Model Service\nGroup 3: Entry Point (Deploy last) 6. API Gateway\nGroup 1: Business Logic Services Deployment These services require both ALB integration (for external access) and Service Discovery (for internal communication).\nDeployment Parameters Reference Table:\n| Service | Task Definition | Service Name | Target Group | Service Discovery Name |\n| — | — | — | — | — |\n| Auth | auth-service-td | auth-service | auth-tg | auth |\n| User | user-service-td | user-service | user-tg | user |\n| Taskflow | taskflow-service-td | taskflow-service | task-tg | taskflow |\n| Notification | notification-service-td | notification-service | noti-tg | notification |\nStandard Deployment Process (Repeat 4 times):\n1. Navigate to ECS Cluster → Services → Create\n2. Service Configuration:\n- Task definition family: Select corresponding task definition (e.g., auth-service-td)\n- Revision: Latest\n- Service name: Enter service name (e.g., auth-service)\n3. Compute Configuration:\n- Compute options: Capacity provider strategy\n- Capacity provider strategy: Use custom (Advanced)\n- Capacity provider: FARGATE\n- Base: 0\n- Weight: 1\n- Platform version: LATEST\n4. Deployment Configuration:\n- Application type: Service\n- Scheduling strategy: Replica\n- Desired tasks: 1\n- Desired tasks: 1\n- 5. Networking:\n- VPC: SGU-Microservices-VPC\n- Subnets: Select both Public Subnets\n- SGU-Microservices-subnet-public1-ap-southeast-1a\n- SGU-Microservices-subnet-public2-ap-southeast-1b\n- Security group: Remove default, select ecs-app-sg\n- Public IP: Turned on\n6. Service Connect:\n- Do not enable (using traditional DNS-based Service Discovery)\n7. Service Discovery:\n- Enable: Use service discovery\n- Configure namespace: Select an existing namespace\n- Namespace: sgu.local\n- Configure service discovery service: Select an existing service discovery service\n- Existing service discovery service: Choose discovery name (e.g., auth)\n- Result DNS: auth.sgu.local (for example)\n8. Load Balancing:\n- Enable: Use load balancing\n- Load balancer type: Application Load Balancer\n- Load balancer: sgu-alb\n- Container to load balance: Select service container (e.g., auth-service 9999:9999)\n- Listener: Use an existing listener → HTTPS:443\n- Target group: Use an existing target group → Select corresponding TG (e.g., auth-tg)\n9. Service Auto Scaling:\n- Disable (for cost optimization)\n10. Click Create\nWait for each service to reach RUNNING state before deploying the next one.\nGroup 2: AI Model Service Deployment This is an internal service accessed only through API Gateway.\nConfiguration:\n1. Navigate to ECS Cluster → Services → Create\n2. Service Configuration:\n- Task definition: ai-model-service-td\n- Service name: ai-model-service\n- Desired tasks: 1\n3. Networking:\n- VPC: SGU-Microservices-VPC\n- Subnets: Both Public Subnets\n- Security group: ecs-app-sg\n- Public IP: Turned on\n4. Load Balancing:\n- Disable (internal service only)\n5. Service Discovery:\n- Enable: Use service discovery\n- Namespace: sgu.local\n- Service discovery name: ai-model\n- Result DNS: ai-model.sgu.local\n6. Click Create\nGroup 3: API Gateway Deployment This is the main entry point that routes traffic to all backend services.\nConfiguration:\n1. Navigate to ECS Cluster → Services → Create\n2. Service Configuration:\n- Task definition: api-gateway-td\n- Service name: api-gateway\n- Desired tasks: 1\n3. Compute Configuration:\n- Same as Group 1 services\n4. Networking:\n- VPC: SGU-Microservices-VPC\n- Subnets: Both Public Subnets\n- Security group: ecs-app-sg\n- Public IP: Turned on\n5. Service Discovery:\n- Enable: Use service discovery\n- Namespace: sgu.local\n- Service discovery name: api-gateway\n6. Load Balancing:\n- Enable: Use load balancing\n- Load balancer type: Application Load Balancer\n- Load balancer: sgu-alb\n- Container to load balance: api-gateway:8080\n- Listener: Use an existing listener → HTTP:80\n- Target group: Use an existing target group → api-gateway-tg\n7. Click Create\nDeployment Monitoring and Troubleshooting After deploying all services, monitor their status in the ECS Cluster.\nExpected Task States:\nNavigate to Cluster → Tasks tab\n1. Normal progression: PROVISIONING → PENDING → RUNNING\n2. PENDING state: 1-2 minutes is normal (downloading image from ECR)\n3. RUNNING state: Task is healthy and serving traffic\nCommon Issues and Resolution:\nIssue 1: Task Status = STOPPED Symptom: Task stops immediately after starting\nDiagnosis Steps:\n1. Click on the stopped task ID\n2. Check Stopped reason field\n3. Switch to Logs tab for detailed error messages\nCommon Causes:\n| Error Message | Cause | Solution |\n| — | — | — |\n| Connection refused | Incorrect RDS/Redis endpoint in environment variables | Verify endpoint values in Task Definition |\n| CannotPullContainerError | Missing Public IP or subnet without internet access | Ensure Public IP is enabled and using Public Subnets |\n| Health check failed | Service startup time exceeds ALB timeout | Increase health check interval and timeout in Target Group settings |\n| ResourceInitializationError | Insufficient CPU/Memory | Verify task size configuration |\nSolution for Health Check Failures:\n1. Navigate to Target Group → Select the failing TG\n2. Health checks tab → Edit\n3. Increase values:\n- Interval: 60 seconds\n- Timeout: 30 seconds\n- Healthy threshold: 3\n4. Save changes\nIssue 2: Container Keeps Restarting Possible Causes:\n- Database connection failures\n- Missing environment variables\n- Application configuration errors\nDiagnosis:\n1. Check CloudWatch Logs for the task\n2. Verify all environment variables are correctly set\n3. Confirm RDS and Redis are accessible from ECS security group\nDeployment Verification Checklist Before proceeding to final configuration:\nServices Status:\n- [ ] Auth Service: RUNNING (1/1 tasks)\n- [ ] User Service: RUNNING (1/1 tasks)\n- [ ] Taskflow Service: RUNNING (1/1 tasks)\n- [ ] Notification Service: RUNNING (1/1 tasks)\n- [ ] AI Model Service: RUNNING (1/1 tasks)\n- [ ] API Gateway: RUNNING (1/1 tasks)\nTarget Groups Health:\n- [ ] auth-tg: 1 healthy target\n- [ ] user-tg: 1 healthy target\n- [ ] task-tg: 1 healthy target\n- [ ] noti-tg: 1 healthy target\n- [ ] api-gateway-tg: 1 healthy target\nService Discovery:\n- [ ] All services registered in Cloud Map\n- [ ] DNS names resolving correctly (verify in Route 53)\n⬅ BƯỚC 4: Task Definitions Creation\rBƯỚC 6: Completion \u0026 Verification ➡",
    "description": "This phase deploys containerized applications to ECS Fargate following a specific order to ensure proper dependency resolution.\nCritical Network Configuration For ALL service deployments, the following network settings are mandatory:\n1. VPC: SGU-Microservices-VPC\n2. Subnets: Select both Public Subnets (required for image pulling)\n3. Security Group: ecs-app-sg (remove default)\n4. Public IP: Turned on (critical for ECR access)\nDeployment Priority Order Services must be deployed in the following sequence to respect dependencies:",
    "tags": [],
    "title": "Services Deployment ",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.5-services-deployment/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 5 Objectives: Further research AWS security services. Complete the remaining labs of Module 5. Develop APIs for the Taskflow service. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translate blogs for week 5 - Hold weekly team meetings to update work progress. 06/10/2025 06/10/2025 original link 3 - Practice: + lab33: Encryption at Rest with AWS KMS 07/10/2025 07/10/2025 https://000033.awsstudygroup.com/vi/ 4 - Develop APIs for the Taskflow service in the project, section module 08/10/2025 08/10/2025 5 - Practice: + lab12: Using AWS IAM Identity Center for robust identity management 09/10/2025 09/10/2025 https://000012.awsstudygroup.com/vi/ 6 - Develop APIs for the Taskflow service in the task, member, comment module - Update the worklog. 10/10/2025 10/10/2025 Week 5 Achievements: Practice encryption at rest with AWS KMS:\nLearn about symmetric and asymmetric keys. Explore AWS CloudTrail to log, continuously monitor, and maintain activities related to operations on AWS infrastructure. Learn how to create KMS keys to encrypt data on S3. Understand how to create groups, users, and permission sets to manage account permissions within the organization.\nKnow how to connect accounts in the organization via CLI to use project resources.\nCreate permission sets in various ways for more flexible management.\nComplete several APIs for the Taskflow service.",
    "description": "Week 5 Objectives: Further research AWS security services. Complete the remaining labs of Module 5. Develop APIs for the Taskflow service. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translate blogs for week 5 - Hold weekly team meetings to update work progress. 06/10/2025 06/10/2025 original link 3 - Practice: + lab33: Encryption at Rest with AWS KMS 07/10/2025 07/10/2025 https://000033.awsstudygroup.com/vi/ 4 - Develop APIs for the Taskflow service in the project, section module 08/10/2025 08/10/2025 5 - Practice: + lab12: Using AWS IAM Identity Center for robust identity management 09/10/2025 09/10/2025 https://000012.awsstudygroup.com/vi/ 6 - Develop APIs for the Taskflow service in the task, member, comment module - Update the worklog. 10/10/2025 10/10/2025 Week 5 Achievements: Practice encryption at rest with AWS KMS:",
    "tags": [],
    "title": "Week 5",
    "uri": "/1-worklog/1.5-week5/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "Cost-Optimized SaaS Task Management Platform Overview SGU TodoList is a robust and resilient task management application built using a Microservices architecture designed for high scalability and decoupling. The primary goal of this workshop is to guide you through deploying and managing a complex application stack on the AWS Cloud platform, with a strong focus on cost optimization and serverless operations.\nBy leveraging core AWS services such as ECS Fargate, S3, ALB, and VPC, you will learn how to set up an infrastructure that is performant, highly available, and minimizes overhead costs compared to traditional EC2 instances.\nKey Technologies and Concepts This workshop covers practical implementation using:\nArchitecture: Microservices, Event-Driven (Kafka). Compute: AWS ECS Fargate (Serverless Containers). Backend: Spring Boot (Java 21). Frontend: ReactJS. Data Stores: AWS RDS (MySQL), ElastiCache (Redis). Networking \u0026 Discovery: AWS ALB, AWS Cloud Map (Service Discovery). Deployment: AWS CLI and PowerShell Automation Scripts. Table of Contents 1. Workshop Overview - Goals and learning outcomes.\n2. Prerequisite - Tools and AWS permissions required for deployment.\n3. Deploy Flow - Step-by-step guide on deploying Backend and Frontend.\n4. Clean Up - Instructions to tear down all resources to avoid charges.\n5. Mystic Skills - Additional mystic skills",
    "description": "Cost-Optimized SaaS Task Management Platform Overview SGU TodoList is a robust and resilient task management application built using a Microservices architecture designed for high scalability and decoupling. The primary goal of this workshop is to guide you through deploying and managing a complex application stack on the AWS Cloud platform, with a strong focus on cost optimization and serverless operations.\nBy leveraging core AWS services such as ECS Fargate, S3, ALB, and VPC, you will learn how to set up an infrastructure that is performant, highly available, and minimizes overhead costs compared to traditional EC2 instances.",
    "tags": [],
    "title": "Workshop",
    "uri": "/5-workshop/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "GitOps continuous delivery with ArgoCD and EKS using natural language Jagdish Komakula, Aditya Ambati, and Anand Krishna Varanasi | 17/07/2025 | Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, Developer Tools, Technical How-to | Permalink\nIntroduction ArgoCD is a leading GitOps tool that empowers teams to manage Kubernetes deployments declaratively, using Git as the single source of truth. Its robust feature set, including automated sync, rollback support, drift detection, advanced deployment strategies, RBAC integration, and multi-cluster support, makes it a go-to solution for Kubernetes application delivery. However, as organizations scale, several pain points and operational challenges become apparent.\nPain Points with Traditional ArgoCD Usage GArgoCD’s UI and CLI are designed for users with extensive technical background. Interacting with YAML manifests, understanding Kubernetes resource relationships, and troubleshooting sync errors require specialized knowledge. This limits access to GitOps workflows for less technical stakeholders and increases reliance on DevOps engineers.\nManaging ArgoCD across multiple clusters or environments (using hub-spoke, per-cluster, or grouped models) introduces significant operational complexity. Teams must handle multiple ArgoCD instances, maintain consistent configuration, and coordinate deployments, which can become a bottleneck as service footprints grow.\nArgoCD excels at syncing and monitoring Kubernetes resources but lacks built-in mechanisms for pre-deployment (e.g., image scanning) or post-deployment (e.g., load testing) tasks. This forces teams to rely on external tools or custom scripts, fragmenting the deployment pipeline and increasing maintenance effort.\nPromoting applications across environments (Dev → Test → Prod) is not natively streamlined. Teams must manually orchestrate or script these promotions, slowing down urgent fixes and complicating the release process.\nAs organizations adopt multi-cluster strategies, managing ArgoCD’s access, RBAC, and resource visibility across environments becomes cumbersome, often leading to fragmented workflows and potential security gaps.\nHow ArgoCD MCP Server with Amazon Q CLI addresses these challenges: The integration of the ArgoCD MCP (Model Context Protocol) Server with Amazon Q CLI fundamentally transforms the user experience by introducing natural language interaction for GitOps operations.\nWith MCP, users can manage deployments, monitor application states, and perform sync or rollback operations using plain conversational language rather than technical commands or YAML. For example, a user can simply ask, “What applications are out of sync in production?” or “Sync the api-service application,” and the system executes the appropriate ArgoCD API calls in the background.\nThis democratizes access to GitOps, enabling less technical team members (such as QA, product managers, or support engineers) to safely interact with deployment workflows.\nNatural language interfaces abstract away the complexity of multi-cluster and multi-environment management. Users can query or act on resources across clusters without memorizing resource names, namespaces, or API endpoints.\nThe MCP server handles authentication, session management, and robust error handling, reducing the need for manual troubleshooting and custom scripting.\nThe integration provides detailed feedback, intelligent endpoint handling, and comprehensive error messages, making it easier to diagnose and resolve issues. Full static type checking and environment-based configuration further enhance reliability and maintainability.\nBy leveraging Amazon Q CLI’s extensibility, users gain access to pre-built integrations and context-aware prompts, accelerating development and deployment workflows.\nThe MCP server enables AI assistants and language models to automate routine tasks, recommend actions, and even debug issues, acting as a virtual DevOps engineer. This can significantly reduce manual effort and speed up incident response.\nTraditional ArgoCD vs. ArgoCD MCP Server with Amazon Q CLI Feature/Challenge Traditional ArgoCD With MCP Server + Amazon Q CLI User Interface Technical UI/CLI, YAML required Natural language, conversational Access for Non-Engineers Limited Broad, democratized Multi-Cluster Management Complex, manual Simplified, abstracted Pre-Post Deployment Tasks External tools/scripts needed (Still external, but easier to invoke) Application Promotion Manual or scripted Natural language, easier orchestration Troubleshooting Technical, error-prone Guided, AI-assisted, detailed feedback Automation Scripting required AI/agent-driven, proactive You can perform the following actions using natural language using Amazon Q CLI integration with ArgoCD MCP server.\nApplication Management: List, create, update, and delete ArgoCD applications\nSync Operations: Trigger sync operations and monitor their status\nResource Tree Visualization: View the hierarchy of resources managed by applications\nHealth Status Monitoring: Check the health of applications and their resources\nEvent Tracking: View events related to applications and resources\nLog Access: Retrieve logs from application workloads\nResource Actions: Execute actions on resources managed by applications\nSetting Up Your Environment Pre-requisites Following are the pre-requisites for setting up your EKS environment to be managed by ArgoCD using Amazon Q CLI.\nAn AWS account with appropriate permissions\nAWS CLI v2.13.0 or later\nNode.js v18.0.0 or later\nnpm v9.0.0 or later\nAmazon Q CLI v1.0.0 or later (npm install -g @aws/amazon-q-cli)\nAn EKS cluster (v1.27 or later) with ArgoCD v2.8 or later installed\nConnecting to your EKS cluster Use AWS CLI to update your kubeconfig aws eks update-kubeconfig --name \u003ccluster_name\u003e --region \u003cregion\u003e --role-arn \u003ciam_role_arn\u003e\nVerify ArgoCD pods are running properly in the argocd namespace kubectl get pods -n argocd\nAccess the ArgoCD server UI locally using port forwarding command kubectl port-forward svc/blueprints-addon-argocd-server -n argocd 8080:443\nCreate AgroCD API Token Access the ArgoCD UI at https://localhost:8080 Log in with the admin credentials Navigate to User Settings \u003e API Tokens Click “Generate New” to create a token Create an Amazon Q CLI MCP configuration file at.amazonq/mcp.json and update the ARGOCD_BASE_URL and ARGOCD_API_TOKEN as per your environment setup. Integrating with Amazon Q CLI { \"mcpServers\": { \"argocd-mcp-stdio\": { \"type\": \"stdio\", \"command\": \"npx\", \"args\": [ \"argocd-mcp@latest\", \"stdio\" ], \"env\": { \"ARGOCD_BASE_URL\": \"\u003cARGOCD_BASE_URL\u003e\", \"ARGOCD_API_TOKEN\": \"\u003cARGOCD_API_TOKEN\u003e\", \"NODE_TLS_REJECT_UNAUTHORIZED\": \"0\" } } } }\rOnce configured, you can start using natural language commands with Amazon Q CLI to interact with your ArgoCD applications.\nManaging ArgoCD applications using natural language Listed below are some example prompts to interact with ArgoCD applications in your EKS cluster.\nList ArgoCD application\nPrompt: List all ArgoCD applications in my cluster\nAmazon Q will use the ArgoCD MCP server to retrieve and display all applications\nCreate new ArgoCD application\nPrompt: Create new argocd application using App name: game-2048 Repo: https://github.com/aws-ia/terraform-aws-eks-blueprints Path: patterns/gitops/getting-started-argocd/k8s. Branch: main Namespace: argocd\nAmazon Q will create a new application from GitRepo information provided\nViewing deployment status\nCâu lệnh: Show me the resource tree for team-carmen app\nAmazon Q will display the hierarchy of Kubernetes resources managed by the application\nSynchronizing applications\nPrompt: Show me the applications that’s out of sync\nAmazon Q will display the out of sync applications\nPrompt: Sync the application\nAmazon Q syncing application\nAmazon Q will:\nInitiate a sync operation for the specified application Monitor the sync progress Report the final status of the sync operation Healthchecks and monitoring\nPrompt: Check the health of all resources in the team-geordie application\nAmazon Q showing health status of all the resources in an application\nAmazon Q will:\nRetrieve the health status of all resources Identify any unhealthy components Provide recommendations for addressing issues Prompt: Show me the logs for the failing pod in the team-platform application\nAmazon Q showing logs of problematic pod\nAmazon Q will:\nIdentify problematic pods Retrieve and display relevant logs Highlight potential error messages Conclusion The integration of Amazon Q CLI with ArgoCD through the MCP server marks a transformative advancement in Kubernetes management, combining ArgoCD’s GitOps capabilities with Amazon Q’s natural language processing. By transforming complex Kubernetes operations into simple conversational interactions, this solution allows teams to focus on what truly matters – creating value for their business. Rather than spending time memorizing commands or navigating technical complexities, teams can now manage their cloud infrastructure through natural dialogue, making the cloud-native journey more accessible and efficient for everyone.Ready to transform your EKS and ArgoCD experience? It’s highly recommended to try out Amazon Q CLI integration with ArgoCD MCP and discover why DevOps teams are making it an essential part of their toolkit.",
    "description": "GitOps continuous delivery with ArgoCD and EKS using natural language Jagdish Komakula, Aditya Ambati, and Anand Krishna Varanasi | 17/07/2025 | Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, Developer Tools, Technical How-to | Permalink\nIntroduction ArgoCD is a leading GitOps tool that empowers teams to manage Kubernetes deployments declaratively, using Git as the single source of truth. Its robust feature set, including automated sync, rollback support, drift detection, advanced deployment strategies, RBAC integration, and multi-cluster support, makes it a go-to solution for Kubernetes application delivery. However, as organizations scale, several pain points and operational challenges become apparent.",
    "tags": [],
    "title": " Blog 6",
    "uri": "/3-blogstranslated/3.6-blog6/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This final phase connects the deployed backend to the public domain and verifies end-to-end functionality.\nDNS Configuration Step 1: Create Route 53 A Record\nNavigate to Route 53 → Hosted zones Select sgutodolist.com hosted zone Click Create record Configure record: Record name: api Record type: A Alias: Enable Route traffic to: Alias to Application Load Balancer Region: Asia Pacific (Singapore) Load balancer: Select sgu-alb Click Create records DNS Propagation: Wait 2-5 minutes for DNS changes to propagate.\nVerification:\nbash\nnslookup sgutodolist.com\r# Should return ALB's IP addresses\rGoogle OAuth Configuration Update Step 1: Update Authorized Redirect URIs\nAccess Google Cloud Console (console.cloud.google.com) Navigate to APIs \u0026 Services → Credentials Select your OAuth 2.0 Client ID Under Authorized redirect URIs, add: https://sgutodolist.com/api/auth/login/oauth2/code/google\rClick Save Note: Keep existing localhost URIs for local development.\nSystem Health Verification Perform the following tests to verify deployment success:\nTest 1: API Gateway Health Check bash\ncurl https://sgutodolist.com/actuator/health\rExpected Response:\njson\n{\r\"status\": \"UP\"\r}\rTest 2: Individual Service Health Checks bash\n# Auth Service\rcurl https://sgutodolist.com/api/auth/actuator/health\r# User Service\rcurl https://sgutodolist.com/api/user/actuator/health\r# Taskflow Service\rcurl https://sgutodolist.com/api/taskflow/actuator/health\r# Notification Service\rcurl https://sgutodolist.com/api/notification/actuator/health\rAll should return {\"status\":\"UP\"}.\nTest 3: Service Discovery Verification From the Bastion Host, verify internal DNS resolution:\nbash\n# SSH to Bastion\rssh -i sgutodolist-key.pem ec2-user@[BASTION-IP]\r# Test DNS resolution\rnslookup auth.sgu.local\rnslookup user.sgu.local\rnslookup taskflow.sgu.local\rnslookup notification.sgu.local\rnslookup ai-model.sgu.local\rnslookup kafka.sgu.local\rAll should resolve to internal ECS task IP addresses.\nTest 4: Database Connectivity Verify services can connect to RDS:\nCheck CloudWatch Logs for any service Look for successful database connection messages Verify no connection errors in startup logs Test 5: Redis Connectivity bash\n# From Bastion Host\rredis-cli -h [REDIS-ENDPOINT] ping\r# Expected response: PONG\rTest 6: End-to-End Authentication Flow Access frontend at https://sgutodolist.com Click “Sign in with Google” Complete OAuth flow Verify successful login and token issuance Verify user profile loads correctly Performance Baseline Record initial performance metrics:\nResponse Time Benchmarks:\nbash\n# API Gateway response time\rtime curl -o /dev/null -s https://sgutodolist.com/actuator/health\r# Auth service response time\rtime curl -o /dev/null -s https://sgutodolist.com/api/auth/actuator/health\rCloudWatch Metrics to Monitor:\nECS Task CPU Utilization ECS Task Memory Utilization ALB Target Response Time ALB Request Count RDS CPU Utilization Redis CPU Utilization Post-Deployment Security Checklist All sensitive environment variables secured (not in version control) Database password meets complexity requirements Security groups follow least privilege principle SSL/TLS certificates valid and auto-renewal enabled Bastion Host accessible only from authorized IPs CloudWatch Logs retention configured AWS Budget alerts active Final Deployment Checklist Infrastructure:\nVPC and subnets operational All 4 security groups correctly configured RDS database accessible and initialized Redis cache operational Kafka service running ALB active with healthy targets Application:\nAll 6 services deployed and running Service Discovery functional ALB routing rules working correctly Health checks passing CloudWatch Logs collecting data Integration:\nDNS record pointing to ALB SSL certificate valid Google OAuth configured Frontend can communicate with backend Authentication flow working Monitoring:\nCloudWatch dashboards created Budget alerts configured Performance baseline recorded Known Limitations and Future Improvements Current Architecture Constraints:\nSingle-AZ Database: RDS is deployed in a single availability zone for cost optimization Single-Node Redis: No automatic failover for cache layer Single-Node Kafka: Not production-grade for high-throughput scenarios Public Subnet ECS Tasks: Security trade-off for cost savings Recommended Production Enhancements:\nEnable RDS Multi-AZ deployment Implement Redis Cluster Mode with multiple replicas Deploy Kafka with multiple brokers across AZs Add NAT Gateway and move ECS tasks to private subnets Implement AWS WAF on ALB for DDoS protection Enable ECS Service Auto Scaling Implement CI/CD pipeline for automated deployments Troubleshooting Guide Quick Diagnosis Commands bash\n# Check ECS service status\raws ecs describe-services --cluster [CLUSTER-NAME] --services [SERVICE-NAME] --region ap-southeast-1\r# Check task status\raws ecs describe-tasks --cluster [CLUSTER-NAME] --tasks [TASK-ARN] --region ap-southeast-1\r# View recent logs\raws logs tail /ecs/[SERVICE-NAME] --follow --region ap-southeast-1\r# Check target health\raws elbv2 describe-target-health --target-group-arn [TG-ARN] --region ap-southeast-1\rEmergency Rollback Procedure If deployment fails:\nIdentify failing service: bash\naws ecs list-services --cluster [CLUSTER-NAME] --region ap-southeast-1\rUpdate service to previous task definition revision: bash\naws ecs update-service\\\r--cluster [CLUSTER-NAME]\\\r--service [SERVICE-NAME]\\\r--task-definition [TASK-DEF-FAMILY]:[PREVIOUS-REVISION]\\\r--region ap-southeast-1\rForce new deployment: bash\naws ecs update-service\\\r--cluster [CLUSTER-NAME]\\\r--service [SERVICE-NAME]\\\r--force-new-deployment\\\r--region ap-southeast-1\rSuccess Criteria Deployment is considered successful when:\n✅ All 6 ECS services show status: RUNNING ✅ All 5 target groups show: Healthy ✅ https://sgutodolist.com/actuator/health returns HTTP 200 ✅ Frontend at https://sgutodolist.com can authenticate via Google OAuth ✅ CloudWatch Logs show no critical errors ✅ All services accessible via internal DNS (*.sgu.local) ⬅ BƯỚC 5: Code Update \u0026 Image Build",
    "description": "This final phase connects the deployed backend to the public domain and verifies end-to-end functionality.\nDNS Configuration Step 1: Create Route 53 A Record\nNavigate to Route 53 → Hosted zones Select sgutodolist.com hosted zone Click Create record Configure record: Record name: api Record type: A Alias: Enable Route traffic to: Alias to Application Load Balancer Region: Asia Pacific (Singapore) Load balancer: Select sgu-alb Click Create records DNS Propagation: Wait 2-5 minutes for DNS changes to propagate.",
    "tags": [],
    "title": "Completion \u0026 Verification (Route 53, Google Console, Final Test)",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.6-completion--verification-route-53-google-console-final-test/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 5: POINT OFFICIAL DOMAIN (DNS RECORD) Go to Route 53 \u003e Hosted zones \u003e sgutodolist.com.\nCreate Record for Root domain:\nClick Create record.\nRecord name: (leave blank).\nType: A.\nAlias: Yes (Swipe the button to the right).\nRoute traffic to: Alias ​​to CloudFront distribution.\nChoose distribution: Select the CloudFront domain (e.g. d123...cloudfront.net).\nClick Create records.\nCreate Record for WWW: Do the same, but fill in www for Record name. ⬅ STEP 5: S3 Policy\rSTEP 7: Deploy and Test ➡",
    "description": "PHASE 5: POINT OFFICIAL DOMAIN (DNS RECORD) Go to Route 53 \u003e Hosted zones \u003e sgutodolist.com.\nCreate Record for Root domain:\nClick Create record.\nRecord name: (leave blank).\nType: A.\nAlias: Yes (Swipe the button to the right).\nRoute traffic to: Alias ​​to CloudFront distribution.\nChoose distribution: Select the CloudFront domain (e.g. d123...cloudfront.net).\nClick Create records.\nCreate Record for WWW: Do the same, but fill in www for Record name. ⬅ STEP 5: S3 Policy\rSTEP 7: Deploy and Test ➡",
    "tags": [],
    "title": "DNS Record",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.6-dns-record/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learning and working with AWS services while building a Todoist-like task management application, through which I improved my skills in problem-solving, teamwork, self-learning, and application development.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026 skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ☐ ✅ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Proactively explore and learn new knowledge, enhancing my self-learning abilities.\nFurther develop communication skills to achieve higher efficiency in work.\nImprove programming skills, effectively apply industry tools and knowledge.",
    "description": "During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learning and working with AWS services while building a Todoist-like task management application, through which I improved my skills in problem-solving, teamwork, self-learning, and application development.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.",
    "tags": [],
    "title": "Self Evaluation",
    "uri": "/6-self-evaluation/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 6 Objectives: Learn about AWS cloud database services Practice labs related to using RDS Review the content and practice of module 2 Tasks to be carried out this week: Day Task Start date Completion date Reference Material 2 - Review module 2: Understanding basic VPC architecture and related important components - Hold weekly team meetings to update work progress. 12/10/2025 12/10/2025 3 - Study module: Database hosting service on AWS 13/10/2025 13/10/2025 Video series about the module on AWS Study Group channel 4 - Practice: lab 05: Amazon Relational Database Service (Amazon RDS) 14/10/2025 14/10/2025 https://000005.awsstudygroup.com/vi/ 5 - Learn to use cache, Redis for final project 15/10/2025 15/10/2025 https://www.youtube.com/watch?v=HSknuSIoK6A 6 - Practice: lab 03: Getting Started with Amazon Virtual Private Cloud (VPC) and AWS Site-to-Site VPN - Update the worklog. 16/10/2025 16/10/2025 https://000003.awsstudygroup.com/vi/ Week 6 Achievements: Learn about different database services: RDS, ElastiCache, Redshift.\nUnderstand database concepts and the key terminology to know when working with databases.\nPractice a lab to deploy a system using RDS for storing information and check the database logs.\nRe-practice the VPC deployment lab to become more proficient and learn how to fix errors during deployment.\nLearn how to cache data in Redis and implement measures to protect the system from spam and data loss.",
    "description": "Week 6 Objectives: Learn about AWS cloud database services Practice labs related to using RDS Review the content and practice of module 2 Tasks to be carried out this week: Day Task Start date Completion date Reference Material 2 - Review module 2: Understanding basic VPC architecture and related important components - Hold weekly team meetings to update work progress. 12/10/2025 12/10/2025 3 - Study module: Database hosting service on AWS 13/10/2025 13/10/2025 Video series about the module on AWS Study Group channel 4 - Practice: lab 05: Amazon Relational Database Service (Amazon RDS) 14/10/2025 14/10/2025 https://000005.awsstudygroup.com/vi/ 5 - Learn to use cache, Redis for final project 15/10/2025 15/10/2025 https://www.youtube.com/watch?v=HSknuSIoK6A 6 - Practice: lab 03: Getting Started with Amazon Virtual Private Cloud (VPC) and AWS Site-to-Site VPN - Update the worklog. 16/10/2025 16/10/2025 https://000003.awsstudygroup.com/vi/ Week 6 Achievements: Learn about different database services: RDS, ElastiCache, Redshift.",
    "tags": [],
    "title": "Week 6",
    "uri": "/1-worklog/1.6-week6/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple Ryan Coleman and Belle Guttman | 15/07/2025 | Amazon Machine Learning, Announcements, Artificial Intelligence, Open Source| Permalink | Comments\nToday we are excited to announce version 1.0 of the Strands Agents SDK, marking a significant milestone in our journey to make building AI agents simple, reliable, and production-ready. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production.\nSince launching as a preview in May 2025, we’ve seen over 2,000 stars on GitHub and over 150K downloads on PyPI. Strands 1.0 brings the same level of simplicity to multi-agent applications that Strands has provided for single agents, with the addition of four new primitives and support for the Agent to Agent (A2A) protocol. To take multi-agent architectures into production, 1.0 also includes a new session manager for retrieving agent state from a remote datastore, and improved async support throughout the SDK. For flexibility to build your agents with any model, support for five additional model provider APIs were contributed by partners like Anthropic, Meta, OpenAI, Cohere, Mistral, Stability, Writer and Baseten (see the pull request). Let’s get into these updates in detail. Complete code samples are available on strandsagents.com.\nSimplifying multi-agent patterns Multi-agent patterns enable specialized AI agents to work together—delegating tasks, sharing knowledge, and coordinating actions—to solve complex problems that single agents cannot handle alone. Strands 1.0 introduces four intuitive primitives that make orchestrating multiple agents a simple extension of the model/tool/prompt combination that you use to create single agents.\n1. Agents-as-Tools: Hierarchical Delegation Made Simple\nThe agents-as-tools pattern transforms specialized agents into intelligent tools that other agents can call, enabling hierarchical delegation where agents acting as the orchestrator dynamically consult domain experts without giving up control of the request. This mirrors how human teams work—a project manager doesn’t need to know everything, they just need to know which specialist to consult for each task.\nfrom strands import Agent, tool from strands_tools import calculator, file_write, python_repl, journal @tool def web_search(query: str) -\u003e str: return \"Dummy web search results here!\" # Create specialized agents research_analyst_agent = Agent( system_prompt=\"You are a research specialist who gathers and analyzes information about local startup markets\", tools=[web_search, calculator, file_write, python_repl] ) travel_advisor_agent = Agent( system_prompt=\"You are a travel expert who helps with trip planning and destination advice\", tools=[web_search, journal] ) # Convert the agents into tools @tool def research_analyst(query: str) -\u003e str: response = research_analyst_agent(query) return str(response) @tool def travel_advisor(query: str) -\u003e str: response = travel_advisor_agent(query) return str(response) # Orchestrator naturally delegates to specialists executive_assistant = Agent( tools=[research_analyst, travel_advisor] ) result = executive_assistant(\"I have a business meeting in Portland next week. Suggest a nice place to stay near the local startup scene, and suggest a few startups to visit\") In this abridged example, we define travel and research agents who have specialized prompts and tools for their areas of focus, which the executive assistant agent can call upon for input on the user’s request. The executive assistant agent is responsible for synthesizing input from other agents into the response back to the user. Learn more about Agents-as-Tools in the Strands documentation.\n2. Handoffs: Explicit transfer of control\nHandoffs enable agents to explicitly pass responsibility to humans when they encounter tasks outside their expertise, preserving full conversation context during the transfer. Strands provides a built-in handoff_to_user tool that agents can use to seamlessly transfer control while maintaining conversation history and context—like a customer service representative asking the customer for more information about their case.\nfrom strands import Agent from strands_tools import handoff_to_user SYSTEM_PROMPT=\"\"\" Answer the user's support query. Ask them questions with the handoff_to_user tool when you need more information \"\"\" # Include the handoff_to_user tool in our agent's tool list agent = Agent( system_prompt=SYSTEM_PROMPT, tools=[handoff_to_user] ) # The agent calls the handoff_to_user tool which includes the question for the customer agent(\"I have a question about my order.\")\rCác tác tử cũng có thể đặt câu hỏi cho con người khi được yêu cầu làm như vậy\nfrom strands import Agent SYSTEM_PROMPT=\"\"\" Answer the user's support query. Ask them questions when you need more information \"\"\" agent = Agent( system_prompt=SYSTEM_PROMPT, ) # The agent asks questions by streaming them back as text agent(\"I have a question about my order.\")\r3. Swarms: Self-Organizing Collaborative Teams\nA Swarm creates autonomous agent teams that dynamically coordinate through shared memory, allowing multiple specialists to collaborate on complex tasks. Think of it as a brainstorming session where experts build on each other’s ideas, with the team self-organizing to deliver the best collective result.\nimport logging from strands import Agent from strands.multiagent import Swarm from strands_tools import memory, calculator, file_write # Enables Strands debug logs level, and prints to stderr logging.getLogger(\"strands.multiagent\").setLevel(logging.DEBUG) logging.basicConfig( format=\"%(levelname)s | %(name)s | %(message)s\", handlers=[logging.StreamHandler()] ) researcher = Agent( name=\"researcher\", system_prompt=\"You research topics thoroughly using your memory and built-in knowledge\", tools=[memory] ) analyst = Agent( name=\"analyst\", system_prompt=\"You analyze data and create insights\", tools=[calculator, memory] ) writer = Agent( name=\"writer\", system_prompt=\"You write comprehensive reports based on research and analysis\", tools=[file_write, memory] ) # Swarm automatically coordinates agents market_research_team = Swarm([researcher, analyst, writer]) result = market_research_team( \"What is the history of AI since 1950? Create a comprehensive report\" ) Learn more about Swarms in the Strands documentation.\n4. Graphs: Deterministic Workflow Control\nGraphs let you define explicit agent workflows with conditional routing and decision points, helpful for processes that require specific steps, approvals, or quality gates. Like a well-designed assembly line or approval chain, graphs ensure agents work through predefined business rules in the correct order every time.\nfrom strands import Agent from strands.multiagent import GraphBuilder analyzer_agent = Agent( name=\"analyzer\", system_prompt=\"Analyze customer requests and categorize them\", tools=[text_classifier, sentiment_analyzer] ) normal_processor = Agent( name=\"normal_processor\", system_prompt=\"Handle routine requests automatically\", tools=[knowledge_base, auto_responder] ) critical_processor = Agent( name=\"critical_processor\", system_prompt=\"Handle critical requests quickly\", tools=[knowledge_base, escalate_to_support_agent] ) # Build deterministic workflow builder = GraphBuilder() builder.add_node(analyzer_agent, \"analyze\") builder.add_node(normal_processor, \"normal_processor\") builder.add_node(critical_processor, \"critical_processor\") # Define conditional routing def is_approved(state): return True def is_critical(state): return False builder.add_edge(\"analyze\", \"normal_processor\", condition=is_approved) builder.add_edge(\"analyze\", \"critical_processor\", condition=is_critical) builder.set_entry_point(\"analyze\") customer_support_graph = builder.build() # Execute the graph with user input results = customer_support_graph(\"I need help with my order!\") Learn more about Graphs in the Strands documentation.\nThese multi-agent patterns are designed to be gradually adopted and freely combined—start with single agents, add specialists as tools, evolve to swarms, and orchestrate with graphs as your needs grow. Mix and match patterns to create sophisticated systems: swarms can contain graphs, graphs can orchestrate swarms, and any pattern can use agents equipped with other agents as tools.\nfrom strands import Agent, tool from strands.multiagent import GraphBuilder, Swarm from strands_tools import memory, calculator, python_repl, file_write # Start simple with a single agent agent = Agent(tools=[memory]) # Create specialist agents that a lead orchestrator agent can consult data_analyst = Agent(name=\"analyst\", tools=[calculator, python_repl]) @tool def data_analyst_tool(query: str) -\u003e str: return str(data_analyst(query)) analyst_orchestrator = Agent(tools=[memory, data_analyst_tool]) # Agents-as-tools # Compose patterns together - a graph that uses a swarm researcher = Agent(name=\"researcher\", tools=[memory]) writer = Agent(name=\"writer\", tools=[file_write]) research_swarm = Swarm([researcher, analyst_orchestrator, writer]) review_agent = Agent(system_prompt=\"Review the research quality and suggest improvements\") builder = GraphBuilder() builder.add_node(research_swarm, \"research\") # Swarm as graph node builder.add_node(review_agent, \"review\") builder.add_edge(\"research\", \"review\") graph = builder.build() # The patterns nest naturally - swarms in graphs, agents as tools everywhere result = graph(\"How has green energy evolved over the last few years?\") Multi-Agent Systems with A2A Strands 1.0 includes support for the Agent-to-Agent (A2A) protocol, an open standard that enables agents from different platforms to communicate seamlessly. Any Strands agent can be wrapped with A2A capabilities to become network accessible and adhere to the A2A protocol. A2A agents from external organizations can also be used directly within all Strands multi-agent patterns.\nfrom strands import Agent from strands.multiagent.a2a import A2AServer from strands_tools.a2a_client import A2AClientToolProvider # Serve your agent via A2A protocol local_agent = Agent(name=\"analyzer\", tools=[web_search, data_analysis]) a2a_agent = A2AServer(agent=local_agent, port=9000) a2a_agent.serve() # AgentCard available at http://localhost:9000/.well-known/agent.json # Use remote A2A agents partner_agent_url = \"https://partner.com\" cloud_agent_url = \"https://cloud.ai\" # Connect to remote A2A enabled agents a2a_tool_provider = A2AClientToolProvider(known_agent_urls=[partner_agent_url, cloud_agent_url]) # Orchestrate remote agents orchestrator = Agent(tools=[a2a_tool_provider.tools]) Because A2A provides features like the agent card, a standardized description of agent capabilities, A2A-enabled multi-agent systems can easily discover and connect to agents created by other teams or other organizations. Strands auto-generates the agent card based on the tools you’ve given the agent. To see complete working examples and get started with the A2A integration, check out oursample repository and the Strands A2A documentation.\nProduction-Ready While Strands has been used in production by Amazon teams like Amazon Q Developer and AWS Glue long before its public release, we’ve been working backwards with hundreds of customers worldwide to extend Strands to support your production needs. These updates include a session management abstraction to support persisting data to and recovering from external data stores, structured output, improved async support, and much more (releases changelog).\nDurable Session Management: We’ve added SessionManager, a session management abstraction that enables automatic persistence and restoration of agent conversations and state. This allows agents to save their complete history to a storage backend like Amazon Simple Storage Service (Amazon S3) and seamlessly resume conversations even after compute restarts. Here’s an example using basic file-based persistence.\nfrom strands import Agent from strands.session.file_session_manager import FileSessionManager # Create a session manager with file-based storage Session_manager = FileSessionManager(session_id=”customer_support”, base_dir=\"./agent_sessions\") # Agent automatically persists all conversations agent = Agent( id=\"support_bot_1\", session_manager=session_manager, tools=[knowledge_base, ticket_system] ) # Messages are automatically saved as the conversation progresses agent(\"Help me reset my password\") agent(\"I can't access my email\") # Later, even after a restart, restore the full conversation You can extend this abstraction with your own storage backend implementation through a Data Access Object (DAO) pattern, and Strands includes local filesystem and Amazon S3 backends by default. Each agent gets a unique ID for tracking, and the system handles concurrent agents within the same session for multi-agent scenarios, ensuring your agents maintain context across deployments, scaling events, and system restarts. Learn more about Session Management in the Strands documentation.\nNative Async Support and Improved Performance roduction workloads demand reliability and responsive performance. For 1.0, we’ve improved the Strands event loop architecture to support async operations throughout the entire stack. Tools and model providers can now run asynchronously without blocking, enabling true concurrent execution. The new stream_async method streams all agent events—text, tool usage, reasoning steps—in real-time, with built-in cancellation support for when users navigate away.\nimport asyncio from fastapi import FastAPI from fastapi.responses import StreamingResponse from strands import Agent from strands_tools import calculator app = FastAPI() @app.post(\"/chat\") async def chat_endpoint(message: str): async def stream_response(): agent = Agent(tools=[web_search, calculator]) # Stream agent responses in real-time async for event in agent.stream_async(message): if \"data\" in event: yield f\"data: {event['data']}\\n\\n\" elif \"current_tool_use\" in event: yield f\"event: tool\\ndata: Using {event['current_tool_use']['name']}\\n\\n\" return StreamingResponse(stream_response(), media_type=\"text/event-stream\") # Concurrent agent evaluation async def evaluate_models_concurrently(prompt: str): async def stream(agent: Agent): print(f\"STARTING: {agent.name}\") async for event in agent.stream_async(prompt): # handle events print(f\"ENDING: {agent.name}\") return event[“result”] # last event is the agent result agents = [ Agent(name=\"claude\", model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0”), Agent(name=\"deepseek”, model=\"us.deepseek.r1-v1:0”), Agent(name=\"nova\", model=\"us.amazon.nova-pro-v1:0\") ] # Execute all agents concurrently responses = await asyncio.gather(*[stream(agent) for agent in agents]) return responses Learn more about Native Async Support in the Strands documentation.\nExpanded Model Provider Support: Customers told us they needed flexibility to use different models for different tasks. To deliver this, Strands Agents has received strong support from the model provider community. Model providers like Anthropic, Meta, OpenAI, Cohere, Mistral, Stability and Writer have made contributions which enable their own model API to be used by a Strands Agent with code. Accessing Strands Agents through a provider’s API infrastructure allows developers to focus on building AI-powered solutions, without managing infrastructure. These additions complement preview launch support for any model on Amazon Bedrock, OpenAI, and any OpenAI-compatible endpoint through LiteLLM. Strands lets you use different models for each agent, or switch models and model providers without modifying your tools or logic.\nfrom strands import Agent from strands.models import BedrockModel from strands.models.openai import OpenAIModel from strands.models.anthropic import AnthropicModel # Configure different model providers bedrock_model = BedrockModel( model_id=\"us.amazon.nova-pro-v1:0\", temperature=0.3, top_p=0.8, region_name=\"us-west-2\" ) openai_model = OpenAIModel( client_args={ \"api_key\": \"your-api-key\", }, model_id=\"gpt-4o\", params={ \"max_tokens\": 1000, \"temperature\": 0.7, } ) anthropic_model = AnthropicModel( client_args={ \"api_key\": \"your-api-key\", }, max_tokens=1028, model_id=\"claude-3-7-sonnet-20250219\", params={ \"temperature\": 0.5, } ) # Swap models or use different models for different agents in the same system researcher = Agent( name=\"researcher\", model=anthropic_model, tools=[web_search] ) writer = Agent( name=\"writer\", model=openai_model, tools=[document_formatter] ) analyzer = Agent( name=\"analyzer\", model=bedrock_model, tools=[data_processor] )\rThe Strands community has been a critical voice in shaping all of these updates, through usage, feedback and direct code contributions. Of the over 150 PRs merged into Strands between 0.1.0 and 1.0, 22% were contributed by community members who fixed bugs, added model providers, wrote docs, added features, and refactored classes to improve performance. We’re deeply grateful to each of you for helping make Strands the simplest way to take an agent from prototype to production.\nThe future of AI is multi-agent, and with Strands 1.0, that future is ready for production. Start building today at strandsagents.com.",
    "description": "Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple Ryan Coleman and Belle Guttman | 15/07/2025 | Amazon Machine Learning, Announcements, Artificial Intelligence, Open Source| Permalink | Comments\nToday we are excited to announce version 1.0 of the Strands Agents SDK, marking a significant milestone in our journey to make building AI agents simple, reliable, and production-ready. Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to deployment in production.",
    "tags": [],
    "title": " Blog 7",
    "uri": "/3-blogstranslated/3.7-blog7/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 6: DEPLOY \u0026 TESTING Step 6.1: Build \u0026 Deploy On your local computer (in the React project folder):\n# 1. Build to the production folder npm run build # 2. Upload to the MAIN bucket (Singapore) # Note: Just upload to Sing, AWS will automatically copy to US aws s3 sync build/ s3://sgutodolist-frontend-sg --delete # 3. Clear the CloudFront cache so users can see the new code immediately aws cloudfront create-invalidation --distribution-id \u003cID_CUA_BAN\u003e --paths \"/*\"\rStep 6.2: Test Go to https://sgutodolist.com.\nTry reloading the page (F5) at the sub-links (e.g. /tasks) to see if there is a 404 error.\nTest Replication: Go to S3 Console bucket Virginia to see if the file has appeared automatically (usually takes 15s - 1 minute).\n⬅ STEP 6: DNS Record",
    "description": "PHASE 6: DEPLOY \u0026 TESTING Step 6.1: Build \u0026 Deploy On your local computer (in the React project folder):\n# 1. Build to the production folder npm run build # 2. Upload to the MAIN bucket (Singapore) # Note: Just upload to Sing, AWS will automatically copy to US aws s3 sync build/ s3://sgutodolist-frontend-sg --delete # 3. Clear the CloudFront cache so users can see the new code immediately aws cloudfront create-invalidation --distribution-id \u003cID_CUA_BAN\u003e --paths \"/*\"\rStep 6.2: Test Go to https://sgutodolist.com.",
    "tags": [],
    "title": "Deploy and Test",
    "uri": "/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.7-deploy-and-test/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "Overall Evaluation 1. Working Environment\nThe working environment is friendly and welcoming. Everyone is willing to support and motivate each other in their tasks. The workspace is modern, tidy, and comfortable. 2. Support from Mentor / Team Admin\nThe mentors and admin team members always provide clear and detailed guidance, as well as support with procedures and learning materials, helping me complete the internship process smoothly. They are also willing to share knowledge based on their personal work experience. 3. Relevance of Work to Academic Major\nThe assigned tasks allow me to apply the knowledge I learned at school while also being exposed to new knowledge in a real work environment. I see this as an opportunity to reinforce what I have learned, acquire additional skills and knowledge from my colleagues. 4. Learning \u0026 Skill Development Opportunities\nDuring the internship, I had the opportunity to access new knowledge and working skills, and to learn how to use new services and toolkits to work more effectively. Additionally, this was also an opportunity to learn effective teamwork and communication with team members to achieve common goals. 5. Company Culture \u0026 Team Spirit\nThe company culture is positive and enjoyable: everyone is always willing to support and share knowledge, not only technical expertise but also professional working skills. When issues arise, the team collaborates to solve problems together rather than leaving anyone to handle them alone. 6. Internship Policies / Benefits\nThe company provides sufficient learning materials and good facilities to enable interns to study and work conveniently. The company also frequently organizes workshops, allowing me to learn new knowledge from senior colleagues and providing opportunities to connect with other interns. Additional Questions What I am most satisfied with during the internship is the positive and enjoyable working environment. Everyone is always willing to support and share their knowledge.\nIf I were to recommend this internship to friends, I would do so because it is an opportunity to experience a modern professional working environment, learn cutting-edge knowledge in the industry, and connect with an active and highly skilled community, such as the AWS Study Group.\nSuggestions \u0026 Expectations I would like to suggest that some of the current labs are slightly outdated in terms of the AWS interface compared to the screenshots in the lab materials, which may result in some steps being missing or different during practical exercises. In addition, some AWS services have updated certain operations compared to the guidance provided, which can make practice a bit challenging. This is my feedback aimed at improving the learning experience during the internship.",
    "description": "Overall Evaluation 1. Working Environment\nThe working environment is friendly and welcoming. Everyone is willing to support and motivate each other in their tasks. The workspace is modern, tidy, and comfortable. 2. Support from Mentor / Team Admin\nThe mentors and admin team members always provide clear and detailed guidance, as well as support with procedures and learning materials, helping me complete the internship process smoothly. They are also willing to share knowledge based on their personal work experience. 3. Relevance of Work to Academic Major",
    "tags": [],
    "title": "Feedback",
    "uri": "/7-feedback/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 7 Objectives: Practice labs on deploying applications to AWS\nTranslate blog posts\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Integrate methods to send messages to the corresponding notification topics in the Taskflow service using Kafka. - Hold weekly team meetings to update work progress. 20/10/2025 20/10/2025 3 - Practice: lab15: Deploying applications on Docker with AWS 21/10/2025 21/10/2025 https://000015.awsstudygroup.com/vi/ 4 - Translating blog posts: Continuous deployment based on GitOps methodology with ArgoCD and EKS using natural language 22/10/2025 22/10/2025 original link 5 - Translated blogs: Introducing Strands Agents 1.0: Simplifying Multi-Agent Orchestration Ready for Deployment 23/10/2025 23/10/2025 original link 6 - Practice: lab58: Working with Amazon System Manager - Session Manager - Update the worklog. 24/10/2025 24/10/2025 https://000058.awsstudygroup.com/vi/ Week 7 Achievements: Practice lab to create image and deploy application with docker\nPractice saving docker image to docker hub and Amazon ECR\nPractice accessing ec2 through session manager to enhance security. Avoid opening port 22 to ssh or exposing key-pair to ssh into instance",
    "description": "Week 7 Objectives: Practice labs on deploying applications to AWS\nTranslate blog posts\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Integrate methods to send messages to the corresponding notification topics in the Taskflow service using Kafka. - Hold weekly team meetings to update work progress. 20/10/2025 20/10/2025 3 - Practice: lab15: Deploying applications on Docker with AWS 21/10/2025 21/10/2025 https://000015.awsstudygroup.com/vi/ 4 - Translating blog posts: Continuous deployment based on GitOps methodology with ArgoCD and EKS using natural language 22/10/2025 22/10/2025 original link 5 - Translated blogs: Introducing Strands Agents 1.0: Simplifying Multi-Agent Orchestration Ready for Deployment 23/10/2025 23/10/2025 original link 6 - Practice: lab58: Working with Amazon System Manager - Session Manager - Update the worklog. 24/10/2025 24/10/2025 https://000058.awsstudygroup.com/vi/ Week 7 Achievements: Practice lab to create image and deploy application with docker",
    "tags": [],
    "title": "Week 7",
    "uri": "/1-worklog/1.7-week7/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e BlogsTranslated",
    "content": "Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances Eric Allen, Mark Azadpour, Deepthi Shankar, Olivia Choudhury, and Shyamal Mehtalia | 15/07/2025 | High Performance Computing, Life Sciences, Partner solutions\nThis post was contributed by Eric Allen (AWS), Olivia Choudhury (AWS), Mark Azadpour (AWS), Deepthi Shankar (Illumina), and Shyamal Mehtalia (Illumina)\nThe analysis of ever-increasing amounts of genomic and multiomic data demands efficient, scalable, and cost-effective computational solutions. Amazon Web Services (AWS) continues to support these workloads through FPGA accelerated compute offerings such as Amazon EC2 F2 instances.\nIllumina’s DRAGEN (Dynamic Read Analysis for GENomics) secondary analysis solution has established itself as a leading secondary analysis solution for next generation sequencing data offering highly optimized algorithms for genomic analysis and hardware-accelerated implementations of comprehensive genomic and multiomic pipelines including germline DNA, somatic DNA, as well as bulk and single cell RNA analysis, proteomics, spatial, and more.\nDRAGEN runs natively on EC2 F2 instances and provides customers with an accelerated approach for analyzing their biological data sets. Migration to F2s is streamlined as the same DRAGEN AMI is used for both F1 and F2, and analysis results will be equivalent1. In this post, we will discuss the performance characteristics of DRAGEN across different AWS compute environments as well as how to run the recently launched DRAGEN v4.4 on Amazon EC2 F2 instances.\nAmazon EC2 F2 instances overview F2 instances are the second generation of FPGA powered EC2 instances designed to accelerate analysis of genomic and multimedia data in the cloud. These instances offer significant improvements over their predecessors, the F1 instances, providing up to 60% better price performance. Here are the key features and specifications of F2 instances:\nFPGA Configuration: F2 instances are equipped with up to eight AMD Virtex UltraScale+ HBM VU47P FPGAs, each featuring 16GB of high-bandwidth memory (HBM).\nProcessor: Powered by 3rd generation AMD EPYC (Milan) processors, F2 instances offer up to 192 vCPUs, which is three times the number of processor cores compared to F1 instances.\nMemory: These instances provide up to 2 TiB of system memory, doubling the memory capacity of F1 instances.\nStorage: F2 instances come with up to 7.6 TiB of NVMe SSD storage, which is twice the storage capacity of F1 instances.\nNetworking: They offer up to 100 Gbps of networking bandwidth, quadrupling the network bandwidth available in F1 instances.\nInstance Name\rFPGAs\rvCPUs\rInstance Memory\rNVMe Storage\rNetwork Bandwidth\rf1.2xlarge\r1\r8\r122\r470\rUp to 10 Gbps\rf1.4xlarge\r2\r16\r244\r940\r10 Gbps\rf2.6xlarge\r1\r24\r256\r950\r10 Gbps\rF2.12xlarge\r2\r48\r512\r1900\r25 Gbps\rf1.16xlarge\r8\r64\r976\r44x940\r25 Gbps\rF2.48xlarge\r8\r192\r2048\r7600\r100 Gbps\rTable 1: Table showing comparative compute, memory, storage, and networking specs for F1 vs F2 instances.\nPerformance benchmarking approach Illumina recommends using f1.4xlarge when using F1 instances and f2.6xlarge when using F2 instances. To assess performance across these instances, DRAGEN was configured and run on AWS following the Illumina DRAGEN user guide and the genome reference file links can be found on the Illumina DRAGEN Product Files web page.\nWhole Genome Sequencing (WGS) analysis at approximately 35x depth using a publicly available sample. The sample HG002 from the NIST Genome in a Bottle project was used. This sample was analyzed using DRAGEN v4.4 Germline pipeline in two different manners. The first “basic” analysis included only basic alignment and small variant calling, to facilitate comparisons with common bioinformatics pipelines for germline samples, such as BWA/GATK. The second “full” analysis was with all variant callers (including for copy number and structural variants) and additional options such as pharmacogenetic star allele calling and HLA (Human Leukocyte Antigen) calling turned on, to produce a fully analyzed whole genome. In both cases, DRAGEN’s hg38 multigenome graph reference was used for WGS analysis. The sample data fastq files can be accessed on Amazon S3 using these links: fastq R1, fastq R2.\nTumor Normal analysis using a pair of samples examined in a prior DRAGEN cancer analysis publication. These two samples had approximate depths of 110x (tumor) and 40x (normal). These samples were analyzed using the DRAGEN v4.4 Somatic pipeline, including alignment and small variant calling, plus CNV and SV analysis. DRAGEN’s hg38 linear genome reference was used for the Tumor Normal analysis. The sample data fastq files can be accessed on Amazon S3 using these links: Tumor fastq R1, Tumor fastq R2, Normal fastq R1, Normal fastq R2.\nSpeed \u0026 cost performance comparison: WGS analysis WGS analysis using DRAGEN v4.4 demonstrated significant price performance advantages on Amazon EC2 F2 instances vs F1, while also generating equivalent analysis results between the two instance generations1:\nBasic WGS analysis, including alignment and small variant calling. DRAGEN v4.4 analysis on f2.6xlarge had 1.5x the speed and 40% of the EC2 compute cost vs f1.4xlarge.\nComplete WGS analysis, including alignment, small variant calling and calling of CNVs, SVs, and repeat expansions, and variant annotation. DRAGEN analysis on f2.6xlarge had 2x the speed and 30% of the EC2 compute cost vs f1.4xlarge.\nFigure 1: f2.6xlarge is 1.5x faster for WGS Basic Analysis and 2.1x faster for WGS Full Analysis than f1.4xlarge.\nFigure 2: EC2 compute cost on f2.6xlarge is 40% of the cost on f1.4xlarge for WGS Basic Analysis and 30% of the cost on f1.4xlarge for WGS Full Analysis.\nSpeed \u0026 cost performance comparison: Tumor Normal analysis As with the WGS results, for Tumor Normal analysis DRAGEN v4.4 also demonstrated significant price performance advantages on Amazon EC2 F2 instances vs F1, while also generating equivalent analysis results between the two instance generations1:\nTumor Normal analysis, including alignment, small variant calling, and calling of CNVs and SVs. DRAGEN analysis on f2.6xlarge had 1.7x the speed and 35% of the EC2 compute cost vs f1.4xlarge. Figure 3: f2.6xlarge is 1.7x faster than f1.4xlarge for Tumor Normal Analysis.\nFigure 4: EC2 compute cost on f2.6xlarge is 35% of the cost on f1.4xlarge for WGS Tumor Normal Analysis.\nOther advantages Traditional genomic analysis pipelines using BWA-MEM and GATK running on CPUs have been the industry standard in the past, but DRAGEN has gained traction by providing speed and accuracy advantages. A number of peer reviewed publications have compared DRAGEN speed and accuracy vs BWA/GATK based pipelines running on CPU. For example, Ziegler et al. (2022) found that DRAGEN analysis on FPGA hardware was over 8x faster and more accurate than BWA/GATK based pipelines running on CPU, while Sedlazek et al. (2024) also found DRAGEN to have higher speed and accuracy vs BWA/GATK based pipelines.\nUtilizing DRAGEN on F instances, which are powered by FPGAs, also offers power consumption advantages over traditional CPU and GPU solutions. FPGAs are inherently more energy-efficient, consuming less power for the same level of computational performance for these workloads. This is particularly important in genomic data analysis tasks, where the volume of data and amount of processing time required can be immense.\nFor instance, FPGAs achieve higher performance per watt compared to CPUs and GPUs for DRAGEN WGS workloads. FPGA-based accelerators can deliver superior throughput with lower power consumption. This is due to the dynamic customizability of FPGAs, which allows for optimized configurations that further enhance energy efficiency. In contrast, CPUs and GPUs, while powerful, often require more energy to perform the same tasks, leading to higher operational costs and a larger environmental footprint.\nThe lower power consumption of FPGAs translates to reduced cooling requirements, which can be a significant cost factor in large-scale computing environments. Additionally, the energy efficiency of FPGAs makes them an attractive option for high-performance computing applications, where scalability and cost-effectiveness are critical.\nIn summary, leveraging DRAGEN on Amazon EC2 ‘F’ family instances provides a more energy-efficient solution for genomic data analysis compared to traditional CPU or GPU-based approaches, offering both cost savings and environmental benefits.\nF2 instance availability and cloud implementation options Amazon EC2 F2 instances are available in multiple AWS Regions, including US East (N. Virginia), US West (Oregon), Europe (London), and Asia Pacific (Sydney), with plans for expanded Regional availability. They come in different sizes, such as f2.6xlarge, f2.12xlarge and f2.48xlarge, catering to various workload requirements.\nWhen configuring storage and compute for running DRAGEN workloads on F instances on AWS, it’s important to select the right options to balance performance and cost. Consider storage options such as Amazon EBS gp3 volumes configured for RAID, Amazon FSx for Lustre for higher throughput, and Amazon Elastic File System (EFS) for persistent storage. Additionally, streaming BAM files and reference data from Amazon Simple Storage Service (Amazon S3) or using Mountpoint for Amazon S3 to mount S3 buckets on the local file system can provide economical and simplified file access. By carefully choosing and configuring these storage solutions, you can ensure optimal performance and cost-effectiveness for your HPC workloads. Additionally, consider usingg Illumina Connected Analytics (ICA) or AWS Batch for workflow management. Illumina provides guidance for running DRAGEN trên AWS as part of their online DRAGEN user guide.\nConclusion In summary, Amazon EC2 F2 instances represent a significant advancement in FPGA-powered cloud computing, offering enhanced performance, memory, storage, and networking capabilities over the previous generations. The combination of DRAGEN’s comprehensive pipelines with Amazon EC2 F2’s improved computational power delivers faster, more efficient processing of complex datasets – from whole genome sequencing to single-cell RNA analysis.\nStart your move to F2s today. Please reach out to your AWS account team or Illumina for help with your migration to AWS EC2 F2 instances.\nFor more info about DRAGEN on AWS, search for DRAGEN Complete Suite trên AWS Marketplace, view the blog DRAGEN v4.4, or visit trang chủ Illumina Informatics Solutions.\nReferences Scheffler, K. et al. “Somatic small-variant calling methods in Illumina DRAGEN™ Secondary Analysis.” BioRxiv (2023). https://www.biorxiv.org/content/10.1101/2023.03.23.534011v2\nSedlazeck, F. J. et al. “Comprehensive genome analysis and variant detection at scale using DRAGEN.” Nature Biotechnology (2024). https://www.nature.com/articles/s41587-024-02382-1\nZiegler, A. et al. “Comparison of calling pipelines for whole genome sequencing: an empirical study demonstrating the importance of mapping and alignment.” Scientific Reports (2022). https://pubmed.ncbi.nlm.nih.gov/36513709/\nFootnotes Equivalence between F1 and F2 hard-filtered.vcf and other results files based on analysis results using DRAGEN 4.4.4 following Illumina DRAGEN usage instructions. The vim diff command shows the only difference between the two VCF files is an entry showing the analysis execution time. Customers can do similar analyses themselves to verify or reach out to Illumina for more information.",
    "description": "Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances Eric Allen, Mark Azadpour, Deepthi Shankar, Olivia Choudhury, and Shyamal Mehtalia | 15/07/2025 | High Performance Computing, Life Sciences, Partner solutions\nThis post was contributed by Eric Allen (AWS), Olivia Choudhury (AWS), Mark Azadpour (AWS), Deepthi Shankar (Illumina), and Shyamal Mehtalia (Illumina)\nThe analysis of ever-increasing amounts of genomic and multiomic data demands efficient, scalable, and cost-effective computational solutions. Amazon Web Services (AWS) continues to support these workloads through FPGA accelerated compute offerings such as Amazon EC2 F2 instances.",
    "tags": [],
    "title": " Blog 8",
    "uri": "/3-blogstranslated/3.8-blog8/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 8 Objectives: Translate the remaining blog posts.\nPractice some exercises that support system optimization.\nIdentify application optimizations for the final project.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translate blog 8: Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances - Hold weekly team meetings to update work progress. 27/10/2025 27/10/2025 original link 3 - Revise the structure of worklogs - Review theoretical content of several AWS services - Learn about Design Patterns to apply in the final project such as: Proxy Pattern, Prototype Pattern, Builder Pattern 28/10/2025 28/10/2025 https://viblo.asia/p/proxy-design-pattern-tro-thu-dac-luc-cua-developers-RQqKLB2bl7z https://viblo.asia/p/prototype-design-pattern-tro-thu-dac-luc-cua-developers-GrLZDBQO5k0 4 - Practice: lab 22: Optimizing EC2 Costs with Lambda 29/10/2025 29/10/2025 https://000022.awsstudygroup.com 5 - Practice: lab 29: Getting started with Grafana basic 30/10/2025 30/10/2025 https://000029.awsstudygroup.com 6 - Learn about WebSocket - Update the worklog. 31/10/2025 31/10/2025 https://www.geeksforgeeks.org/web-tech/what-is-web-socket-and-how-it-is-different-from-the-http/ Week 8 Achievements: Learn about some useful design patterns to integrate into the final project:\nProxy Pattern: Helps enhance security by building an API Gateway for the system to prevent exposure of other modules’ endpoints.\nBuilder Pattern: Simplifies object creation in the system by constructing them step by step.\nPractice using AWS Lambda to optimize the system:\nLearn how to create Lambda Functions to automatically start and stop instances.\nLearn how to use environment variables in Lambda functions.\nUse CloudWatch to schedule automatic execution of Lambda Functions.\nPractice using Grafana to monitor AWS resources.\nUnderstand WebSocket to start integrating it into the project. This enables real-time notifications for users without reloading the page, ensuring timely information updates.",
    "description": "Week 8 Objectives: Translate the remaining blog posts.\nPractice some exercises that support system optimization.\nIdentify application optimizations for the final project.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translate blog 8: Enabling Rapid Genomic and Multiomic Data Analysis with Illumina DRAGEN™ v4.4 on Amazon EC2 F2 Instances - Hold weekly team meetings to update work progress. 27/10/2025 27/10/2025 original link 3 - Revise the structure of worklogs - Review theoretical content of several AWS services - Learn about Design Patterns to apply in the final project such as: Proxy Pattern, Prototype Pattern, Builder Pattern 28/10/2025 28/10/2025 https://viblo.asia/p/proxy-design-pattern-tro-thu-dac-luc-cua-developers-RQqKLB2bl7z https://viblo.asia/p/prototype-design-pattern-tro-thu-dac-luc-cua-developers-GrLZDBQO5k0 4 - Practice: lab 22: Optimizing EC2 Costs with Lambda 29/10/2025 29/10/2025 https://000022.awsstudygroup.com 5 - Practice: lab 29: Getting started with Grafana basic 30/10/2025 30/10/2025 https://000029.awsstudygroup.com 6 - Learn about WebSocket - Update the worklog. 31/10/2025 31/10/2025 https://www.geeksforgeeks.org/web-tech/what-is-web-socket-and-how-it-is-different-from-the-http/ Week 8 Achievements: Learn about some useful design patterns to integrate into the final project:",
    "tags": [],
    "title": "Week 8",
    "uri": "/1-worklog/1.8-week8/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 9 Objectives: Integrate WebSocket into the Notification module\nPractice labs related to Serverless Computing\nComplete the interface and API calls for the Task UpComing feature\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Integrate WebSocket into the project and run a demo - Hold weekly team meetings to update work progress. 3/11/2025 3/11/2025 3 - Build the interface for the Task UpComing feature, including displaying the task list, filtering by date, and creating tasks 4/11/2025 4/11/2025 4 - Practice: lab 78: Serverless – Interaction between Lambda, S3, and DynamoDB 5/11/2025 5/11/2025 https://000078.awsstudygroup.com/vi/ 5 - Complete creating, editing, and deleting tasks in the Task UpComing feature - Design the interface for comments of each task 6/11/2025 6/11/2025 6 - Complete API integration for the Task UpComing feature - Update the worklog. 7/11/2025 7/11/2025 Week 9 Achievements: Complete the Task UpComing feature:\nRetrieve the list of upcoming tasks based on their scheduled time\nCreate, delete, and edit tasks (including updating status, updating new start dates, etc.)\nRetrieve the list of task comments, add new comments, edit, and delete comments\nPractice the first lab in the serverless series – Interaction between Lambda, S3, and DynamoDB:\nCreate a Lambda function using Node.js 18.x runtime for image processing\nUse the Lambda function to write data into DynamoDB",
    "description": "Week 9 Objectives: Integrate WebSocket into the Notification module\nPractice labs related to Serverless Computing\nComplete the interface and API calls for the Task UpComing feature\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Integrate WebSocket into the project and run a demo - Hold weekly team meetings to update work progress. 3/11/2025 3/11/2025 3 - Build the interface for the Task UpComing feature, including displaying the task list, filtering by date, and creating tasks 4/11/2025 4/11/2025 4 - Practice: lab 78: Serverless – Interaction between Lambda, S3, and DynamoDB 5/11/2025 5/11/2025 https://000078.awsstudygroup.com/vi/ 5 - Complete creating, editing, and deleting tasks in the Task UpComing feature - Design the interface for comments of each task 6/11/2025 6/11/2025 6 - Complete API integration for the Task UpComing feature - Update the worklog. 7/11/2025 7/11/2025 Week 9 Achievements: Complete the Task UpComing feature:",
    "tags": [],
    "title": "Week 9",
    "uri": "/1-worklog/1.9-week9/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 10 Objectives: Complete the FE for comments in tasks within the project.\nComplete the notifications FE with real-time updates.\nBuild APIs for storing data on AWS S3.\nBuild APIs for the comment attachment feature in task comments.\nTasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Set up WebSocket on the frontend and connect it to the server - Create ModalNotification and ItemNotification components for displaying notifications - Hold weekly team meetings to update work progress. 10/11/2025 10/11/2025 3 - Build the UI for the comment feature in the task detail view - Integrate API into the comment UI 11/11/2025 11/11/2025 4 - Build API for uploading files to AWS S3 12/11/2025 12/11/2025 5 - Build API to allow users to attach files stored on S3 to comments 13/11/2025 13/11/2025 6 - Complete the notifications UI - Update the worklog. 14/11/2025 14/11/2025 Week 10 Achievements: Completed the comment UI in the task detail UI:\nUsers can add, edit, and delete their own comments. Users can view all comments made by others in the task. Completed the notifications UI:\nUsers can view all notifications or only unread ones. Users can accept/reject project invitations directly from this page. Users can mark individual notifications as read or mark all as read at once. Built APIs for S3 file upload and comment attachments:\nUsers can upload files to S3. Users can attach files uploaded to S3 to comments while keeping the markdown file format intact.",
    "description": "Week 10 Objectives: Complete the FE for comments in tasks within the project.\nComplete the notifications FE with real-time updates.\nBuild APIs for storing data on AWS S3.\nBuild APIs for the comment attachment feature in task comments.\nTasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Set up WebSocket on the frontend and connect it to the server - Create ModalNotification and ItemNotification components for displaying notifications - Hold weekly team meetings to update work progress. 10/11/2025 10/11/2025 3 - Build the UI for the comment feature in the task detail view - Integrate API into the comment UI 11/11/2025 11/11/2025 4 - Build API for uploading files to AWS S3 12/11/2025 12/11/2025 5 - Build API to allow users to attach files stored on S3 to comments 13/11/2025 13/11/2025 6 - Complete the notifications UI - Update the worklog. 14/11/2025 14/11/2025 Week 10 Achievements: Completed the comment UI in the task detail UI:",
    "tags": [],
    "title": "Week 10",
    "uri": "/1-worklog/1.10-week10/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 11 Objectives: Write the internship report and revise the content of worklogs, blogs, and events in preparation for submission\nComplete the UI for the task comment attach\nFix WebSocket logic issues that prevented user notifications from displaying correctly and integrate WebSocket calls via API Gateway\nTasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Write internship report in preparation for submission - Revise the content of worklogs, translated blogs, and events - Hold weekly team meetings to update work progress. 17/11/2025 17/11/2025 3 - Fix WebSocket logic issue when using pagination in the notifications page 18/11/2025 18/11/2025 4 - Develop UI to integrate attached image files for comments 19/11/2025 19/11/2025 5 - Complete comment attach UI and integrate API 20/11/2025 21/11/2025 6 - Implement WebSocket calls via API Gateway instead of direct calls and fix double-header issues - Update the worklog. 21/11/2025 21/11/2025 Week 11 Achievements: Completed the internship report for submission\nRevised content and UI for the workshop including:\nStructure of worklogs Images and content of translated blogs Participated events content Completed the UI for attaching image files in user comments\nFixed logic issues with notifications pagination using WebSocket and implemented WebSocket calls via API Gateway. Additionally, resolved errors occurring when calling through API Gateway",
    "description": "Week 11 Objectives: Write the internship report and revise the content of worklogs, blogs, and events in preparation for submission\nComplete the UI for the task comment attach\nFix WebSocket logic issues that prevented user notifications from displaying correctly and integrate WebSocket calls via API Gateway\nTasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Write internship report in preparation for submission - Revise the content of worklogs, translated blogs, and events - Hold weekly team meetings to update work progress. 17/11/2025 17/11/2025 3 - Fix WebSocket logic issue when using pagination in the notifications page 18/11/2025 18/11/2025 4 - Develop UI to integrate attached image files for comments 19/11/2025 19/11/2025 5 - Complete comment attach UI and integrate API 20/11/2025 21/11/2025 6 - Implement WebSocket calls via API Gateway instead of direct calls and fix double-header issues - Update the worklog. 21/11/2025 21/11/2025 Week 11 Achievements: Completed the internship report for submission",
    "tags": [],
    "title": "Week 11",
    "uri": "/1-worklog/1.11-week11/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 12 Objectives: Continue developing and fixing UI for the final project\nAdd functionality for users to customize receiving notifications via email\nAdd functionality to respond to project invitations directly on the notification page\nTasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Test the UI and APIs completed in previous weeks to see if any new issues arise - Hold weekly team meetings to update work progress. 24/11/2025 24/11/2025 3 - Write 2 APIs to accept and decline project invitations - Add functionality to respond to project invitations directly on the notification page 25/11/2025 25/11/2025 4 - Write API allowing users to customize whether they receive notifications via email 26/11/2025 26/11/2025 5 - Develop UI for the settings page to allow users to customize which notifications they will receive 27/11/2025 27/11/2025 6 - Add a CRITICAL priority level for tasks - Add UI to select priority levels and labels in task details - Update the worklog. 28/11/2025 28/11/2025 Week 12 Achievements: Completed testing of previously developed UI and APIs\nAdded functionality to respond to project invitations on the notification page\nCreated a settings page allowing users to customize receiving notifications via email\nUpdated task details UI to allow selecting priority levels and labels for tasks",
    "description": "Week 12 Objectives: Continue developing and fixing UI for the final project\nAdd functionality for users to customize receiving notifications via email\nAdd functionality to respond to project invitations directly on the notification page\nTasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Test the UI and APIs completed in previous weeks to see if any new issues arise - Hold weekly team meetings to update work progress. 24/11/2025 24/11/2025 3 - Write 2 APIs to accept and decline project invitations - Add functionality to respond to project invitations directly on the notification page 25/11/2025 25/11/2025 4 - Write API allowing users to customize whether they receive notifications via email 26/11/2025 26/11/2025 5 - Develop UI for the settings page to allow users to customize which notifications they will receive 27/11/2025 27/11/2025 6 - Add a CRITICAL priority level for tasks - Add UI to select priority levels and labels in task details - Update the worklog. 28/11/2025 28/11/2025 Week 12 Achievements: Completed testing of previously developed UI and APIs",
    "tags": [],
    "title": "Week 12",
    "uri": "/1-worklog/1.12-week12/index.html"
  },
  {
    "breadcrumb": "Intership Report \u003e Worklog",
    "content": "Week 13 Objectives: Edit, finalize, and review the final project\nComplete the remaining parts for the workshop and prepare the report\nTasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Add AI model suggestion button in the add labels interface of task details and integrate API to test the AI model - Replace manual mapper in taskflow service with MapStruct to optimize performance - Hold weekly team meetings to update work progress. 01/12/2025 01/12/2025 3 - Build API for the completed interface of the system. Allow reviewing activities of project members 02/12/2025 02/12/2025 4 - Finalize UI edits and call API for the completed interface to review project members’ activities 03/12/2025 03/12/2025 5 - Edit comment interface; fix the issue of taskdetails page not opening - Fix notification service after integrating MapStruct which caused incorrect DTO responses 04/12/2025 04/12/2025 6 - Edit and finalize the personal workshop - Update the worklog. 05/12/2025 05/12/2025 Week 13 Achievements: Integrated AI model suggestion feature for labels in the interface\nBuilt and integrated API into the system’s completed interface. Users can review project members’ activities\nFixed issues in the comment interface and taskdetails page\nFixed notification service errors caused by MapStruct integration affecting DTO responses\nEdited and finalized the personal workshop",
    "description": "Week 13 Objectives: Edit, finalize, and review the final project\nComplete the remaining parts for the workshop and prepare the report\nTasks to be carried out this week: Day Task Start Date End Date Reference Material 2 - Add AI model suggestion button in the add labels interface of task details and integrate API to test the AI model - Replace manual mapper in taskflow service with MapStruct to optimize performance - Hold weekly team meetings to update work progress. 01/12/2025 01/12/2025 3 - Build API for the completed interface of the system. Allow reviewing activities of project members 02/12/2025 02/12/2025 4 - Finalize UI edits and call API for the completed interface to review project members’ activities 03/12/2025 03/12/2025 5 - Edit comment interface; fix the issue of taskdetails page not opening - Fix notification service after integrating MapStruct which caused incorrect DTO responses 04/12/2025 04/12/2025 6 - Edit and finalize the personal workshop - Update the worklog. 05/12/2025 05/12/2025 Week 13 Achievements: Integrated AI model suggestion feature for labels in the interface",
    "tags": [],
    "title": "Week 13",
    "uri": "/1-worklog/1.13-week13/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Student Information: Full Name: Le Trung Kien\nPhone Number: 0931261009\nEmail: trungkien1862@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1213\nInternship Company: AMAZON WEB SERVICES VIETNAM COMPANY LIMITED\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback",
    "description": "Student Information: Full Name: Le Trung Kien\nPhone Number: 0931261009\nEmail: trungkien1862@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1213\nInternship Company: AMAZON WEB SERVICES VIETNAM COMPANY LIMITED\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback",
    "tags": [],
    "title": "Intership Report",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Intership Report",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
